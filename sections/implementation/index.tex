\chapter{Implementation}\label{ch:implementation}

Zur Analyse der Daten wurden diese von der SHL-Website\footnote{\url{http://www.shl-dataset.org/activity-recognition-challenge-2020/} (Abgerufen am 7.9.2021)} in eine Cloud-Instanz auf Google Colab\footnote{\url{https://colab.research.google.com/} (Abgerufem am 7.9.2021)} in einem Jupyter-Notebook \cite{kluyver_jupyter_2016} heruntergeladen. Die Bibliotheken NumPy \cite{harris_array_2020} und Pandas \cite{the_pandas_development_team_pandas-devpandas_2020,mckinney_data_2010} wurden verwendet, um generelle Transformationen und effiziente Operationen auf den Daten durchzuführen. Mithilfe der Bibliothek Matplotlib \cite{hunter_matplotlib_2007} wurden die Daten statistisch analysiert und grafisch aufbereitet.

Mithilfe der \texttt{CSVReader}-API von Pandas wurde ein speicheroptimierter Lademechanismus implementiert, welcher die entpackten Daten in \textit{Chunks} fester Länge lädt, um die Arbeitsspeicher-Limits der Cloud-Instanz nicht zu überschreiten. Die Implementation der Yeo-Johnson-Transformation auf Seite der Cloud-Instanz greift auf die \texttt{PowerTransformer}-API der Bibliothek Scikit-Learn \cite{pedregosa_scikit-learn_2011} zurück. Die trainierten Yeo-Johnson-Transformer wurden mithilfe der Python-Standardbibliotheken in das \texttt{json}-Format für die Integration in die iOS-Umgebung konvertiert und zusätzlich als \texttt{joblib}-Format für die spätere Wiederverwendung in Python gespeichert. Für die Klassen-Gewichtung der traditionelle Modelle wurde die \texttt{SMOTE}-API der Bibliothek Imbalanced Learn \cite{lemaitre_imbalanced-learn_2021} verwendet.

Die Implementation der traditionellen Modelle wurde mithilfe der Modell-APIs von Scikit-Learn durchgeführt, darunter \texttt{KNeighborsClassifier}, \texttt{SVC}, \texttt{DecisionTreeClassifier}, \texttt{RandomForestClassifier} und \texttt{MLPClassifier}. Die Konfiguration der prototypischen Modelle entstammt dem Scikit-Learn User-Guide\footnote{\url{https://scikit-learn.org/stable/user_guide.html} (Abgerufen am 7.9.2021)}. Die Evaluation der Accuracy wurde für die traditionellen Modelle mithilfe von Scikit-Learn-APIs durchgeführt.

Für die Implementation der Deep-Learning-Modelle wurden die Bibliotheken TensorFlow 2.0 \cite{martin_abadi_tensorflow_2015} und vor allem das dazugehörige Modul Keras \cite{chollet_keras_2015} verwendet. Für eine verbesserte Trainingsgeschwindigkeit wurden die speicheroptimiert geladenen Daten zusätzlich in das TensorFlow-eigene Format \texttt{TFRecord} überführt. Hierbei werden die Funktionalitäten des \texttt{TFRecord}s genutzt, um den gesamten Datensatz zu mischen und in Batches zu unterteilen. Das Training der Deep-Learning-Modelle erfolgte unter Hardwarebeschleunigung mithilfe einer NVIDIA Tesla P-100 GPU, welche der Cloud-Instanz in der Peripherie zur Verfügung steht. Das Training mithilfe einer Cloud-TPU wurde ebenfalls versucht, dies scheiterte jedoch an der Limitierung, dass die Trainingsdaten so auf einer kostenintensiven Cloud-Plattform zwischengespeichert werden müssten.

Beim Training der Modelle wurden die Keras-Schnittstellen für Early Stopping und für die Reduktion der Learning Rate bei nicht fortschreitender Validation Accuracy verwendet. Die Evaluation der Modelle fand ebenfalls durch die Keras-eigenen Schnittstellen statt, für das Monitoring des Trainingsprozesses wurden Logs gespeichert und über eine grafische TensorBoard-Oberfläche aufbereitet. Für die Erstellung des Hyperparameter-Metamodells wurde die Bibliothek Kerastuner \cite{omalley_keras_2019} verwendet. Hierbei wurde das \texttt{HyperResNet}-Modell von Kerastuner um eine eindimensionale Variante erweitert. Die Implementation der zusätzlichen STFT-Schichten bei der zweidimensionalen Variante greift auf die Bibliothek Kapre \cite{choi_kapre_2017} zurück. Die Rastersuche verwendet das Hyperband-Orakel der Kerastuner-Bibliothek. Die bei der Rastersuche trainierten Modelle sowie deren Trainingsverläufe wurden in Google Drive für die spätere Analyse zwischengespeichert.

Von den getesteten Modellen wurden die 10 besten Modelle für die Finalisierung ausgewählt. Für die Finalisierung wurden anhand der vorigen Beobachtungen des Trainingsverhaltens 15 Epochen als Limit festgesetzt, teils beendeten die Modelle ihr Training bereits vorher durch Early Stopping. Zur Portierung der Deep-Learning-Modelle wurde TensorFlow Lite verwendet. Dies ermöglicht gegenüber der iOS-eigenen Bibliothek CoreML eine vereinfachte Portierung auf Android-Geräte. Die Kapre-STFT-Schichten mussten hierbei in eine kompatible Variante überführt werden, da TensorFlow Lite auf iOS keine komplexen Zahlen unterstützt, welche jedoch ein Resultat aus der Forier-Transformation sind. Die Layer-Gewichtungen der serverseitig trainierten Modelle konnten jedoch problemlos auf das portierbare Modell übertragen werden. Die portierbaren Modelle wurden durch TensorFlow Lite ohne Optimierung in ein durch \texttt{gzip} komprimiertes \textit{Flatbuffer}-Format überführt, welches durch den iOS-seitigen TensorFlow Lite Interpreter gelesen werden kann. Die finalisierten und portierten Modelle wurden ebenfalls samt Trainingsverläufen auf Google Drive zwischengespeichert.

Die komprimierten Modelle wurden in eine prototypische iOS-App eingefügt. Anschließend wurden die Vorverarbeitungsschritte portiert. Hierzu wurde die Implementation der Yeo-Johnson-Transformation aus dem Scikit-Learn-Framework studiert, nach Swift portiert und in eine Stream-Processing-Pipeline integriert. Hierbei wurden die zuvor als \texttt{json} exportierten Konfigurationdateien der Yeo-Johnson-Transformation importiert. Ausgangspunkt der Stream-Processing-Pipeline ist die Erfassung der Sensordaten, wobei die Konfiguration des auf Android\footnote{\url{https://github.com/STRCWearlab/DataLogger} (Abgerufen am 7.9.2021)} aufgezeichneten SHL-Datensatzes nachgeahmt werden musste. Die Abtastrate der Sensordaten wurde analog zum SHL-Datensatz auf 100Hz festgelegt. Als Akzelerometer-Input wurden die rohen Akzelerometerdaten verwendet, wobei diese mit dem Faktor 9.81 skaliert werden müssen, da iOS die Akzelerometerdaten in Vielfachen der Erdbeschleunigung angibt\footnote{\url{https://developer.apple.com/documentation/coremotion/cmmotionmanager} (Abgerufen am 7.9.2021)}. Die Gyrosensordaten werden in iOS in gleicher Form wie auf Android angeboten, daher können diese direkt übernommen werden. Als Magnetfelddaten wiederum wurden nicht die direkten Daten des Magnetometers, sondern die kalibrierten Daten der \textit{DeviceMotion}-Schnittstelle von iOS verwendet, wobei das eigene Magnetfeld des Smartphones bereits herausgerechnet wird.

\begin{figure}[h]
  \centering
  \frame{
    \begin{subfigure}[t]{.33\textwidth}
      \centering
      \includegraphics[width=\linewidth]{app-1.png}
    \end{subfigure}
    \begin{subfigure}[t]{.33\textwidth}
      \centering
      \includegraphics[width=\linewidth]{app-2.png}
    \end{subfigure}
    \begin{subfigure}[t]{.33\textwidth}
      \centering
      \includegraphics[width=\linewidth]{app-3.png}
    \end{subfigure}
  }
  \caption{Screenshots der prototypischen App mit Modellselektion und aktivierbarer Visualisierung.}
  \label{fig:app}
\end{figure}

Die Inferenzrate der Modelle wurde über die durchgeführten Tests hinweg auf 2Hz festgelegt. Die Anzahl von Threads wurde auf 1 festgelegt, dies ist der beobachtbare Standardwert von TensorFlow Lite. Die Tests wurden auf einem iPhone SE und auf einem iPhone XS durchgeführt. Um verschiedene Hardware-Beschleuniger, darunter die Smartphone-GPU und die Apple Neural Engine des iPhone XS zu berücksichtigen, wurden das TensorFlow Lite \texttt{GPUDelegate}\footnote{\url{https://www.tensorflow.org/lite/performance/gpu} (Abgerufen am 11.9.2021)} und das \texttt{CoreMLDelegate}\footnote{\url{https://www.tensorflow.org/lite/performance/coreml_delegate} (Abgerufen am 11.9.2021)} genutzt. Außerdem wurde gemessen, wie viele Operationen jeweils auf die Hardware-Beschleuniger ausgelagert werden konnte, da dies von Modell zu Modell unterschiedlich ist. Diese Information dient später in der Evaluation der besseren Interpretation der Ressourcenparameter.

Für die Integrationstests der portierten Elemente wurden serverseitig Testdaten erzeugt und in \texttt{json}-Form in die App integriert. In Isolation getestet wurde hierbei die Datenvorverarbeitung und das ML-Modell. Die Integrationstests wurden erfolgreich bestanden. Für die Implementation eines Unit-Tests zur Messung der Inferenzzeit wurde auf die \texttt{measure}-API\footnote{\url{https://developer.apple.com/documentation/xctest/xctestcase/1496290-measure} (Abgerufen am 7.9.2021)} von XCode zurückgegriffen. Für das weitere Profiling der Ressourcenparameter wurde auf den Instruments-Profiler zurückgegriffen. Die Inferenztests und die Profiling-Schritte wurden für jedes Modell auf einem iPhone SE (keine Apple Neural Engine) und einem iPhone XS (mit Apple Neural Engine) unter den vordefinierten experimentellen Bedingungen durchgeführt. Die Ergebnisse wurden aufgezeichnet und für die spätere Analyse abgespeichert.

Das selektierte Modell wurde einer weiteren Optimierung unterzogen. Hierfür wurde auf das TensorFlow Model Optimization Toolkit\footnote{\url{https://www.tensorflow.org/model_optimization} (Abgerufen am 7.9.2021)} zurückgegriffen. Für die hieraus entstandenen Modelle wurden die Profiling-Schritte unter Aufzeichnung der Ergebnisse erneut durchgeführt, um schließlich zu einem finalen Modell zu kommen.
