\chapter{Verwandte Arbeiten}\label{ch:verwandte-arbeiten}

In diesem Kapitel wird näher auf die wissenschaftlichen Arbeiten eingegangen, deren Forschungsthema und Anforderungen sich mit dem dieser Arbeit stark überschneiden. Hierzu gehören die Movebis-Ansätze aus \cites{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020}. Dazu wird zunächst ein taxonomischer Überblick über Kategorien gegeben, in welche sich verwandte Arbeiten eingliedern lassen. Anschließend werden die Konzepte und Ergebnisse näher diskutiert und verglichen.

\section{Forschungsüberblick}

Der Forschungsbereich Machine Learning erhält aktuell eine große Aufmerksamkeit, allein in 2020 wurden geschätzt 25800 Publikationen mit dem Begriff \enquote{Machine Learning} im Titel veröffentlicht\footnote{Quelle: Google Scholar, erweiterte Suchfunktion. Teile dieser Ergebnisse können auch wiss. Artikel, Patente oder ähnliche Veröffentlichungen ohne garantierte wiss. Qualität (z.B. durch Peer-Review) sein.}. Ein Teil dieser Arbeiten fokussiert sich auf den Fachbereich \acrshort{har}, \cite{demrozi_human_2020} zählten in 2020 insgesamt 149 wissenschaftliche Publikationen, davon 53 mit Fokus auf Deep Learning und 96 mit Fokus auf traditionellen ML-Ansätzen. Dabei sind die auf Deep Learning basierenden Ansätze in der Erkennungsqualität nur marginal überlegen, mit einer durchschnittlichen Accuracy von $93.0\%$ gegenüber $92.2\%$. Die Accuracy eines jeweiligen Ansatzes muss jedoch immer in dessen Kontext betrachtet werden, denn sie ist stark abhängig vom verwendeten Datensatz und der verwendeten Teststrategie \cite{wang_summary_2020}. Die meisten der in \cite{demrozi_human_2020} analysierten Arbeiten nutzen zur Klassifikation Akzelerometerdaten, gefolgt von Gyrosensordaten, Magnetometerdaten und weiteren Sensordaten. Unter den Ansätzen befinden sich teils aber auch unkonventionelle Ideen, \cite{wang_sound-based_2019} verwenden beispielsweise das Mikrofon als Sensor.

Eine initiale anforderungsbezogene Recherche ergab die in der nachfolgenden Tabelle hervorgehobenen Arbeiten. Die Arbeiten sind taxonomisch unterteilt. In den unteren zwei Sektionen der Tabelle befinden sich Ansätze, welche eine Verkehrsmittelklassifikation über ML-Modelle realisieren, ohne konkreten Fokus auf eine Portierung oder die Lauffähigkeit des Ansatzes auf Smartphones. Unter anderem ordnen sich hier auch die bekannten Movebis-Ansätze, sowie die meisten Forschungsansätze für \acrshort{vme} ein. Darüber befinden sich Ansätze, welche einen allgemeineren Klassifikationsschwerpunkt (\acrshort{har}) ohne Fokus auf Verkehrsmittel haben, gleichzeitig jedoch für Smartphones ausgelegt wurden. Im obersten Teil der Tabelle ist ein Ansatz gezeigt, welcher eine Verkehrsmittelklassifikation explizit auf Smartphones über ML-Modelle realisiert, sowie eine Arbeit, die nochmals limitiertere Smartwatches betrachtet.

\begin{landscape}
  \begin{table}[]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{@{}llp{3cm}p{2cm}p{7cm}p{4cm}p{2cm}p{3cm}p{4cm}@{}}
  \toprule
    Arbeit &
    Kat. &
    Labels &
    Daten &
    Preprocessing &
    Features &
    Modell(e) &
    Regularisierung &
    Training \\ \midrule
  \cite{bhattacharya_smart_2016} &
    \acrshort{har}-W &
    Mehrere Varianten &
    \acrshort{acc} &
    Segmentierung, Abs. Signalstärke, \acrshort{fft} &
    Shallow &
    \acrshort{rbm} &
    Keine Angabe &
    Supervised Learning \\ \midrule
  \cite{hemminki_accelerometer-based_2013}  &
    \acrshort{vme}-S &
    Train, Bus, Stationary, Metro, Tram, Car &
    \acrshort{acc} &
    Tiefpass, Segmentierung, Elimination der Gravitation, Berechnung der Features &
    Shallow &
    \acrshort{dt} &
    Limitierung der Modellgröße &
    Supervised Learning, Ensemble Learning \\
  \midrule
  \cite{mairittha_-device_2021} &
    \acrshort{har}-S &
    12 Aktivitäten &
    \acrshort{gyr}, \acrshort{acc} &
    Abs. Signalstärke, Segmentierung &
    Vorverarbeitete Daten &
    \acrshort{rnn}-\acrshort{lstm} &
    Keine Angabe &
    Supervised Learning, Transfer Learning, On-Device Fine-Tuning \\
  \cite{mairittha_improving_2020} &
    \acrshort{har}-S &
    6 Aktivitäten &
    \acrshort{gyr}, \acrshort{acc} &
    Abs. Signalstärke, Segmentierung &
    Vorverarbeitete Daten &
    \acrshort{cnn} &
    Dropout &
    Supervised Learning, On-Device Fine-Tuning \\
  \cite{mairittha_-device_2019} &
    \acrshort{har}-S &
    6 Aktivitäten &
    \acrshort{acc} &
    Segmentierung, Feature-Berechnung &
    Shallow &
    \acrshort{rnn}-\acrshort{lstm} &
    Keine Angabe &
    Supervised Learning \\
  \cite{nutter_design_2018} &
    \acrshort{har}-S &
    9 Aktivitäten &
    \acrshort{gyr}, \acrshort{acc} &
    Glättung, Hochpass, Tiefpass, Elimination der Gravitation &
    Non-Shallow &
    \acrshort{cnn} &
    Dropout, Batch-Normalization &
    Transfer Learning \\
  \cite{zebin_design_2019} &
    \acrshort{har}-S &
    5 Aktivitäten &
    \acrshort{gyr}, \acrshort{acc} &
    Segmentierung, Windowing, Normalisierung &
    Vorverarbeitete Daten &
    \acrshort{cnn} &
    Dropout, Batch-Normalization &
    Supervised Learning \\
  \cite{ravi_deep_2016} &
    \acrshort{har}-S &
    Mehrere Varianten &
    \acrshort{gyr}, \acrshort{acc} &
    Feature-Berechnung, \acrshort{fft} &
    Non-Shallow &
    \acrshort{cnn} &
    Weight Decay, Momentum, Dropout &
    Supervised Learning \\ \midrule
  \cite{liang_convolutional_2017} &
    \acrshort{vme} &
    Bike, Car, Walk, Train, Metro, Bus, Stationary &
    \acrshort{acc} &
    Elimination der Gravitation, Gleitfenster, Abs. Signalstärke &
    Vorverarbeitete Daten &
    \acrshort{cnn} &
    L2-Regularisierung &
    Supervised Learning \\
  \cite{friedrich_transportation_2019} &
    \acrshort{vme} &
    Stationary, Walk, Run, Bike, Car, Bus, Train, Metro &
    \acrshort{acc}, \acrshort{gyr}, \acrshort{mag}, BAR &
    Datensatzgewichtung, Abs. Signalstärke, Normalisierung &
    Vorverarbeitete Daten &
    \acrshort{rnn}-\acrshort{lstm} &
    Dropout &
    Supervised Learning \\ \midrule
  \cite{werner_kontinuierliche_2020} &
    \acrshort{vme} &
    Walk, Bike, Car, Bus, Tram, Train &
    \acrshort{gnss}, \acrshort{acc}, ROT, \acrshort{mag} &
    Skalierung, Interpolation, \acrshort{ahrs}, Glättung &
    Vorverarbeitete Daten &
    \acrshort{rnn}-\acrshort{lstm} und Postprocessing &
    Dropout, Early Stopping &
    Supervised Learning, Validierung mith. Semi-Supervised Learning \\
  \cite{stojanov_continuous_2020} &
    \acrshort{vme} &
    Walk, Bike, Car, Bus, Tram, Train, Boat &
    \acrshort{gnss}, \acrshort{acc}, ROT, \acrshort{mag} &
    \acrshort{ahrs}, Normalisierung, Alignment, Abs. Signalstärke, Downsampling, \acrshort{fft}, Gleitfenster, Segmentierung &
    Shallow &
    \acrshort{cnn} und Postprocessing &
    Dropout, Early Stopping &
    Supervised Learning \\
  \cite{matusek_anwendung_2019} &
    \acrshort{vme} &
    Walk, Bike, Car, Bus, Tram, Train &
    \acrshort{gnss}, \acrshort{acc}, ROT, \acrshort{mag} &
    Interpolation und Alignment, GPS-Korrektur, Rauschfilterung über Moving Average, Zeitfenster &
    Vorverarbeitete Daten &
    FFN (Kernkonzept) &
    Early Stopping &
    Supervised Learning \\ \bottomrule
  \end{tabular}
  \caption[Vergleichender Überblick über eine initiale Auswahl verwandte Arbeiten.]{Vergleichender Überblick über eine initiale Auswahl verwandte Arbeiten. Abkürzungen wie folgt. Kategorien: \acrshort{vme} (Verkehrsmittelerkennung), \acrshort{vme}-W (\acrshort{vme} auf Wearables), \acrshort{vme}-S (\acrshort{vme} auf Smartphones), \acrshort{har} (Human Activity Recognition), \acrshort{har}-S (\acrshort{har} auf Smartphones). Daten: \acrshort{acc} (Akzelerometer), \acrshort{gyr} (Gyrosensor), \acrshort{mag} (Magnetometer), BAR (Barometer - Luftdruck). Preprocessing: \acrshort{fft} (Fast Fourier Transformation). Modelle: \acrshort{rbm} (Restricted Boltzmann Machine), \acrshort{dt} (Decision Tree), FFN (Feed-Forward-Network), \acrshort{rnn}-\acrshort{lstm} (Recurrent Neural Network mit Long-Short-Term-Memory-Neuronen), \acrshort{cnn} (Convolutional Neural Network).}
  }
  \end{table}
\end{landscape}

\nocite{chen_deep_2015, ravi_deep_2017, kwapisz_activity_2011, jahangiri_applying_2015, nurhanim_classification_2017, zeng_convolutional_2014, abu_alsheikh_deep_2015, inoue_deep_2018}

In der Übersicht spiegelt sich die Beobachtung aus \cite{demrozi_human_2020} wider, dass die gewählten konzeptuellen Kernkomponenten der Datenverarbeitungspipelines mitunter sehr heterogen sind. Dies erschwert die Auswahl vielversprechender und häufig eingesetzter Komponenten durch die Betrachtung konzeptioneller Überschneidungen, mit Hinblick auf die Movebis-Konzepte aus \cites{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020}. Die nachfolgenden Abschnitte dissektionieren daher zunächst allgemeine Gemeinsamkeiten und Unterschiede der betrachteten Arbeiten, um die unterschiedlichen Dimensionalitäten kennenzulernen, in denen die Forschungsansätze differieren.

\subsection{Labels und Datengrundlage}

Beginnend mit der Auswahl der Labels wird diese in der Regel in Abhängigkeit des Testdatensatzes durchgeführt, welcher zur Evaluation des jeweiligen Konzeptes verwendet wird. Die Ansätze sind meist nicht auf diese Labels beschränkt und können flexibel auf eine andere Auswahl (durch Modifikation einzelner Komponenten) erweitert oder reduziert werden. \cite{ravi_deep_2016} testen ihren Ansatz beispielsweise auf verschiedenen Datensätzen, mit verschiedenen zu klassifizierenden Labels. In manchen Ansätzen, die wie \cite{hemminki_accelerometer-based_2013} Ensemble Learning nutzen, ist dies möglich über das Hinzufügen, Modifizieren und Entfernen der partiellen Klassifikatoren. Weiterhin ist auch die Wahl der Daten, auf deren Grundlage die Klassifikation stattfindet, häufig wie in \cite{ravi_deep_2016} an die Verfügbarkeit der Sensortypen im Evaluationsdatensatz gebunden. \cites{liang_convolutional_2017,hemminki_accelerometer-based_2013,ravi_deep_2017} und beispielsweise auch \cite{byon_real-time_2014} beziehen ihre Datensätze aus eigenen Aufzeichnungen, teils werden jedoch nur wenige Angaben zur Menge der Daten und zur Aufzeichnung selbst gemacht. Dies limitiert die allgemeine Reproduzierbarkeit der Arbeiten \cite[S. 14]{demrozi_human_2020}. \cites{zebin_design_2019,nutter_design_2018,bhattacharya_smart_2016,mairittha_-device_2019,radu_towards_2016,friedrich_transportation_2019} nutzen öffentlich verfügbare Datensätze, teils auch aus anderen Forschungsarbeiten.

\subsection{Vorverarbeitung}

Eine Gemeinsamkeit der betrachteten Konzepte besteht darin, dass in den meisten verwandten Arbeiten die sequentiellen Daten explizit in Segmente unterteilt werden. Die Gleitfenstermethode wird hierbei häufig mit verwendet. Außerdem wird der Gravitationsvektor der Akzelerometerdaten in den meisten verwandten Arbeiten explizit als konzeptuell störender Faktor identifiziert und infolgedessen eliminiert. Auch die Skalierung der Daten wird in einigen Arbeiten explizit als Bestandteil des Vorverarbeitungskonzepts beschrieben, in \cite{werner_kontinuierliche_2020} nimmt diese eine zentrale konzeptuelle Rolle ein.

Die Vorverarbeitung ist jedoch auch Gegenstand unterschiedlicher Herangehensweisen. Die Notwendigkeit und konkrete Strategien zur Synchronisation oder dem Alignment beschreiben die Movebis-Arbeiten \cites{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} als essenziellen Bestandteil der Datenvorverarbeitung, hingegen argumentieren \cite{mairittha_improving_2020}, dass bei der Aufzeichnung der Sensoren auf demselben Gerät keine Synchronisation notwendig sei. Möglicherweise ist dieser Widerspruch zurückführbar auf die unterschiedlichen Eigenschaften und Inkonsistenzen der Datensätze und das starke Downsampling und die Interpolation auf $1Hz$ in \cites{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020}. Eine explorative Analyse des Datensatzes kann zur Klärung herangezogen werden, inklusive einer Analyse der Gewichtungen der Labels im Datensatz \cite[S. 15ff]{sosnovshchenko_machine_2018}. Sind diese unausgeglichen, lernt das ML-Modell unter Umständen ungleichmäßig (\textit{Bias}). Die Gleichgewichtung der Labels ist beispielsweise expliziter Bestandteil der Arbeit von \cite{friedrich_transportation_2019,mairittha_-device_2019}.

Die verwandten Arbeiten differenzieren sich weiter in der Entscheidung, ob die aufgezeichneten Daten nur praktisch verlustfrei transformiert werden (z.B. durch Segmentierung und Skalierung), oder im Voraus einer Glättung und weiteren verlustbehafteten Vorverarbeitungsmethoden (z.B. Tiefpass, Hochpass) unterzogen werden. In vielen verwandten Arbeiten wie \cite{nutter_design_2018} und auch in einer Metaanalyse von 2010 \cite{figo_preprocessing_2010} wird auf die grundlegende Problematik hingewiesen, dass die Sensordaten verrauscht sein können, und dass sich deshalb zumindest eine Glättung und die Anwendung von Frequenzbandfiltern anbietet. Es finden sich aber auch Ansätze wie \cite{mairittha_improving_2020}, welche bewusst keine dieser Signalverarbeitungsmethoden anwenden, mit der Argumentation, dass das jeweilige ML-Modell trotzdem eine geeignete interne Repräsentation erlernen könne.  \cite{demrozi_human_2020} beobachten, dass eine entsprechende Vorverarbeitung vor allem bei traditionellen Modellen außerhalb des Deep-Learning angewandt wird. Deep-Learning-Ansätze seien nach \cite[S. 14]{demrozi_human_2020} nicht zwingend auf eine transformative Datenvorverarbeitung angewiesen.

\subsection{ML-Modelle und Features}

Analog zur Vorverarbeitung ist auch die Wahl der konkreten Eingaben für das ML-System sehr unterschiedlich. In einem Teil der Arbeiten werden die vorverarbeiteten Daten direkt als Input verwendet, andere wiederum erzeugen Shallow Features durch Transformationen oder die Aggregation der Segmentdaten. In der Regel werden Shallow Features bei traditionellen ML-Ansätzen verwendet, aktuelle Deep-Learning-Ansätze sind nicht zwingend auf die Auswahl von Features angewiesen und erreichen auch auf Zeitliniendaten sehr gute Ergebnisqualitäten \cite[S. 2]{friedrich_combining_2020}. Hinzu kommen Ansätze mit Non-Shallow Features. \cite{nutter_design_2018} zeigen, dass weniger Non-Shallow Features benötigt werden können, um die gleiche Ergebnisqualität wie auf Shallow Features zu erreichen.

Die letztendlich gewählten Features sind oft eng gekoppelt an die Konfiguration der zentralen ML-Modelle, betrachtbar ist also ein Co-Design von Modellen und Vorverarbeitungsschritten. Beispielsweise können die vorverarbeiteten Sensordaten in Spektrogramme überführt werden, für die bildhafte Klassifikation über \acrshort{cnn}s \cite{ravi_deep_2016}. \cite{demrozi_human_2020} zeigt hierbei, dass generell verschiedene Modelle gewählt werden können, ohne einen durch seine Ergebnisqualität signifikant herausstechenden Ansatz.

\begin{figure}[h]
  \includegraphics[width=0.5\linewidth, bb=0 0 287 148]{deep-learning-trad-vergleich.pdf}
  \caption[Die Performanz von Deep-Learning-Ansätzen im Vergleich zu traditionellen ML-Modellen.]{Die Performanz von Deep-Learning-Ansätzen im Vergleich zu traditionellen ML-Modellen nach \cite[S. 7]{alom_state---art_2019}. Skalierung nicht linear oder maßstabsgerecht, nur zur Illustration.}\label{fig:deep-learning-trad-vergleich}
\end{figure}

Seit deren Erfindung in 2015 \cite{lecun_deep_2015} werden Deep-Learning-Modelle in der KI-Forschung als vielversprechendere Ansätze im Vergleich zu traditionellen Verfahren betrachtet, da sich mit diesen komplexere Probleme lösen lassen, sofern ausreichend Trainingsdaten vorliegen \cite[S. 7]{alom_state---art_2019}. Die Movebis-Ansätze nutzen beispielsweise auch Deep-Learning-Modelle \cite{werner_kontinuierliche_2020,stojanov_continuous_2020}, wobei das von \cite{matusek_anwendung_2019} verwendete MLP taxonomisch auch als traditionelles Modell eingeordnet werden kann \cite{demrozi_human_2020} und \cite{matusek_anwendung_2019} zusätzlich auch traditionelle Ansätze betrachtet.

Analog zu \Cref{fig:deep-learning-trad-vergleich} scheint \acrshort{har} als Anwendungsfall der ML-Klassifikation prinzipiell in einen Komplexitätsbereich zu fallen, der sowohl durch traditionelle, als auch durch Deep-Learning-Ansätze bedienbar ist. Sowohl Deep-Learning-Modelle, als auch traditionelle ML-Modelle finden sich verteilt über die zahlreichen Arbeiten der Forschungsdomäne \acrshort{har}. \cite[S. 14]{demrozi_human_2020} beobachten, dass die Entscheidung zwischen traditionellen Modellen und Deep-Learning-Ansätzen vor allem von der verfügbaren Rechenleistung abhängt, neben den verfügbaren Daten. Traditionelle Modelle seien typischerweise weniger rechenaufwändig und benötigten weniger Trainingsdaten, was diese Modelle wiederum attraktiver für durch Hardware limitierte Anwendungen wie die Ausführung auf Smartphones im Kontext dieser Arbeit wirken lässt.

\subsection{Training}

Abschließend lässt sich zur Forschungsübersicht erwähnen, dass zum Training der Modelle grundsätzlich verschiedene Methoden angewandt werden können. Das Reinforcement Learning findet sich allerdings in keiner der betrachteten Arbeiten wieder. Die Trainingsansätze sind nicht durch die Verfügbarkeit von Trainingsdaten limitiert, entweder durch die Nutzung eines der verschiedenen öffentlichen Datensatze oder durch die Aufzeichnung neuer Daten. Der wesentliche limitierende Faktor in den Trainingsdaten liegt nach \cite[S. 14]{demrozi_human_2020} häufig in der fehlenden Heterogenität, bedingt durch die Aufzeichnung von zu wenigen Personen, Geräten und Tragepositionen. Dies erhöhe den Generalisierungsfehler auf neuen Daten, evaluieren \cite{demrozi_human_2020}. Dabei ist die Aufzeichnung neuer Daten mit verhältnismäßig geringem Aufwand realisierbar, dies zeigen \cites{liang_convolutional_2017,hemminki_accelerometer-based_2013,ravi_deep_2017}. Durch die gute Verfügbarkeit der Trainingsdaten wird typischerweise das Supervised Learning als primäres Trainingsverfahren angewendet. Der Movebis-Ansatz \cite[S. 30]{werner_kontinuierliche_2020} erweitert den Trainingsdatensatz zur Validierung des Modells zusätzlich um ein Verfahren aus dem Semi-Supervised Learning, um die manuell zu bearbeitende Datenmenge zu reduzieren. \cite{nutter_design_2018} wenden auf bestehenden \acrshort{cnn}s darüberhinaus das Transfer Learning an, durch Abwandlung der äußeren Netzwerkschichten und Beibehaltung der (auf Bildern) vortrainierten Gewichtungen. Auch \cite{mairittha_-device_2021} nutzen eine Variante des Transfer Learnings, mit der Besonderheit, dass ein Teil des Netzwerkes vortrainiert und dann auf dem mobilen Gerät eingefroren wird. Ein kleiner Teil des Netzwerkes bleibt veränderlich und kann auf dem Gerät analog zu den Nutzereingaben angepasst werden (\textit{On-Device Fine-Tuning}). \cite{hemminki_accelerometer-based_2013} verwendenen mehrere miteinander kooperierende Klassifikatoren, zur Unterscheidung jeweils zueinander ähnlicher Klassen (\textit{Ensemble Learning}). Als expliziter Bestandteil der meisten Trainingskonzepte wird auch eine auf das Modell angepasste Regularisierung durchgeführt, beispielsweise wird die Dropout-Regularisierung und die Batch-Normalization häufig in Verbindung mit \acrshort{cnn}s verwendet, um die Ergebnisqualität zu verbessern.

\section{Movebis-Ansätze}

Die in \Cref{fig:movebis-vergleich} gezeigten Ansätze \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} bilden den Ausgangspunkt für diese Arbeit. Das im Movebis-Projekt initial verwendete heuristische Verfahren zur Verkehrsmittelerkennung auf Daten der STADTRADELN-App konnte durch das in \cite{matusek_anwendung_2019} vorgestellte ML-System signifikant in der Ergebnisqualität übertroffen werden.

\subsection{Vorverarbeitung nach \cite{matusek_anwendung_2019}}

\cite{matusek_anwendung_2019} legt dabei einen zentralen Fokus auf die Vorverarbeitung der Daten. Ein automatisiertes Encoding von Zeitliniendaten direkt durch das ML-Modell oder durch Non-Shallow Features erfolgt nicht. Die Vorverarbeitungsschritte inkludieren eine aufwändige Filtrierung der Daten und eine Überführung dieser in Shallow Features, sowie deren One-Hot-Encoding. Zu den Filtrierungsschritten aus \cite{matusek_anwendung_2019} gehört die Synchronisation der lokalen kinematischen Messdaten mit den GNSS-Daten auf eine Abtastrate von 1Hz sowie deren Interpolation. Die GNSS-Daten werden durch ein Kalman-Filter korrigiert. Die lokalen kinematischen Messdaten werden mithilfe eines Madgwick-Filters fusioniert. Rauschmuster werden durch Einsatz eines Moving-Average Filters reduziert. Die Daten werden mithilfe der Gleitfenstermethode als Segmente fester Länge aus dem Datensatz extrahiert. Als primäres ML-Modell wählt \cite{matusek_anwendung_2019} ein MLP, zur Evaluation zieht \cite{matusek_anwendung_2019} zusätzlich traditionelle Modelle hinzu. Gegenüber diesen, hierbei einem Decision Tree und einer \acrshort{svm}, erzeugte das MLP im Rahmen der Evaluation keine signifikant höhere Ergebnisqualität \cite[S. 96]{matusek_anwendung_2019}.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 606 305]{movebis-vergleich.pdf}
  \caption[Vergleich der Movebis-\acrshort{vme}-Pipelines.]{Vergleich der Movebis-Pipelines zur Verkehrsmittelerkennung. Abbildung schematisch nach den in \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} beschriebenen Konzeptkomponenten.}\label{fig:movebis-vergleich}
\end{figure}

Ein zentrales Ergebnis aus \cite{matusek_anwendung_2019} ist die resultierende Datenverarbeitungspipeline, welche eine aufwändige Vorverarbeitung inkludiert. Als zentrale Verbesserungsmöglichkeiten evaluierte \cite{matusek_anwendung_2019} die Feature-Berechnung und die Lückenhaftigkeit der GNSS-Daten im STADTRADELN-Datensatz \cite[S. 105]{matusek_anwendung_2019}. Außerdem bestehe weiteres Potenzial in der sauberen Abtrennung der Verkehrsmittel bei der Aufzeichnung von neuen Aktivitätsdaten, da insbesondere bei den Verkehrsmittelübergängen und bei der Beendigung der Aktivität wiederholt falsche Labels auftraten \cite[S. 98ff]{matusek_anwendung_2019}.

\subsection{ML-Modelle nach \cite{werner_kontinuierliche_2020,stojanov_continuous_2020}}

\begin{table}[h]
  \begin{tabular}{@{\extracolsep{\fill}}lllll}
  \toprule
  Arbeit & Healing & Beste Modell-Output-Accuracy & Accuracy n. Healing \\
  \midrule
  \cite{matusek_anwendung_2019} & \xmark & 67,84 \% n. \cite[S. 98]{stojanov_continuous_2020} & k.A. \\
  \cite{werner_kontinuierliche_2020} & \cmark & 92,00 \% n. \cite[S. 58]{werner_kontinuierliche_2020} & 98,27 \% n. \cite[S. 98]{stojanov_continuous_2020} \\
  \cite{stojanov_continuous_2020} & \cmark & 92,07 \% n. \cite[S. 90]{stojanov_continuous_2020} & 93,56 \% n. \cite[S. 98]{stojanov_continuous_2020} \\
  \bottomrule
  \end{tabular}\label{tab:movebis-vergleich}
  \caption{Erreichte Ergebnisqualitäten der Movebis-Ansätze.}
\end{table}

Auf dieser Grundlage entwickelten \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} ihre Konzepte mit größerem Fokus auf die Wahl der ML-Modelle. Sie übernehmen Teilschritte der Vorverarbeitung aus \cite{matusek_anwendung_2019}. \cite{werner_kontinuierliche_2020} nutzt ein mehrschichtiges \acrshort{rnn} mit \acrshort{lstm}-Neuronen, während \cite{stojanov_continuous_2020} ein \acrshort{cnn} zur Verkehrsmittelerkennung einsetzt. Analog zur Architektur der Modelle dienen in \cite{werner_kontinuierliche_2020} die vorverarbeiteten Datenpunkte als Zeitlinien-Eingabe des \acrshort{rnn}, gleichzeitig nutzt \cite{stojanov_continuous_2020} eine Auswahl von Shallow Features als Eingabe für das \acrshort{cnn}. Als zusätzlichen Schritt führen \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} eine Nachverarbeitung der Ausgaben durch Glättung anhand der Nachbarwerte ein, welche das Ergebnis der Ansätze weiter verbessern konnte. Bei der quantitativen Evaluation der Ansätze wurde insbesondere die Accuracy-Metrik der Netzwerke auf Testdaten durch Unterteilung des STADTRADELN-Datensatzes bestimmt. \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} konnten \cite{matusek_anwendung_2019} analog zu \Cref{tab:movebis-vergleich} deutlich übertreffen. Insbesondere das eingesetzte Postprocessing konnte die Accuracy des Gesamtsystems signifikant erhöhen.

\subsection{Limitationen in der Portierbarkeit nach \nameref{qa:p}}

Die Movebis-Architekturen aus \cites{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} stellen serverseitige Ansätze dar. Bei der Erstellung der dazugehörigen Konzepte lag somit kein verstärkter Fokus auf einer Optimierung oder der Messung von Energie- und Speicherverbrauch sowie der Inferenzzeit. Auch die Portierung auf Smartphones wurde in den Arbeiten nicht betrachtet. Innerhalb der Konzepte stellen sich verschiedene Kernaspekte heraus, welche für eine Portierung auf Smartphones als problematisch bewertet werden können. Im Vordergrund steht hierbei die Interpolation der Sensordaten auf die GNSS-Rate von 1Hz. Hierdurch werden verhältnismäßig große Fensterlängen von 30 Sekunden und mehr notwendig, um ausreichend Datenpunkte zur Klassifikation eines Segments zu erhalten. Während dies für die nachträgliche Klassifikation von Daten auf Servern kein Problem darstellt, würden Fensterlängen von über $30s$ auch eine entsprechende Wartezeit nach sich ziehen, bevor die Klassifikation auf dem Smartphone nach Start der Aufzeichnung aktiv werden könnte. Dies widerspricht den Anforderungen an die Nutzbarkeit (\nameref{qa:n}) bei Integration (\nameref{qa:int}) in eine vom Nutzer zu bedienende App, durch die funktionale Anforderung \nameref{fa:v2}. Dem liegt zugrunde, dass die Aufzeichnung des GNSS von den Movebis-Ansätzen benötigt wird. Ohne das Downsampling auf 1Hz könnten mehr Informationen in den Sensorsignalen erhalten werden, welche zur Klassifikation verwendet werden können. Eine höhere Abtastrate innerhalb des Samples kann die benötigte Dauer der Aufzeichnung bis zu einer Klassifikation reduzieren. Da in der STADTRADELN-App bereits GNSS-Daten aufgezeichnet werden, stellt deren Verzicht im Klassifikationsmodell keinen Vorteil für die Movebis-Ansätze dar. Vor dem Hintergrund einer Integration als Framework unabhängig von der STADTRADELN-App sollte dies jedoch neben der Notwendigkeit der extensiven und komplexen Vorverarbeitungsschritte nochmals in Abhängigkeit des Modells evaluiert werden, um die Berechnungsintensität möglichst gering zu halten (\nameref{qa:r}) und die Portierbarkeit der Vorverarbeitung zu verbessern (\nameref{qa:p}).

\section{\acrshort{har} auf Smartphones}

Die gezeigten Movebis-Ansätze beschreiben Kernkonzepte zur Vorverarbeitung, zur Modellkonzeption und zur Nachverarbeitung der Sensorwerte, welche als Ausgangspunkt für die nachfolgenden Betrachtungen in dieser Arbeit dienen. Hierbei existieren diverse Limitationen, welche durch Erweiterung und Modifikation der Ansätze behandelt werden müssen. Um dies zu realisieren, werden in den nachfolgenden Sektionen weitere Forschungsansätze betrachtet, welche insbesondere das Deployment auf Smartphones und die dazugehörigen Ressourcenfaktoren berücksichtigen.

Zunächst steht hierbei die Frage der Dimensionierung der Fensterlänge zur Realisation einer geringen Latenz bis zur initialen Klassifikation im Raum. \cite{banos_window_2014} zeigen beispielsweise, dass \acrshort{har}-Verfahren, welche eng mit Verkehrsmittelerkennungsverfahren verwandt sind, auch mit deutlich kürzeren Fensterlängen möglich sind. Entgegen der intuitiven Vermutung stechen hier sogar die Klassifikatoren mit besonders hoher Ergebnisqualität heraus, welche vergleichsweise kurze Fensterlängen von $0,25s$ bis $0,5s$ nutzen \cite[S. 20]{banos_window_2014}. Betrachtet wurden hierbei jedoch auch Ansätze, welche dedizierte Akzelerometer-Systeme mit potenziell höheren Abtastraten und Signalqualitäten bieten, im Vergleich zu Smartphone-Akzelerometern. Die Wartezeit, welche durch die Gleitfenstermethode zu Beginn einer kontinuierlichen Klassifikation notwendig wäre, befindet sich mit diesen Fensterlängen in einem akzeptablen Bereich. Auch mit einer geringen Latenz muss jedoch die Echtzeitfähigkeit gewährleistet sein. Einen direkten Einfluss auf die Echtzeitfähigkeit hat die Dauer, welche die Verkehrsmittelerkennung für die Berechnung der Inferenz benötigt. Die Kernkomponenten Vorverarbeitung, Klassifikation und Nachverarbeitung tragen jeweils, je nach Berechnungskomplexität, zur Gesamtinferenzzeit bei.

\subsection{Inferenzzeit bezüglich \nameref{qa:inf}}

\cite[S. 6]{zebin_design_2019} führten eine quantitative Analyse der Modell-Inferenzzeit durch. Traditionelle ML-Modelle unterschieden sich in deren Betrachtung nicht signifikant von \acrshort{cnn}-Modellen und lagen bei $3,53ms$ bis $12,6ms$. Ob diese Daten auf einem Computer oder auf einem Smartphone erfasst wurden, wurde nicht angegeben. Zum Teil sind die beschriebenen Daten auch widersprüchlich, in \cite[S. 8]{zebin_design_2019} übersteigen in der Analyse der Ausführungszeit einzelner Verarbeitungsschritte des Modells bereits einzelne Schritte mit bis zu $244,813ms$ die an anderer Stelle angegebene Gesamtinferenzzeit des Modells. Abgesehen davon konnte die kürzeste Inferenzzeit mit einem \acrshort{cnn} erreicht werden, welches gleichzeitig auch die höchste Ergebnisqualität, in Form einer Accuracy von $96,4\%$, produzierte. Die Ergebnisse decken sich mit den Analysen in \cite[S. 6]{ravi_deep_2016}, welche auch ein \acrshort{cnn}-Modell testeten. Hierbei lag die Inferenzzeit bei $5,7ms$ bis $14,9ms$, wobei explizit angegeben wurde, dass die Unterschiede durch Tests auf unterschiedlichen Smartphones und Prozessoren entstanden. Die gemessenen Inferenzzeiten sind vergleichbar mit der Verarbeitungsdauer einer Fourier-Transformation, welche in denselben experimentellen Bedingungen bei $5,4ms$ bis $42ms$ gemessen wurde \cite[S. 6]{ravi_deep_2016}. Allein diese Arbeit betrachtend, scheinen ML-Modelle also keinen besonderen Einfluss auf die Gesamtinferenzzeit zu besitzen. In anderen \acrshort{har}-Arbeiten zeigen sich jedoch Ausreißer. Das auf \acrshort{lstm} basierende \acrshort{rnn}-Modell aus \cite[S. 9]{mairittha_-device_2019} benötigte beispielsweise eine Inferenzzeit von $2846ms$ und damit wesentlich mehr als die langsamste \acrshort{cnn}-Konfiguration aus \cite{ravi_deep_2016}. Hierbei ist fraglich, ob eventuell eine suboptimale Implementation vorliegt, denn in einem späteren Ansatz konnten \cite[S. 10]{mairittha_-device_2021} die Inferenzzeit auf $10,6ms$ für ein ähnliches \acrshort{lstm}-Modell senken. Insgesamt lässt sich daher trotz der teilweise vorhandenen experimentellen Unklarheiten vermuten, dass sowohl \acrshort{rnn}-Modelle, als auch \acrshort{cnn}-Modelle und traditionelle Modelle vergleichsweise gute Inferenzzeiten erreichen können. Ein bestmöglicher Ansatz ist allein anhand der obigen Betrachtungen jedoch nicht sichtbar.

\subsection{Energieverbrauch bezüglich \nameref{qa:e}}

Ein weiterer Faktor, der bei serverseitigen Verkehrsmittelerkennungen wie den Movebis-Ansätzen aus \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} vernachlässigbar ist, jedoch bei Smartphone-Anwendungen einen essenziellen Faktor darstellt, ist der Energieverbrauch. \cite{ravi_deep_2016,nutter_design_2018,mairittha_improving_2020,mairittha_-device_2021} geben keine Auskunft über diesen Ressourcenparameter. \cite{mairittha_-device_2019} beschreiben, dass deren weiter oben bereits erwähntes \acrshort{rnn}-Modell mit einer Inferenzzeit von $2846ms$ den durchschnittlichen Energieverbrauch der Applikation, im Vergleich zu einer Applikation ohne Klassifikator, um $5\%$ erhöht. Das System aus \cite{mairittha_-device_2019} verwendet jedoch auch eine Fensterlänge von $60s$, welche aus bereits genannten Gründen problematisch für die Nutzbarkeit ist und daher mit der Grundanforderung an die Latenz kollidiert. Im Zuge dessen bleiben auch die relevanten experimentellen Bedingungen zur Ermittlung des Energieverbrauchs unklar, wie zum Beispiel, ob kontinuierlich nach Ablauf von $60s$ klassifiziert wurde oder nur einmal alle $60s$. Das mit TensorFlow Lite trainierte, optimierte und exportierte \acrshort{cnn} aus \cite[S. 9]{zebin_design_2019} verbrauchte bei dessen experimenteller Evaluation $39,30mW$ in einem kontinuierlichen Inferenzmodus. Neben der Inferenzzeit, bei der anhand der Nutzbarkeit eine Anforderung für die größtmögliche Latenz (wenige Sekunden) definiert werden konnte, ist auch die Definition eines \enquote{guten} Energieverbrauchs problematisch. Hierbei kann beispielsweise einfließen, ob es sich um einen kurzzeitigen Peak-Energieverbrauch, oder um einen langfristigen Durchschnittsverbrauch handelt. Da sich der Energieverbrauch durch die Fläche unter der Kurve bildet, welche Laufdauer mit dem Momentanverbrauch in Verbindung setzt, sind vor allem langfristige Lasten problematisch und sollten vorrangig betrachtet werden. Eine Möglichkeit der Betrachtung der Akzeptierbarkeit eines Energieverbrauchs liegt in dem Vergleich mit dem Verbrauch anderer, alltäglicher Applikationen. \cite[S. 9]{zebin_design_2019} setzen dies beispielsweise in Relation mit der Youtube-App, welche $116mW$ verbrauche, und schlussfolgern, dass $39,30mW$ entsprechend einen akzeptablen kontinuierlichen Energieaufwand darstelle.

\subsection{Speicherverbrauch bezüglich \nameref{qa:s}}

Soll ein ML-Modell auf Smartphones lauffähig sein, so stellt neben der Inferenzzeit und dem Energieverbrauch noch der Speicherverbrauch einen zentralen Ressourcenparameter dar, der zur Ermittlung des Tradeoffs betrachtet werden muss. Bei ML-Modellen ist es leicht, durch Erhöhung der Größe der internen Repräsentation komplexe Aufgaben besser lösen zu können, sofern kein Overfitting vorliegt und ausreichend qualitativ hochwertige Daten zur Verfügung stehen. Die betrachteten Modelle aus dem Anwendungsbereich Smartphone-\acrshort{har} können jedoch auch mit Modellgrößen von wenigen hunderten von Kilobyte zufriedenstellende Ergebnisse erzielen. \cite{mairittha_-device_2021} zeigen beispielsweise mit einem \acrshort{lstm}-\acrshort{rnn} im Umfang von $520kB$, dass auch kleine Modelle für die Aktivitätserkennung geeignet sind. Hierbei hängt der Speicherverbrauch indirekt auch mit der Inferenzzeit und dem Energieverbrauch zusammen, über die verbrauchte Zeit und Energie bei der Prozessierung der gespeicherten Parameter. Durch Optimierung, in Form einer Verkleinerung des Modells, können somit auch die Inferenzzeit und der Energieverbrauch gesenkt werden. \cite[S. 10]{zebin_design_2019} konnten allein durch 8-Bit-Quantisierung eines 32-Bit-Modells dessen Größe um das bis zu $7,62$-Fache reduzieren, bei einem Verlust der Accuracy von $96,4\%$ auf $93,6\%$. \cite{mairittha_-device_2021} verwenden Pruning zur Verkleinerung des Netzwerks, hierbei konnte eine $1,6$-Fache Reduktion bei einem vernachlässigbarem Verlust der Accuracy von $0,18\%$, sowie eine weitere Reduktion um das $1,7$-Fache durch verlustfreie Kompression erreicht werden.

\subsection{Ergebnisqualität}

\begin{table}[h]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{llll}
  \toprule
  Arbeit & Modell & Acc. ($\%$) & Evaluationsgrundlage \\
  \midrule
    \multirow{3}{*}{\cite{ravi_deep_2016}} & Flaches \acrshort{cnn} & $95,1$ & 4,4 Mio. Samples, 7 Aktivitäten \cite{esprit_team_of_the_hamlyn_centre_and_the_institute_of_global_health_innovation_imperial_college_london_activemiles_2012} \\
     & Flaches \acrshort{cnn} & $98,2$ & 1,1 Mio. Samples, 6 Aktivitäten \cite{wisdm_lab_department_of_computer__information_science_fordham_university_bronx_ny_wisdm_2013} \\
     & Flaches \acrshort{cnn} & $95,9$ & 1,9 Mio. Samples, 2 Aktivitäten \cite{daniel_roggen_uci_2013} \\
    \midrule
    \multirow{5}{*}{\cite{zebin_design_2019}} & 4-Layer \acrshort{cnn} & $92$ & \multirow{5}{*}{1,3 Mio. Samples, 5 Aktivitäten, Eigener Datensatz} \\
     & Regularisiertes 4-Layer \acrshort{cnn} & $96,4$ & \\
     & Quantisiertes 4-Layer \acrshort{cnn} & $93,6$ & \\
     & Quadratische SVM & $93,4$ & \\
     & Multi-Layer-Perzeptron & $91$ & \\
    \midrule
    \cite{nutter_design_2018} & MobileNet & $>95$ & 29,1 Mio. Samples, 9 Aktivitäten \cite{garcia-gonzalez_public_2020} \\
    \midrule
    \cite{mairittha_-device_2019} & \acrshort{lstm}-\acrshort{rnn} & $97,5$ & 1,1 Mio. Samples, 6 Aktivitäten \cite{wisdm_lab_department_of_computer__information_science_fordham_university_bronx_ny_wisdm_2013}\\
    \cite{mairittha_improving_2020} & 3-Layer \acrshort{cnn} & $92$ & 6 Aktivitäten, Eigener Datensatz \\
    \cite{mairittha_-device_2021} & \acrshort{lstm}-\acrshort{rnn} & $97,5$ & 12 Aktivitäten, Eigener Datensatz \\
  \bottomrule
  \end{tabular}
  \caption{Vergleich der Accuracy verschiedener ML-Ansätze für \acrshort{har} auf Smartphones.}
  }
\end{table}\label{tab:har-vergleich}

Die erreichten Ergebnisqualitäten der HAR-Ansätze sind in \Cref{tab:har-vergleich} dargestellt. In der Tabelle bestätigt sich die eingangs erwähnte Analyse von \cite{demrozi_human_2020}, mit typischen Accuracy-Werten für den betrachteten Anwendungsfall. Je nach Datensatz, Teststrategie und den betrachteten Aktivitäten kann die Accuracy variieren, dies zeigt die starke Abhängigkeit der Ergebnisqualität von der Datengrundlage \cite{ravi_deep_2016}. Gleichzeitig kann bei \acrshort{cnn}s beobachtet werden, dass das Hinzufügen von Hidden Layers die Ergebnisqualität erhöht. \cite{zebin_design_2019} zeigen dies, indem sie verschiedene Anzahlen von Hidden Layers betrachten, beginnend bei einem einschichtigen \acrshort{cnn} mit $12,9\%$ geringerer Accuracy als ein vierschichtiges Modell. Gleichzeitig erhöht sich die Dauer des Trainings. Die beste Accuracy erzielten \cite{zebin_design_2019} mit dem vierschichtigen Modell, konfiguriert durch eine Filtergröße von $1\times2$ und einer Pooling-Größe von $1\times2$. \cite{zebin_design_2019} betrachteten auch traditionelle Modelle, welche analog zu \cite{demrozi_human_2020} nur marginal schlechter abschnitten als die betrachteten \acrshort{cnn}-Architekturen. \cite{nutter_design_2018} nutzten einen besonders umfangreichen Datensatz und konnten über das Transfer Learning von verschiedenen mobilen \acrshort{cnn}-Architekturen zeigen, dass hierdurch eine Accuracy von mehr als $95\%$ erreicht werden kann. Am besten schnitt hierbei das in \cite{howard_mobilenets_2017} konzeptionierte MobileNet ab, im Vergleich zu anderen auf dem ImageNet-Datensatz\footnote{\url{https://image-net.org/} (Abgerufen am 13.7.2021)} vortrainierten Netzwerken wie InceptionV3 \cite{szegedy_rethinking_2015}.

\section{\acrshort{vme} auf Smartphones}

In den vorigen Abschnitten wurde ergänzend zu den Movebis-Ansätzen gezeigt, wie sich verwandte \acrshort{har}-Ansätze in ihren, für den Tradeoff relevanten, Charakteristika unterscheiden. Dafür wurden Ansätze betrachtet, welche explizit für Smartphones konzipiert wurden. Diese Ansätze sind jedoch zur Erkennung von Aktivitäten wie \enquote{Gehen}, \enquote{Treppensteigen} oder \enquote{Stehen} gedacht und eignen sich damit in ihrer ursprünglichen Form nicht für die Verkehrsmittelerkennung. Insbesondere steht hierbei die Frage im Raum, ob eine Verkehrsmittelerkennung bezüglich der erforderlichen Komplexität mehr oder weniger Aufwand als \acrshort{har}-Ansätze erfordert, durch die Unterschiedlichkeiten in den betrachteten Signalverläufen. Diese Frage soll nachfolgend anhand von Arbeiten diskutiert werden, welche ebendiesen spezielleren Anwendungsfall betrachten. Hierbei soll auch darauf eingegangen werden, welche Modifikationen an den diskutierten Ansätzen zur \acrshort{har} auf Smartphones gegebenenfalls nötig wären.

\begin{table}[h]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{llllp{8cm}}
  \toprule
  Arbeit & Modell & Acc. ($\%$)\footnote{Accuracy-Metriken nach eigener Aussage der wissenschaftlichen Arbeiten.} & \nameref{qa:int} & Evaluationsgrundlage \\
  \midrule
  \cite{hemminki_accelerometer-based_2013} & Mehrere Decision Trees & $93,60$ & \cmark & Eigener Datensatz, 150h Aufzeichnung, 6 Verkehrsmittel\\
  \cite{reddy_using_2010} & \acrshort{dt} und \acrshort{hmm} & $93,6$ & \cmark & Eigener Datensatz, 120h Aufzeichnung, 6 Verkehrsmittel \\
  \cite{liono_inferring_2018} & Random Forest & $91$ & \cmark & Crowdsignal-Datensatz \cite{welbourne_crowdsignals_2014}, $>$2000h Aufzeichnung (5h öffentlich verfügbar), 10 Verkehrsmittel \\
  \midrule
  \cite{fang_transportation_2016} & SVM & $86$ & \xmark & HTC-Datensatz \cite{yu_big_2014}, 8311h Aufzeichnung (1249h öffentlich verfügbar), 10 Verkehrsmittel \\
  \cite{liang_convolutional_2017} & 6-Layer-\acrshort{cnn} & $94,48$ & \xmark & Eigener Datensetz, 14h Aufzeichnung, 7 Verkehrsmittel \\
  \midrule
  \cite{friedrich_transportation_2019} & \acrshort{lstm}-\acrshort{rnn} & $84,22$ & \xmark & \multirow{4}{*}{\parbox{5cm}{SHL-Datensatz \cite{gjoreski_versatile_2017}, 700h Aufzeichnung, 8 Verkehrsmittel}} \\
  \cite{friedrich_combining_2020} & \acrshort{lstm} und \acrshort{cnn} & $99,74$ & \xmark & \\
  \cite{antar_comparative_2018} & Random Forest & $92$ & \xmark & \\
  \cite{jeyakumar_deep_2018} & \acrshort{lstm} & $98,1$ & \xmark & \\
  \bottomrule
  \end{tabular}\label{tab:vme-vergleich}
  \caption[Vergleich verschiedener ML-Ansätze für \acrshort{vme}.]{Vergleich verschiedener ML-Ansätze für \acrshort{vme}. Legende: Betrachtung von Integrierbarkeit auf Smartphones \nameref{qa:int}.}
  }
\end{table}

Die Accuracy der betrachteten Ansätze ist mit einem Median von $93.6\%$ vergleichbar mit der durchschnittlichen Accuracy von \acrshort{har}-Ansätzen nach \cite{demrozi_human_2020}. Besonders gut schneiden hierbei aktuelle serverseitigen Deep-Learning-Ansätze ab, wie \cite{friedrich_combining_2020}. Während frühe Ansätze wie \cite{reddy_using_2010} noch hauptsächlich auf GNSS-Signalen beruht und Akzelerometersignale als Hilfsmittel evaluiert, gehen aktuelle Ansätze wegen der inhärenten Probleme der GNSS-Signalerfassung dazu über, ausschließlich lokale kinematische Messparameter für die Klassifikation zu verwenden \cite{friedrich_combining_2020}. \cite{friedrich_combining_2020} zeigen, dass bei Verwendung eines Deep-Learning-Ansatzes herausragende Ergebnisse bei minimaler Vorverarbeitung der Daten erreicht werden können. \cite{friedrich_combining_2020} wenden lediglich ein Tiefpassfilter bei $25Hz$ an, sowie einen weiteren Vorverarbeitungsschritt zur Skalierung. Anschließend werden die aufgezeichneten Daten direkt in ein neuronales Netzwerk überführt, welches hauptsächlich aus \acrshort{lstm}-Schichten und Convolution-Schichten zusammengesetzt ist.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 577 239]{friedrich.pdf}
  \caption[Architektur des neuronalen Netzes aus \cite{friedrich_combining_2020}.]{Architektur des neuronalen Netzes aus \cite{friedrich_combining_2020}, welches eine herausragende Ergebnisqualität erzielt. Abkürzungen: Beschleunigung (Acc), Gravitationsvektor (Gra), Winkelauslenkung (Gyr), Lineare Beschleunigung (LAcc), Magnetfeld (Mag), Orientierung (Ori), Luftdruck (Pre)}\label{fig:friedrich}
\end{figure}

\Cref{fig:friedrich} zeigt die Architektur des neuronalen Netzes aus \cite{friedrich_combining_2020}. Die Inputs setzen sich zusammen aus den Hardware-Signalen des Akzelerometer, Gyrosensor, Magnetometer und Barometer, sowie aus durch Software aufbereiteten Signalen lineare Beschleunigung, Gravitation und Orientierung. Auffallend ist bei der Architektur aus \cite{friedrich_combining_2020}, dass sie im Vergleich zu anderen Ansätzen besonders tief ist. \cite[S. 5]{zebin_design_2019} zeigen, dass sich durch eine tiefere Architektur höhere Accuracy-Werte erzielen lassen. Gleichzeitig erhöht sich durch eine solche Architektur auch die Ressourcenintensität. Für das Training der Architektur nutzten \cite{friedrich_combining_2020} Server aus einem spezialisierten High-Performance-Cluster. Ausreichend Daten standen hierbei durch Nutzung des Datensatzes aus \cite{gjoreski_versatile_2017} zur Verfügung, jedoch benötigte das Netzwerk insgesamt mehr als 3 Tage zum Training. Auf der spezialisierten Hardware des HPC konnte das Netzwerk in 146 Sekunden den Testdatensatz klassifizieren, welcher aus 57573 Samples mit $N_{Sample}=500$ bestand, wobei jeweils 5 Sekunden abgetastet wurden. Dies entspricht rechnerisch einer Inferenzzeit von $2,5ms$, die sich aber in dieser Form nicht auf Smartphones übertragen lässt und nur als Richtwert dient. \cite{friedrich_combining_2020} zeigen mit ihrem State-of-the-Art-Ansatz, dass der Verzicht auf GNSS-Sensorinformationen nicht zwangsweise einen Tradeoff mit der erreichbaren Accuracy darstellt. Im Gegensatz erreichen \cite{friedrich_combining_2020}, sofern die geäußerten Accuracy-Werte als mit anderen Ansätzen vergleichbar gewertet werden können, einen der besten Accuracy-Werte in der Forschungsdomäne. Komplexe Vorverarbeitungsmethoden wie filterbasierte AHRS scheinen nach den Ergebnissen aus \cite{friedrich_combining_2020} ebenfalls nicht benötigt zu werden, zumindest mit deren besonders tiefen getesteten Modell.

\subsection{Portabilität}

Auch im Bereich der \acrshort{vme} auf Smartphones lassen sich viele Arbeiten identifizieren, welche zwar gute bis sehr gute Ergebnisqualitäten erzielen, jedoch nicht unbedingt für Smartphones geeignet sind. Da die meisten Forschungsansätze aus dem Bereich VME und HAR lediglich auf eine Maximierung der Accuracy orientiert sind, führt dies auch durch die Verfügbarkeit großer Forschungsdatensätze teils zu großen Modellen wie in \cite{friedrich_combining_2020}. Es lässt sich vermuten, dass diese Modelle auch gleichzeitig viel Speicher und Energie verbrauchen. Während diese Modelle auf hochparallelisierbarer Serverhardware wie großen GPUs oder TPUs problemlos ausgeführt werden können, stellen Smartphones und deren Anwendungskontext andere Anforderungen. Nur wenige der betrachteten \acrshort{vme}-Ansätze betrachten die Anwendungsfälle, die sich aus einer Portierung der Ansätze auf Smartphones ergeben, und versuchen den Ressourcenverbrauch möglichst gering zu halten. In den meisten Arbeiten wird das Smartphone lediglich als nützliche Datenquelle betrachtet, nicht jedoch als Plattform für die Ausführung der Klassifikation. \cite{hemminki_accelerometer-based_2013} bieten ein auf Smartphones ausführbares Konzept mit einem vergleichsweise flachen Klassifikator-Ansatz. Im Zuge der Evaluation wurde hierbei auch der Energieverbrauch analysiert, im Vergleich mit \cite{wang_accelerometer_2010} und \cite{reddy_using_2010}. Die traditionellen ML-Modelle besitzen eine Leistungsaufnahme von $50mW$ bis $240mW$. Es handelt sich jedoch um gerätespezifische Analysen, die zudem dem hardwaretechnischen und softwaretechnischen Fortschritt des vergangenen Jahrzehnts unterliegen. Dennoch können die Ansätze als Baseline-Modell dienen. Als solche sind die Modelle sehr einfach und können als Referenz in der Evaluation dienen oder durch Erweiterung sukzessiv verbessert werden. \cite{hemminki_accelerometer-based_2013} verwendet beispielsweise mehrere Decision Trees, um Verkehrsmittel zu unterscheiden.

\subsection{Hierarchische Klassifikation}

Die meisten ML-Modelle der verwandten Arbeiten können als monolithisch betrachtet werden und geben als solche einen Vektor aus, anhand dessen alle Verkehrsmittel abgelesen werden können. Wie in der Grundlagensektion jedoch diskutiert, ist durch Reduktion der zu klassifizierenden Labels eine Verbesserung der Ergebnisqualität oder eine Verkleinerung der internen Repräsentation des Modells möglich. Daher verwendet \cite{hemminki_accelerometer-based_2013} einen hierarchischen Ansatz, bei dem ein Segment zunächst über einen Decision Tree mit geringer Tiefe in \enquote{Zu Fuß} oder \enquote{Im Fahrzeug} unterschieden wird. Außerdem wird durch einen weiteren Decision Tree detektiert, ob sich der Nutzer im Moment fortbewegt. Wird die Klasse \enquote{Im Fahrzeug} gefunden, so wird diese über einen eigenen Decision Tree nochmals in die einzelnen Fahrzeuge unterschieden. Die Unterscheidung zwischen \enquote{Zu Fuß} und \enquote{Im Fahrzeug} scheint verhältnismäßig einfach zu sein, mit einer Accuracy von $99\%$. Die Unterscheidung der Fahrzeuge wiederum erreicht nur eine Präzision von $80\%$. Hierbei wird also die Betrachtung genutzt, dass verschiedene Verkehrsmittel einfacher zu unterscheiden sind, als andere. Eine solche hierarchische Erkennung findet sich beispielsweise auch in dem aktuelleren Ansatz \cite{liono_inferring_2018} wieder.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 806 345]{generated/conf-mat-rw.pdf}
  \caption[Vergleich der Konfusionsmatrizen von \cite{friedrich_combining_2020} und \cite{hemminki_accelerometer-based_2013}, einem explizit auf Smartphones lauffähigen Konzept.]{Vergleich der Konfusionsmatrizen von \cite{friedrich_combining_2020} (links) und \cite{hemminki_accelerometer-based_2013} (rechts), einem explizit auf Smartphones lauffähigen Konzept.}\label{fig:vme-s-vergleich}
\end{figure}

In \Cref{fig:vme-s-vergleich} werden die Ansätze \cite{hemminki_accelerometer-based_2013} und \cite{friedrich_combining_2020} verglichen. Dies verdeutlicht noch einmal die herausragende Ergebnisqualität des serverseitigen Deep-Learning-Modells im Vergleich zum Modell aus \cite{hemminki_accelerometer-based_2013}, welches als Referenz für On-Device-Ansätze dienen kann. Relativ am häufigsten werden hierbei auch die intuitiv ähnlichsten Verkehrsmittel verwechselt. In \cite{friedrich_combining_2020} sind dies die Paare $(Zug, UBahn) = 130$, $(Zug, Bus) = 65$, aber auch $(Zug, Stehen) = 78$, wobei dies vermutlich mit den längeren Standzeiten an Haltepunkten von Zügen und der damit verbundenen Aufzeichnung von Segmenten zusammenhängt, ähnlich zu \cite{hemminki_accelerometer-based_2013}. In \cite{hemminki_accelerometer-based_2013} werden am häufigsten die Klassen $(Zug, Metro) = 10879$ und $(Bus, Tram) = 10000$ verwechselt, wie in \cite{friedrich_combining_2020}. Diese Erkenntnis besitzt für diese Arbeit zwei wichtige Bedeutungen. Zum einen können leichter zu unterscheidende Verkehrsmittel mit kleineren Modellen hierarchisch in Gruppen von Verkehrsmitteln klassifiziert werden. Zum anderen können schwer voneinander zu unterscheidende Verkehrsmittel zusammengefasst werden, um die Accuracy des Gesamtsystems zu verbessern, sofern beispielsweise keine Unterscheidung von Zug und Metro notwendig ist.

\subsection{Tradeoff}

Die Unterschiede zwischen \cite{hemminki_accelerometer-based_2013} und \cite{friedrich_combining_2020} zeigen, dass prinzipiell eine große Spanne von möglichen Ressourcenverbräuchen und Ergebnisqualitäten existiert. Durch verschiedene Optimierungsmethoden wie Pruning, Kompression oder architekturelle Restrukturierung kann der Ressourcenverbrauch generell reduziert werden \cite{han_learning_2015,han_deep_2016,iandola_squeezenet_2016,howard_mobilenets_2017}. Meist liegt die Maxime der jeweiligen Arbeiten bei einer Maximierung der Ergebnisqualität und die Ressourcenverbräuche geraten in den Hintergrund der Betrachtung. Sowohl für die ressourcenlimitierte Ausführung auf Smartphones, als auch für die generelle Nachhaltigkeit \cite{strubell_energy_2019} von ML-Systemen ist es jedoch von hoher Bedeutung, wie die Vergrößerung des Modells, dessen Energieverbrauch und sekundäre Parameter wie Speicherverbrauch und Inferenzzeit miteinander korellieren und in welcher Form kausale Zusammenhänge vorliegen. Vermuten lässt sich das Folgende -- mit dem Wissen, dass beispielsweise ab einem bestimmten Punkt bei einer Verdopplung der Modellgröße nur ein marginaler Zugewinn an Ergebnisqualität zu erwarten wäre, ließen sich effizientere und nachhaltigere Modelle erstellen. \cite{brownlee_exploring_2021,brownlee_search-based_2017} betrachten vor diesem Hintergrund, dass die Relation zwischen Accuracy und Energieverbrauch tatsächlich einen Tradeoff darstellt. Das Finden der bestmöglichen Lösung ist somit äquivalent mit dem Finden eines sogenannten \textit{Pareto-Optimums}\footnote{\url{https://de.wikipedia.org/wiki/Pareto-Optimum} (Abgerufen am 16.7.2021)}. Allein anhand der in den vorigen Sektionen gezeigten Varianten ist dies nicht möglich, denn die Vergleichbarkeit der Ansätze ist wegen der Unterschiedlichkeit der jeweiligen Architekturen und Anwendungsfälle stark eingeschränkt.

\subsubsection{Rastersuche}

Die Anforderungsanalyse ergab, dass der Tradeoff durch Austausch der Datenvorverarbeitung und der Modelle analysiert werden kann. \cite{brownlee_exploring_2021} zeigen eine konkrete Methodik, wie dies umgesetzt werden kann. Idee ist die \textit{Rastersuche} durch Variation einer konkreten Architektur und deren Hyperparameter. Das in \cite{brownlee_exploring_2021} selektierte und variierte Modell ist das MLP, ähnlich zur Basisarchitektur des Movebis-Ansatzes \cite{matusek_anwendung_2019}. \cite{brownlee_exploring_2021} fokussieren sich dabei auf das Hyperparameter Tuning. Eine wichtige Erkenntnis der in \cite{brownlee_exploring_2021} durchgeführten Rastersuche ist, dass ein Energie-Accuracy-Tradeoff grundsätzlich besteht, jedoch stark vom betrachteten Datensatz abhängt. Bei den Ergebnissen besteht keine lineare Korellation, mit steigender Accuracy können Modelle auch zwischenzeitlich sparsamer werden. Außerdem zeigen sich in \cite{brownlee_exploring_2021} als \textit{Knee-Point} bezeichnete Sprünge in der Energieintensität, beispielsweise konnte der Energieverbrauch eines Modells um $77\%$ reduziert werden, während dessen Accuracy lediglich um $1,1\%$ auf $93,2\%$ fiel. Diese Knee-Points können also als Grundlage herangezogen werden, um Modelle zu finden, welche einen vergleichsweise guten Kompromiss zwischen Energie und Accuracy bieten. Als zentrale, zum Energieverbrauch kontribuierende Faktoren konnten außerdem unter anderem die Dimensionierung der Hidden Layer (hohe positive Korellation) und der genutzte Optimierungsalgorithmus des Gradientenabstiegsverfahren identifiziert werden.

\subsubsection{Modellierung von Energieverbrauch über CPU-Zeit}

\begin{figure}[h]
  \captionsetup[subfigure]{justification=centering}
  \centering
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=.9\linewidth, bb=0 0 283 212]{brownlee/plots/mortgage_MLP_metrics_testing_cputime_vs_cpuenergy.pdf}
    \caption{Mortgage Datensatz aus \cite{brownlee_exploring_2021}, Inferenz, MLP.}
    \label{fig:brownlee-1}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=.9\linewidth, bb=0 0 283 212]{brownlee/plots/iris_MLP_metrics_more_training_cputime_vs_cpuenergy.pdf}
    \caption{UCI Iris Datensatz, Training, MLP.}
    \label{fig:brownlee-2}
  \end{subfigure}
  \caption[Korrelation zwischen CPU-Zeit und Energieverbrauch.]{Korrelation zwischen CPU-Zeit und Energieverbrauch. Abbildungen und Evaluation aus \cite{brownlee_exploring_2021}.}\label{fig:brownlee}
\end{figure}

Ein weiteres zentrales Ergebnis aus \cite{brownlee_exploring_2021}, welches methodisch für diese Arbeit relevant ist, besteht darin, dass die in Anspruch genommene CPU-Zeit der Modelle (Training, Inferenzzeit) annähernd linear mit der verbrauchten Energie korelliert, auch wenn keine fehlerfreie Substitution beider Ressourcenparameter möglich ist. Beobachtet wurden die in \Cref{fig:brownlee} sichtbaren Sprünge im Energieverbrauch in der Last. Diese resultieren aus komplexen internen Management-Prozesse der CPU und der umliegenden Komponenten (Caches, RAM, Festspeicher). Eine Modellierung des Energieverbrauchs über die CPU-Zeit ist also möglich, sollte jedoch mit Hinblick auf diese Erkenntnis interpretiert werden.

\subsubsection{Plattformspezifische Modell-Adaption}

\cite{dai_chamnet_2019} kommen zu dem Schluss, dass die Performanz von ML-Modellen auch stark von der verwendeten Plattform und der spezialisierten Hardware-Komponenten abhängt. \cite{dai_chamnet_2019} schlagen einen erweiterten Ansatz zur einfachen Rastersuche vor. Hierbei werden ML-Modelle genutzt, um den Ressourcenverbrauch (Latenz und Energie) zu einer bestimmten Modellkonfiguration auf einem Zielsystem vorherzusagen. Hierfür müssen jedoch im Voraus Modelle durch Variation getestet werden und die entsprechenden Metriken aufgezeichnet werden. Anschließend wird ein regressives Modell auf den erhobenen Daten trainiert. Das regressive Modell sagt den Tradeoff voraus und kann genutzt werden, um Näherungsweise, je nach Abweichung des Modells vom tatsächlichen Tradeoff, das Pareto-Optimum zu finden. Auch hier ist es jedoch wieder essenziell, dass von einer zentralen Modellarchitektur ausgegangen wird, um diese zu variieren. Es können also nicht mehrere Architekturen gleichzeitig und vergleichend einbezogen werden, beispielsweise Random Forests und \acrshort{cnn}s. Die gefundene Pareto-optimale Lösung ist also nicht zwangsweise global die bestmögliche Lösung bezüglich des Tradeoffs.

\section{Abgrenzung}

In den vorigen Sektionen wurden verschiedene ML-Ansätze vorgestellt, welche eine Relevanz für diese Arbeit besitzen. Hierbei stehen die Movebis-Ansätze \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} im Vordergrund. Es existiert eine Vielzahl von ML-Ansätzen zur Verkehrsmittelerkennung, mit unterschiedlichen Konzeptbestandteilen und Schwerpunkten. In der \Cref{tab:abgrenzung} sind die diskutierten Arbeiten nochmals schwerpunktweise gegenübergestellt.

\begin{longtable}[h]{@{\extracolsep{\fill}}lcccccccc}
  \toprule
   & & \multicolumn{3}{c}{\nameref{qa:r}} & & & \\
  \cmidrule(lr){3-5}
  Arbeit &
    \nameref{qa:vme} & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \nameref{qa:e} & % - Betrachtung des Energieverbrauchs qa:e
    \nameref{qa:inf} & % - Betrachtung der Inferenzzeit qa:inf
    \nameref{qa:s} & % - Betrachtung des Speicherverbrauchs qa:s
    \nameref{qa:q} & % - Betrachtung der Ergebnisqualität qa:q
    \nameref{qa:b} & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \nameref{qa:o} & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \nameref{qa:int} % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \midrule
  \endhead
  \multicolumn{9}{l}{Serverseitige Movebis-Ansätze zur Verkehrsmittelerkennung} \\
  \midrule
  \cite{matusek_anwendung_2019} &
     \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
     & % - Betrachtung des Energieverbrauchs qa:e
     & % - Betrachtung der Inferenzzeit qa:inf
     & % - Betrachtung des Speicherverbrauchs qa:s
     \cmark & % - Betrachtung der Ergebnisqualität qa:q
     & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
     & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
     % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{werner_kontinuierliche_2020} &
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \omark\footnote{\label{fn:pp}Nur Optimierung durch Postprocessing.} & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{stojanov_continuous_2020} &
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \omark\footnoteref{fn:pp} & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \midrule
  \multicolumn{9}{l}{Smartphone-orientierte Ansätze zur Verkehrsmittelerkennung} \\
  \midrule
  \cite{hemminki_accelerometer-based_2013} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \cmark & % - Betrachtung des Energieverbrauchs qa:e
    \omark\footnote{\label{fn:ie}Nur indirekte Messung über Energieverbrauch und CPU-Zeit.}& % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{reddy_using_2010} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \omark\footnote{\label{fn:fe}Fokus auf Energieverbrauch der Datenakquise und Sensorsysteme.}& % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \cmark & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{liono_inferring_2018} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \midrule
  \multicolumn{9}{l}{Accuracy-maximierende Ansätze zur Verkehrsmittelerkennung} \\
  \midrule
  \cite{fang_transportation_2016} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    \cmark & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    \omark\footnote{\label{fn:mt}Nur grober Vergleich der Modellgrößen, keine Modellierung des Tradeoffs.} & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{liang_convolutional_2017} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{friedrich_transportation_2019} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    \omark\footnote{\label{fn:tp}Nur Illustration der trainierbaren Parameter eines einzelnen Modellbestandteils.} & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{friedrich_combining_2020} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{antar_comparative_2018} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{jeyakumar_deep_2018} & % - Arbeit
    \cmark & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \midrule
  \multicolumn{9}{l}{Smartphone-orientierte Human Activity Recognition} \\
  \midrule
  \cite{ravi_deep_2016} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{zebin_design_2019} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \cmark & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    \cmark & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    \omark\footnote{Nur Betrachtung der Auswirkungen von Optimierungsmethoden auf Speicherverbrauch, keine Modellierung des Tradeoffs.}& % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \cmark & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{nutter_design_2018} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    \cmark & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{mairittha_-device_2019} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \cmark & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{mairittha_improving_2020} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    & % - Betrachtung des Energieverbrauchs qa:e
    & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{mairittha_-device_2021} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \cmark & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    \cmark & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \cmark & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \midrule
  \multicolumn{9}{l}{Tradeoff zwischen Ressourcenverbrauch und Ergebnisqualität} \\
  \midrule
  \cite{brownlee_exploring_2021} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \cmark & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    \cmark & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \cite{dai_chamnet_2019} & % - Arbeit
    & % - Betrachtung von Verkehrsmittelklassifikation qa:vme
    \cmark & % - Betrachtung des Energieverbrauchs qa:e
    \cmark & % - Betrachtung der Inferenzzeit qa:inf
    & % - Betrachtung des Speicherverbrauchs qa:s
    \cmark & % - Betrachtung der Ergebnisqualität qa:q
    \cmark & % - Ermittlung des bestmöglichen Modells aus einer Selektion qa:b
    \cmark & % - Betrachtung von verschiedenen Optimierungsmöglichkeiten qa:o
    \cmark % - Portierbarkeit auf Smartphones als Konzeptbestandteil qa:int
    \\
  \bottomrule
  \caption[Übersicht über die Betrachtung der Schwerpunkte dieser Arbeit durch verwandte Arbeiten.]{Übersicht über die Betrachtung der Schwerpunkte dieser Arbeit durch verwandte Arbeiten. Legende: Betrachtung einer Verkehrsmittelklassifikation \nameref{qa:vme}, Ressourcenparameter \nameref{qa:r} wie Energieverbrauch \nameref{qa:e}, Inferenzzeit \nameref{qa:inf} und Speicherverbrauch \nameref{qa:s}, Betrachtung der Ergebnisqualität \nameref{qa:q} und Ermittlung eines bestmöglichen Modells bezüglich des Tradeoffs \nameref{qa:b}, Betrachtung von verschiedenen Optimierungsmöglichkeiten \nameref{qa:o} und Integrierbarkeit auf Smartphones \nameref{qa:int}.}
  \label{tab:abgrenzung}
\end{longtable}

Serverseitige Ansätze wie die Movebis-Ansätze \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} geben den Ausgangspunkt für diese Arbeit, betrachten aber weder konkrete Ressourcenparameter, noch orientieren sich an der Portierbarkeit auf Smartphones. Weitere Forschungsansätze dienen der Ergänzung des Gesamtbildes der für diese Arbeit relevanten Forschungsfragen. Die betrachteten smartphone-orientierten Ansätze zur Verkehrsmittelerkennung \cite{hemminki_accelerometer-based_2013,reddy_using_2010} zeigen teils, wie Ressourcenparameter als Anforderung für die Portierbarkeit berücksichtigt werden können. \cite{liono_inferring_2018} betrachtet keinen der identifizierten Ressourcenparameter und fokussiert sich auf die hierarchische Kontextklassifikation. \cite{hemminki_accelerometer-based_2013,reddy_using_2010,liono_inferring_2018} bieten auf Smartphones portierbare Lösungen zur Verkehrsmittelerkennung, hierbei wird jedoch der Tradeoff zwischen Ergebnisqualität und Ressourcenverbrauch nicht einbezogen. \cite{fang_transportation_2016,liang_convolutional_2017,friedrich_transportation_2019,friedrich_combining_2020,antar_comparative_2018,jeyakumar_deep_2018} maximieren die Ergebnisqualität, legen jedoch keinen Fokus auf Effizienz oder die Portierbarkeit auf Smartphones. Für Modelle wie aus \cite{friedrich_combining_2020} ist es fraglich, ob diese überhaupt durch ihre Größe portiert werden könnten. HAR-Ansätze wie \cite{ravi_deep_2016,zebin_design_2019,nutter_design_2018,mairittha_-device_2019,mairittha_improving_2020,mairittha_-device_2021}, welche die Portierbarkeit berücksichtigen, setzen die Ergebnisgüte zwar mit dem Ressourcenverbrauch in Relation, eine konkrete Modellierung des Tradeoffs oder gar die Suche nach einer Pareto-optimalen Lösung wird jedoch von keiner der Arbeiten gezeigt. Die Modellierung des Tradeoffs betrachten \cite{brownlee_exploring_2021,dai_chamnet_2019} ausführlich, hierbei liegt jedoch kein Fokus auf HAR oder VME. Durch die starke Abhängigkeit der Klassifikation vom Datensatz wäre es denkbar, dass die in \cite{brownlee_exploring_2021,dai_chamnet_2019} gefundenen Erkenntnisse nicht auf VME oder HAR übertragbar sind.

\section{Zusammenfassung}

Die Movebis-Ansätze dienen als Ausgangspunkt für diese Arbeit und zeigen konzeptuelle Kernaspekte auf, welche von dieser Arbeit weiter verwendet werden können. Dazu gehört beispielsweise die Verwendung der Gleitfenstermethode und der Segmentierung, die Elimination eines Gravitationsvektors und die Betrachtung der Fehlermuster unterschiedlicher Sensoren. Außerdem geben die Movebis-Ansätze bereits Modellarchitekturen (LSTM und CNN) vor, welche als Modellgrundlage für die weiteren Analysen dieser Arbeit dienen können.

Während traditionelle Ansätze mit weniger Trainingsdaten auskommen und die dazugehörigen Modelle typischerweise vergleichsweise klein sind, erreichen Deep-Learning-Ansätze wie die Movebis-Ansätze aus \cite{werner_kontinuierliche_2020,stojanov_continuous_2020} die besten Ergebnisqualitäten. Im Unterschied zu den Movebis-Ansätzen können Deep-Learning-Modelle jedoch auch mit einer weniger aufwändigen Datenvorverarbeitung eingesetzt werden. Die wenigen noch verbleibenden Verwechselungen des besten gefundenen Ansatzes liegen hierbei bei Verkehrsmitteln, welche sich in ihrer fahrdynamischen Charakteristik intuitiv nur geringfügig unterscheiden. Der beste gefundene Forschungsansatz verwendet zur Maximierung der Ergebnisqualität ein besonders großes Deep-Learning-Modell, welches für eine Ausführung auf Smartphones vermutlich nicht geeignet ist.

Die Trainierbarkeit solcher großen Deep-Learning-Modelle begründet sich in der Verfügbarkeit großer öffentlicher Datensätze wie dem SHL-Datensatz. Daher ist die am meisten verwendete Trainingsmethode auch das Supervised Learning und dessen Derivate. Die Ergebnisqualitäten von \acrshort{vme}-Ansätzen sind nach dem Training vergleichbar mit den Resultaten von \acrshort{har}-Ansätzen. Durch das in den Movebis-Ansätzen \cite{werner_kontinuierliche_2020,stojanov_continuous_2020} vorgestellte Postprocessing über Nachbar-Glättung kann die Ergebnisqualität signifikant erhöht werden. Im Unterschied zur Konfiguration der Movebis-Ansätze eignen sich zur Klassifikation hierfür auch Fensterlängen von unter einer Sekunde. Hierdurch kann die Latenz zu Beginn der Aufzeichnung analog zur Anforderungsanalyse minimal gehalten werden. Die Inferenzzeit ist zusätzlich typischerweise nicht länger als wenige Millisekunden und somit akzeptabel für eine Verkehrsmittelerkennung mit Anspruch an eine Echtzeitfähigkeit. Diese Metrik ist jedoch stark gerätespezifisch. Neben der Fensterlänge beeinflusst auch die Anzahl der zu klassifizierenden Labels durch Beschränkung des Anwendungsfalls die erreichbare Ergebnisqualität. Diese Erkenntnis kann in Form von hierarchischen Klassifizierungsverfahren genutzt werden.

Der Tradeoff von ML-Modellen wird üblicherweise als Relation von Energieverbrauch und Accuracy betrachtet. Während die Inferenz- und Trainingszeit über die CPU-Zeit eng mit dem Energieverbrauch verbunden ist, eignet sich die reine Messung der CPU-Zeit nur bedingt für eine fehlerfreie Projektion auf den Energieverbrauch. Auch der Speicherverbrauch hat über die Modellgröße einen negativen Einfluss auf den Energieverbrauch und kann durch Optimierungsmethoden reduziert werden. Unter dieser Betrachtung kann das Finden des bestmöglichen Ansatzes als Problem der Pareto-Optimierung betrachtet werden. Mithilfe einer Rastersuche über Hyperparameter können unterschiedliche Konfigurationen eines Modells getestet und die bestmögliche Lösung näherungsweise bestimmt werden. Durch Erstellung eines ML-Meta-Modells zur Abbildung der Relation zwischen Modellkonfiguration, Energieverbrauch und Accuracy kann die Granularität der Rastersuche durch Interpolation des so entstehenden Hyperraums mit einem globalen Optimum verbessert werden. Das globale Optimum dieses Modells ist eine Näherungslösung für die bestmögliche Konfiguration des initial analysierten ML-Modells.
