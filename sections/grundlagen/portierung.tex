% energie- und speicherverbrauch
% portierung: quantisierung, ...
% verwandte arbeiten analysieren
% technologien: coreml(tools), tensorflow lite, ...

\section{Portierung}\label{sec:portierung}

In den vorigen Abschnitten wurde diskutiert, wie Daten vorverarbeitet werden können, um anschließend durch Machine-Learning-Modelle klassifiziert zu werden. In \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} nutzen hierfür künstliche neuronale Netzwerke mit verschiedenen Architekturen (Feed-Forward-Netzwerk, RNN, CNN). Dieser Abschnitt soll diskutieren, wie die Konzepte nun für den Einsatz auf Smartphones portiert werden können und welche Faktoren hierbei zu beachten sind.

\subsection{Deployment-Modelle im Edge Computing}

Für einen Einsatz auf Smartphones müssen Machine-Learning-Modelle nicht zwingend auch auf diesem über eine App mitgeliefert oder installiert (engl. \textit{deploy}) werden. \cite[S. 13]{ota_deep_2017} separieren beispielsweise in zwei Deployment-Modelle.

\begin{itemize}
\item \textit{Client-Server-Deployment}, wobei die Inferenz des Machine-Learning-Modells auf dem Server-Backend geschieht.
\item \textit{Client-Only-Deployment}, wobei die Inferenz vom Smartphone selbst übernommen wird, das Machine-Learning-Modell also mit in der jeweiligen App integriert ist.
\end{itemize}

\noindent \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020}, welche mithilfe von künstlichen neuronalen Netzwerken das Filtern der STADTRADELN-\allowbreak Daten im Movebis-Projekt ermöglichen, gliedern sich analog zur obigen Taxonomie in das Client-Server-Deployment ein, wobei jedoch nur eine zeitlich entkoppelte unidirektionale Kommunikation stattfindet. Die Smartphone-App erhält kein direktes Feedback zum Ergebnis der serverseitigen Klassifikation und besitzt so auch keine Möglichkeit zur Adaption auf wichtige Ereignisse wie einen Wechsel des Verkehrsmittels. Vor diesem Hintergrund könnte es allgemeiner gefasst also vorteilhaft sein, die Inferenz vom Server so nah wie möglich an den Client (Smartphone) zu verschieben, unter anderem auch um die Netzwerklast zu verringern (durch Abschalten der Aufzeichnung infolge eines Verkehrsmittelwechsels) und die sonst serverseitig für Inferenz und Speicherung der Daten benötigten Ressourcen auf die Nutzer des Systems aufzuteilen. Dies konstituiert die zentralen Motivationen hinter \textit{Edge Computing}, bei dem Edge-Geräte (Smartphones, Router, IoT-Geräte im Allgemeinen) gezielt Aufwände innerhalb eines Netzwerkes übernehmen und untereinander orchestrieren. Als Teilbereich des Edge Computing beschäftigt sich der Forschungsbereich \textit{Edge AI} vor allem damit, wie Edge-Geräte durch den Einsatz von künstlicher Intelligenz effizienter und proaktiv auf dynamische Veränderungen des Kontexts reagieren können. Die Portierung eines Machine-Learning-Systems auf Smartphones, wie es im Movebis-Projekt noch serverseitig zum Einsatz kommt, kann somit als Spezialfall dieses Forschungsbereichs gewertet werden. Als Edge-Geräte sind Smartphones vor allem durch die auf dem Gerät verfügbaren Ressourcen limitiert, als zentrale Aspekte stehen sich hierdurch die Performanz und die Effizienz des Machine-Learning-Modells gegenüber \cite{zou_edge_2019, ma_survey_2019, deng_model_2020}.

\begin{figure}[H]
\includegraphics[width=0.75\linewidth, bb=0 0 367 304]{six-level-rating-ei.pdf}
\caption{6-stufiger taxonomischer Überblick über Lern- und Inferenzverfahren im Edge Computing nach \cite{zhou_edge_2019}.}\label{fig:six-level-rating-ei}
\end{figure}

\noindent \Cref{fig:six-level-rating-ei} zeigt einen 6-stufigen taxonomischen Überblick über die verschiedene Möglichkeiten für das Training von Machine-Learning-Modellen im Edge Computing in Kooperation mit Servern nach \cite{zhou_edge_2019}. Wird das Machine-Learning-Modell in der Cloud ausgeführt und trainiert, stehen dem Modell im Umkehrschluss auch typischerweise deutlich mehr Ressourcen zur Verfügung. In diesem Fall können die Server mit geeigneter Hardware (z.B. hochparallelisierbare Grafikkarten\footnote{\url{https://www.tensorflow.org/install/gpu} (Abgerufen am 19.5.2021).}, fester Stromanschluss, hohe Bandbreite) für das Training und die Inferenz ausgestattet werden. Je näher an die taxonomische Stufe \textit{All-In-Edge} herangetreten wird, desto weniger dieser Ressourcen stehen direkt zur Verfügung. Es werden spezielle kollaborative Lernkonzepte wie beispielsweise das in \cite{wang_-edge_2019} gezeigte \textit{Federated Learning} notwendig, mithilfe derer die Modellparameter über das Netzwerk zwischen Edge-Geräten und Cloud-Servern ausgetauscht werden können, um die Effizienz zu verbessern und die individuelle Last auf dem Edge-Gerät zu verringern.

\subsection{Zentrale Probleme der Portierung}

Das Smartphone-Ökosystem stellt einige Herausforderungen an die Portierung von Machine-Learning-Modellen, unter anderem auch bedingt durch die große Variation an Smartphones und den dazugehörigen Hardwarevoraussetzungen und Betriebssystemen. Die wichtigsten Problemfaktoren sollen in diesem Abschnitt näher erläutert werden, um anschließend konkrete Lösungsmethoden vorzustellen.

\subsubsection{Hardware-Limitationen}

Als Edge-Geräte sind Smartphones hardwaretechnisch in erster Linie durch die zur Verfügung stehende Netzwerkbandbreite, den Energieverbrauch und den verfügbaren Speicher (Arbeitsspeicher, Persistenter Speicher) limitiert. Die Netzwerkbandbreite ist insbesondere dann für das Machine-Learning-System von Bedeutung, wenn dessen Lernparameter über das Netzwerk zwischen den Edge-Geräten ausgetauscht werden müssen (wie beim Federated Learning) oder das Modell in der Cloud ausgeführt wird. Für die Ausführung eines Machine-Learning-Modells direkt auf dem Smartphone ist die Netzwerkbandbreite nur von sekundärer Relevanz. Primär ist hier der Energieverbrauch und der Speicherverbrauch des Modells von Bedeutung. Künstliche neuronale Netzwerke erreichen je nach Architektur schnell hunderte Millionen von Lernparametern und Speicherdimensionen von hunderten Megabytes \cite[S. 313, 314]{sosnovshchenko_machine_2018}, in für Smartphones unpraktikablen Extremfällen wie \textit{GPT-3} sind es 175 Mrd. Lernparameter und 700 Gigabytes\footnote{\url{https://lambdalabs.com/blog/demystifying-gpt-3/} (Abgerufen am 21.5.2021)}. Wird ein Machine-Learning-Modell wie ein künstliches neuronales Netzwerk mit 100 Megabytes in eine App integriert, so erhöht dies auch wiederum die Größe der App und Nutzer sind möglicherweise nicht mehr bereit oder besitzen ausreichend Speicher, um sich diese zu installieren. Die Nutzererfahrung wird also verschlechtert, dies ist mit dem qualitativen Zugewinn durch Einsatz des Machine-Learning-Modells abzuwägen. Bestenfalls wird jedoch die Größe des Modells weitestgehend bei der Entwicklung reduziert. Ist die App heruntergeladen, kann die Ausführung von künstlichen neuronalen Netzwerken effizient parallelisiert werden, unter der Verwendung von speziellen Vektor-Koprozessoren oder Grafikchips. Dennoch stellt die Inferenz und insbesondere das Training einen hohen Rechenaufwand dar, der den Akku innerhalb kürzester Zeit verbrauchen kann \cite[S. 311]{sosnovshchenko_machine_2018}. Darüber hinaus stehen möglicherweise nicht auf allen Geräten dedizierte Hardwarekomponenten zur Verfügung, unter Umständen wird die CPU genutzt. Hierdurch steigt die Dauer, sowie die Energie- und Speicherintensität des Betriebs durch die geringe Parallelisierung. Die mathematische Optimierung, welche zum Training des Modells notwendig ist, stellt nochmals einen höheren Rechenaufwand dar. Daher ist es auch unüblich, größere Modelle auf Smartphones zu trainieren.

\subsubsection{Software-Limitationen}

In den vorigen Abschnitten wurde gezeigt, dass Machine-Learning-Modelle in der Cloud ausgeführt und trainiert werden können, oder dies auch vollständig auf dem Smartphone durchgeführt werden kann, mit den entsprechenden Hardware-Limitationen. Auch die Software-Limitationen hängen direkt von der Wahl des Deployment-Modells ab. Während vollständig in der Cloud ausgeführte und trainierte Machine-Learning-Modelle bereits eine Vielzahl von etablierten Technologien zur Implementation und zur Ausführung nutzen können (viele Frameworks und Programmiersprachen), ist die Portierung eines trainierten Modells auf Smartphones (inkl. Training und Inferenz), im Moment noch an die sporadische Verfügbarkeit der Portierungsmöglichkeiten gekoppelt. Es besteht also zur Zeit noch eine klar erkennbare, technologisch (durch Programmiersprachen, Limitationen des Betriebssystems) bedingte, künstliche Grenze zwischen serverseitigen und clientseitigen Machine-Learning-Systemen. Grundsätzlich besteht zu Beginn der Portierung als zentraler Teil des Lösungskonzepts daher nach aktuellem Stand immer die Frage, ob das Modell mithilfe von Frameworks über diese Grenze hinweg portiert werden soll, oder selbst mit hardwarenahen Schnittstellen reimplementiert wird. Frameworks bieten in der Regel eine hohe Flexibilität und die Möglichkeit des Vortrainierens auf dem Server, während die resultierenden Modelle auf dem Smartphone unter Umständen nicht mehr trainierbar oder nur als \textit{Black Box}\footnote{Black Box bezeichnet in diesem Fall, dass das Machine-Learning-Modell bei der Framework-gestützten Portierung fest definierte Schnittstellen erhält und die internen Funktionsweisen nach außen versteckt werden.} verfügbar sind. Die manuelle clientseitige Implementation im Smartphone-Ökosystem ist wegen der hohen Komplexität der notwendigen Programmierung vor allem bei größeren Modellen eher unpraktikabel \todo{Diese Stelle ist zu bewertend und sollte noch einmal umformuliert werden.}. Neben dieser Problematik müssen App-Entwickler auch die Gegebenheiten des Betriebssystems und der App-Nutzung mit einbeziehen. Während Cloud-basierte Machine-Learning-Modelle dediziert und kontinuierlich für die vorgesehene Aufgabe ausgeführt werden können, ist die Ausführung von Machine-Learning-Modellen auf Smartphones an häufige Unterbrechungen durch den Nutzer (z.B. Wechsel in andere App) gebunden. Gleichzeitig kann die App permanent in den Hintergrund gelangen, entweder durch Wechsel der App oder durch Aktivierung des Stand-By-Modus. Die Verfügbarkeit der Rechenressourcen kann also zu jedem Zeitpunkt entzogen werden, langfristige Operationen wie das Training sind dann unter Umständen nicht mehr möglich. In diesem Fall besteht grundsätzlich die Möglichkeit, die notwendigen Operationen in den Hintergrund zu verschieben. Um eine Kompetetivität mit anderen Apps zu verhindern, sollte dies bei rechenintensitiven Operationen wie dem Training vermieden oder mit den entsprechenden Systemschnittstellen in Zeitbereiche mit geringer Auslastung verschoben werden\footnote{\url{https://developer.apple.com/documentation/backgroundtasks/choosing_background_strategies_for_your_app} (Abgerufen am 21.5.2021)}. Versucht eine App, sich dennoch selbst durch unsensibles und ressourcenverbrauchendes Hintergrundverhalten zu übervorteilen, so könnte sie bei der App-Prüfung (vor Veröffentlichung im jeweiligen App-Store) zurückgewiesen werden. Je nach Betriebssystem und App-Store gelten hierfür unterschiedliche Entwicklerrichtlinien, wichtig ist bei der Entwicklung also auch die Beachtung der \textit{Compliance}. \\

\noindent Als zentrale Probleme konnten die Rechen- und Energieintensität des Modells bestimmt werden. Das Netzwerk stellt in bestimmten Fällen einen weiteren limitierenden Faktor dar. Zusätzlich ist die Compliance mit den entsprechenden Entwickler-Richtlinien und eine gute Nutzbarkeit wichtig, während der Entwicklungsaufwand und die Portierbarkeit über Frameworks vom Standpunkt des Entwicklers von zentraler Bedeutung sind.

\subsection{Modelloptimierung}

Um die Rechen-, Speicher- und Energieintensität von Machine-Learning-Modellen zu optimieren, können verschiedene Konzept angewandt werden. Da es sich hierbei um ein aktuelles Forschungsthema handelt und die Konzepte in verschiedenste Richtungen gehen, bietet sich ein Blick in spezifische Metaanalysen an. \cite{deng_model_2020} und \cite{choudhary_comprehensive_2020} vermitteln einen detaillierten Überblick über den Stand der Forschung in der Modelloptimierung, \cite[S. 315ff]{sosnovshchenko_machine_2018} gibt einen praxisorientierten Überblick. \cite{nan_deep_2019} und \cite{dai_toward_2021} analysieren verschiedene Optimierungsmethoden für mobile Plattformen unter anderem bezüglich ihrer konkreten Auswirkungen auf die Modellpräzision, die Inferenzzeit, die CPU-Last und die damit verbundenen thermischen Auswirkungen. Die zentralen Ideen der Modelloptimierung sollen in den nachfolgenden Abschnitten im Überblick vorgestellt werden.

\subsubsection{Limitierung des Anwendungsfalles}

Eine für alle Machine-Learning-Systeme anwendbare Methode zur Modelloptimierung ist durch die Limitierung des Anwendungsfalles realisierbar. Je mehr Aufgaben ein Modell erfüllen soll, beispielsweise durch die Unterscheidung von zahlreichen Mustern, desto komplexer wird normalerweise auch die interne Repräsentation. Neuronale Netzwerke müssen also zum Beispiel mehr Neuronen und Schichten erhalten, im Umkehrschluss wird das Modell größer und der Rechen-, Energie- und Speicheraufwand steigt. Außerdem steigt die Trainingsdauer und die Menge an benötigten Trainingsdaten. Es bietet sich also an, den Anwendungsfall des Modells soweit möglich zu reduzieren, beispielsweise könnte bei einer Verkehrsmittelklassifikation die Anzahl der Klassen auf \enquote{Fahrrad fahren} und \enquote{Nicht Fahrrad fahren} limitiert werden. Dasselbe Prinzip kann auf die Entropie der Trainingsdaten angewandt werden. Sind die Trainingsdaten variabler als es der Anwendungsfall erfordert, bedingt dies auch wieder eine größere interne Repräsentation des Modells. Bei einer Klassifikation von handgeschriebenen Buchstaben kann beispielsweise der fotografierte Winkel der Buchstaben beim Training variiert werden, um neue Trainingsdaten zu erzeugen (\textit{Data Augmentation}), jedoch nicht mehr als $\pm 30°$, wenn das inferierende Smartphone später erwartungsgemäß später nicht mehr als $30°$ geneigt wird. Zusammenfassend kann also allein durch die gezielte Konfiguration der zu unterscheidenden Klassen sowie der Trainingsdaten die für eine hinreichend gute Klassifikation benötigte Modellgröße reduziert werden.

\subsubsection{Architekturelle Restrukturierung des Modells}

Die Reduktion der Modellgröße durch Elimination von internen Parametern muss nicht wahllos geschehen. Auch hierfür können gezielte Strategien angewandt werden \cite[S. 317ff]{sosnovshchenko_machine_2018}. Speziell bei CNNs bietet sich die Reduktion von einzelnen Convolution-Schichten an. Wie \cite{iandola_squeezenet_2016} zeigen, können allein durch die Reduktion einer im Ausgaben-nahen Teil des Netzwerkes liegenden $3\times 3$-Convolution-Schicht zu einer $1\times 1$-Convolution-Schicht die gesamten transitiven Parameter eines CNNs um das Neunfache reduziert werden, bei einem geringfügigen Informationsverlust in Relation zum nicht verkleinerten Modell \textit{SqueezeNet}. Eine weitere Idee bei der Optimierung der Convolution-Schichten wurde erstmals in \cite{howard_mobilenets_2017} \textit{MobileNet} vorgestellt. Im Forschungsbereich der Computergrafik und \textit{Computer Vision} ist es ein weit verbreitetes Verfahren, Filter wie den gausschen Weichzeichner in zwei oder mehrere Teilfilter zu zerlegen, um die absolut benötigte Anzahl an Rechenoperationen zu reduzieren. Beim Weichzeichner resultiert so ein horizontales und ein vertikales Filter. Die Convolution-Operationen eines CNNs können analog hierzu, da sie lediglich spezielle Matrixoperationen sind, auch als mathematisches Filter betrachtet und deren Berechnung durch tiefenweise Separation optimiert werden. Bei $3\times 3$-Convolution-Schichten kann die Berechnung so neunfach effizenter (und schneller) ausgeführt werden\footnote{\url{https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/} (Abgerufen am 22.5.2021)}. Weitere Strategien für die architekturelle Optimierung von CNNs sind die Verwendung von speziellen \textit{Shuffle}-Schichten \cite{zhang_shufflenet_2018} \textit{ShuffleNet} oder die gezielte Gruppierung von Convolution-Schichten in \textit{Grouped Convolutions} \cite{huang_condensenet_2018} (\textit{CondenseNet}). Auch für RNNs bestehen Ideen zur architekturellen Optimierung, beispielsweise durch eine effizientere Repräsentation der Matrix-Vektor-Multiplikationen \cite{wang_accelerating_2017} oder der Ersetzung der typischen LSTM-Neuronen durch kompaktifizierbarere Alternativen \cite{kusupati_fastgrnn_2019, luo_neural_2019}. Neben CNNs und RNNs lassen sich auch Feed-Forward-Netzwerke hinsichtlich ihrer Architektur optimieren. Beispielsweise lassen sich spezielle \textit{Multiplexing}-Verbindungen in das Netzwerk einfügen, um (unter leichter Verschlechterung des Zeitverhaltens) bis zu $50\%$ der sonst benötigten Hidden Layers und den dazugehörigen Neuronen einzusparen \cite{khalil_efficient_2018}.

\subsubsection{Verlustfreie Kompressionsverfahren}

Neben einer architekturellen Restrukturierung von Machine-Learning-Modellen wie neuronalen Netzwerken ist es auch möglich, diese für den Netzwerktransport und die persistente Speicherung verlustfrei zu komprimieren. Diese Technologie ist altbewährt, verlustfreie Kompressionsprogramme wie \texttt{gzip}\footnote{\url{https://www.gzip.org/} (Abgerufen am 22.5.2021)} existieren bereits seit mehreren Jahrzehnten und basieren häufig auf der theoretischen Grundlage der \textit{Huffman-Kodierung}\footnote{\url{https://de.wikipedia.org/wiki/Huffman-Kodierung} (Abgerufen am 22.5.2021)}. Die Machine-Learning-Modelle müssen dabei nicht zwingend mit plattformspezifischen Kompressionsverfahren verkleinert werden, sowohl iOS\footnote{\url{https://developer.apple.com/documentation/compression/compression_zlib} (Abgerufen am 22.5.2021)} als auch Android\footnote{\url{https://developer.android.com/reference/java/util/zip/Deflater} (Abgerufen am 22.5.2021)} unterstützen zum Beispiel das weit verbreitete \texttt{zlib}-Verfahren \cite[S. 315]{sosnovshchenko_machine_2018}.

\subsubsection{Verlustbehaftete Kompressionsverfahren}

Während bei der verlustfreien Kompression alle Parameter des Machine-Learning-Modells erhalten bleiben, werden bei der verlustbehafteten Kompression gezielt Parameter verändert oder entfernt. Ziel ist es hierbei, so viele Parameter wie möglich zu reduzieren, bei einer möglichst gleichbleibenden (oder in manchen Fällen sogar verbesserten) Präzision.

\paragraph{Quantisierung:} Die Parameter eines herkömmlichen Machine-Learning-Modells liegen typischerweise in Fließkommazahlen nach dem IEEE 754\footnote{\url{https://de.wikipedia.org/wiki/IEEE_754} (Abgerufen am 23.5.2021)} $32 bit$-Format vor. Diese können zur Reduktion der Modellgröße in Ganzzahlen nach dem $8 bit$ Integer-Format überführt werden. Rechnerisch lässt sich hierüber die Größe des Modells auf bis zu ein Viertel der Ausgangsgröße reduzieren. Gleichzeitig müssen die Fließkommazahlen über eine skalare Multiplikation geeignet in den Integer-Wertebereich $[-128, 127]$ (signed) bzw. $[0, 255]$ (unsigned) überführt werden\footnote{\url{https://de.wikipedia.org/wiki/Integer_(Datentyp)} (Abgerufen am 23.5.2021)}. Hierzu können eine Min-Max-Skalierung oder verschiedene andere Skalierungsmethoden dienen. Da sich die Skalierung bei Änderung der Modellparameter mit ändert, bietet sich die Quantisierung in der Regel für eine Komprimierung \textit{nach} dem Training an. Es bestehen aber auch Möglichkeiten, bereits beim Training eine laufende Quantisierung durchzuführen. Trotz des signifikanten Verlustes der durch die binäre Repräsentation darstellbaren Werten verringert eine Quantisierung die Präzision eines Modells normalerweise nur geringfügig, daher eignet sie sich sehr gut für die verlustbehaftete Komprimierung \cite[S. 14ff]{liang_pruning_2021}.

\paragraph{Pruning:} Nicht alle Parameter eines Machine-Learning-Modells sind für die interne Wissensrepräsentation gleich wichtig. Es besteht die Möglichkeit, dass bestimmte Parameter nur einen geringen Einfluss auf die Ausgabe des Modells haben. Bei künstlichen neuronalen Netzwerken können so beispielsweise gezielt einzelne Neuronen, Gruppen von Neuronen oder ganze Schichten entfernt werden, wenn sie als unwichtig identifiziert werden können. Letzteres ist jedoch ein nichttriviales Problem, welches eine differenzierte Herangehensweise erfordert. Es ist möglich, über die zufällige Elimination von Neuronen und Verbindungen zu testen, ob diese die gesamte Präzision signifikant reduziert (\textit{Brute-Force}-Methode). Durch Heuristiken lässt sich dieses Verfahren jedoch weiter verbessern. Bei einer sehr einfachen Variante des \textit{Magnitude-based Pruning} werden beispielsweise Parameter innerhalb eines Machine-Learning-Modells entfernt, welche sehr kleine Werte nahe $0$ annehmen. Die Annahme hierhinter ist, dass Parameter mit sehr kleinen Werten in der Regel weniger wichtig für die Ausgabe des Modells sind, als Parameter mit größeren Werten. Ähnliche Methoden wie \textit{Optimal Brain Damage} \cite{lecun_optimal_1990} und \textit{Optimal Brain Surgeon} \cite{hassibi_second_1993} nutzen den Gradienten der Kostenfunktion für die Identifikation einzelner zu eliminierender Neuronen. Bei der Elimination von Neuronen ist auch die Regularisierung von Bedeutung, durch Regularisierungsmethoden (\Cref{sec:regularisierung}) wie Dropout oder $l_1$- und $l_2$-Regularisierung kann diese beschleunigt werden \cites[S. 7ff]{liang_pruning_2021}{chang_prune_2019}.

\paragraph{Knowledge Distillation:} Methoden wie Quantisierung und Pruning können auf einem bestehenden Modell direkt angewandt werden, um dieses durch die Elimination von potenziell unwichtigen Parametern zu komprimieren. Beim Verfahren \textit{Knowledge Distillation} wird ein bestehendes (trainiertes) Modell genutzt, um dessen Verhaltensweise durch ein kleineres Modell nachzustellen. Ziel ist es, eine effizientere Wissensrepräsentation durch das kleinere Modell zu erstellen, welche weniger Parameter bei einer annähernd gleichen Präzision besitzt. \\

\noindent Die diskutierten Methoden sind in dieser Form für die meisten Machine-Learning-Modelle flexibel anwendbar und dadurch auch weit verbreitet \cite{nan_deep_2019, choudhary_comprehensive_2020}. Die aufgezeigten Grundlagen erfassen jedoch nicht das gesamte Spektrum an potenziellen Ansätzen. Weitere Ansätze sind beispielsweise:

\begin{itemize}
\item Optimierung des Machine-Learning-Modells für die Inferenz durch die Elimination von Komponenten, die lediglich für das Training benötigt werden.
\item Approximation der Modellparameter durch \textit{Low Rank Tensor Approximation} und die daraus resultierende Verkleinerung der Matrixoperationen\footnote{\url{https://en.wikipedia.org/wiki/Low-rank_approximation} (Abgerufen am 23.5.2021)}.
\item Fusion von bestimmten Modell-Operationen (\textit{Layer/Tensor-Fusion})\footnote{\url{https://www.tensorflow.org/lite/convert/operation_fusion} (Abgerufen am 23.5.2021)} zur weiteren Optimierung und Komprimierung der Berechnungen.
\item Verschiedene hardwareorientierte Ansätze wie die Einführung einer dynamischen Speicherverwaltung, um größere Machine-Learning-Modelle effizient und parallelisiert auf Grafikeinheiten und dedizierten Koprozessoren ausführen zu können \cite{sb_dynamic_2019}.
\end{itemize}

\noindent Im Rahmen der Portierung bietet es sich in Anbetracht der verschiedensten Methoden an, eine konkrete Kompressionsstrategie zu entwickeln, welche einen oder mehrere dieser Schritte inkludiert. Dabei kann auch eine Selektion spezifischerer Varianten und Derivate der gezeigten Optimierungsmethoden gewählt werden.

\subsection{Profiling}


