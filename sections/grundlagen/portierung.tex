
\section{Portierung}\label{sec:portierung}

In den vorigen Abschnitten wurde diskutiert, wie Daten vorverarbeitet werden können, um anschließend durch Machine-Learning-Modelle klassifiziert zu werden. In \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} nutzen hierfür künstliche neuronale Netzwerke mit verschiedenen Architekturen (FFN, \acrshort{rnn}, \acrshort{cnn}). Dieser Abschnitt soll diskutieren, wie die Konzepte nun für den Einsatz auf Smartphones portiert werden können und welche Faktoren hierbei zu beachten sind.

\subsection{Deployment-Modelle im Edge Computing}

Für einen Einsatz auf Smartphones müssen Machine-Learning-Modelle nicht zwingend auch auf diesem über eine App mitgeliefert oder installiert (engl. \textit{deploy}) werden. \cite[S. 13]{ota_deep_2017} separieren beispielsweise in zwei Deployment-Modelle.

\begin{itemize}
\item \textit{Client-Server-Deployment}, wobei die Inferenz des Machine-Learning-Modells auf dem Server-Backend geschieht.
\item \textit{Client-Only-Deployment}, wobei die Inferenz vom Smartphone selbst übernommen wird, das Machine-Learning-Modell also mit in der jeweiligen App integriert ist.
\end{itemize}

\cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020}, welche mithilfe von künstlichen neuronalen Netzwerken das Filtern der STADTRADELN-\allowbreak Daten im Movebis-Projekt ermöglichen, gliedern sich analog zur obigen Taxonomie in das Client-Server-Deployment ein, wobei jedoch nur eine zeitlich entkoppelte unidirektionale Kommunikation stattfindet. Die Smartphone-App erhält kein direktes Feedback zum Ergebnis der serverseitigen Klassifikation und besitzt so auch keine Möglichkeit zur Adaption auf wichtige Ereignisse wie einen Wechsel des Verkehrsmittels. Vor diesem Hintergrund könnte es allgemeiner gefasst also vorteilhaft sein, die Inferenz vom Server so nah wie möglich an den Client (Smartphone) zu verschieben, unter anderem auch um die Netzwerklast zu verringern (durch Abschalten der Aufzeichnung infolge eines Verkehrsmittelwechsels) und die sonst serverseitig für Inferenz und Speicherung der Daten benötigten Ressourcen auf die Nutzer des Systems aufzuteilen. Dies konstituiert die zentralen Motivationen hinter \textit{Edge Computing}, bei dem Edge-Geräte (Smartphones, Router, IoT-Geräte im Allgemeinen) gezielt Aufwände innerhalb eines Netzwerkes übernehmen und untereinander orchestrieren. Als Teilbereich des Edge Computing beschäftigt sich der Forschungsbereich \textit{Edge AI} vor allem damit, wie Edge-Geräte durch den Einsatz von künstlicher Intelligenz effizienter und proaktiv auf dynamische Veränderungen des Kontexts reagieren können. Die Portierung eines Machine-Learning-Systems auf Smartphones, wie es im Movebis-Projekt noch serverseitig zum Einsatz kommt, kann somit als Spezialfall dieses Forschungsbereichs gewertet werden. Als Edge-Geräte sind Smartphones vor allem durch die auf dem Gerät verfügbaren Ressourcen limitiert, als zentrale Aspekte stehen sich hierdurch die Performanz und die Effizienz des Machine-Learning-Modells gegenüber \cite{zou_edge_2019, ma_survey_2019, deng_model_2020}.

\begin{figure}[h]
\includegraphics[width=0.5\linewidth, bb=0 0 367 304]{six-level-rating-ei.pdf}
\caption[6-stufiger taxonomischer Überblick über Lern- und Inferenzverfahren im Edge Computing]{6-stufiger taxonomischer Überblick über Lern- und Inferenzverfahren im Edge Computing nach \cite{zhou_edge_2019}.}\label{fig:six-level-rating-ei}
\end{figure}

\Cref{fig:six-level-rating-ei} zeigt einen 6-stufigen taxonomischen Überblick über die verschiedene Möglichkeiten für das Training von Machine-Learning-Modellen im Edge Computing in Kooperation mit Servern nach \cite{zhou_edge_2019}. Wird das Machine-Learning-Modell in der Cloud ausgeführt und trainiert, stehen dem Modell im Umkehrschluss auch typischerweise deutlich mehr Ressourcen zur Verfügung. In diesem Fall können die Server mit geeigneter Hardware (z.B. hochparallelisierbare Grafikkarten\footnote{\url{https://www.tensorflow.org/install/gpu} (Abgerufen am 19.5.2021).}, fester Stromanschluss, hohe Bandbreite) für das Training und die Inferenz ausgestattet werden. Je näher an die taxonomische Stufe \textit{All-In-Edge} herangetreten wird, desto weniger dieser Ressourcen stehen direkt zur Verfügung. Es werden spezielle kollaborative Lernkonzepte wie beispielsweise das in \cite{wang_-edge_2019} gezeigte \textit{Federated Learning} notwendig, mithilfe derer die Modellparameter über das Netzwerk zwischen Edge-Geräten und Cloud-Servern ausgetauscht werden können, um die Effizienz zu verbessern und die individuelle Last auf dem Edge-Gerät zu verringern.

\subsection{Zentrale Probleme der Portierung}

Das Smartphone-Ökosystem stellt einige Herausforderungen an die Portierung von Machine-Learning-Modellen, unter anderem auch bedingt durch die große Variation an Smartphones und den dazugehörigen Hardwarevoraussetzungen und Betriebssystemen. Die wichtigsten Problemfaktoren sollen in diesem Abschnitt näher erläutert werden, um anschließend konkrete Lösungsmethoden vorzustellen.

\subsubsection{Hardware-Limitationen}

Als Edge-Geräte sind Smartphones hardwaretechnisch in erster Linie durch die zur Verfügung stehende Netzwerkbandbreite, den Energieverbrauch und den verfügbaren Speicher (Arbeitsspeicher, Persistenter Speicher) limitiert. Die Netzwerkbandbreite ist insbesondere dann für das Machine-Learning-System von Bedeutung, wenn dessen Lernparameter über das Netzwerk zwischen den Edge-Geräten ausgetauscht werden müssen (wie beim Federated Learning) oder das Modell in der Cloud ausgeführt wird. Für die Ausführung eines Machine-Learning-Modells direkt auf dem Smartphone ist die Netzwerkbandbreite nur von sekundärer Relevanz. Primär ist hier der Energieverbrauch und der Speicherverbrauch des Modells von Bedeutung. Künstliche neuronale Netzwerke erreichen je nach Architektur schnell hunderte Millionen von Lernparametern und Speicherdimensionen von hunderten Megabytes \cite[S. 313, 314]{sosnovshchenko_machine_2018}, in für Smartphones unpraktikablen Extremfällen wie \textit{GPT-3} sind es 175 Mrd. Lernparameter und 700 Gigabytes\footnote{\url{https://lambdalabs.com/blog/demystifying-gpt-3/} (Abgerufen am 21.5.2021)}. Wird ein Machine-Learning-Modell wie ein künstliches neuronales Netzwerk mit 100 Megabytes in eine App integriert, so erhöht dies auch wiederum die Größe der App und Nutzer sind möglicherweise nicht mehr bereit oder besitzen ausreichend Speicher, um sich diese zu installieren. Die Nutzererfahrung wird also verschlechtert, dies ist mit dem qualitativen Zugewinn durch Einsatz des Machine-Learning-Modells abzuwägen. Bestenfalls wird jedoch die Größe des Modells weitestgehend bei der Entwicklung reduziert. Ist die App heruntergeladen, kann die Ausführung von künstlichen neuronalen Netzwerken effizient parallelisiert werden, unter der Verwendung von speziellen Vektor-Koprozessoren oder Grafikchips. Dennoch stellt die Inferenz und insbesondere das Training einen hohen Rechenaufwand dar, der den Akku innerhalb kürzester Zeit verbrauchen kann \cite[S. 311]{sosnovshchenko_machine_2018}. Darüber hinaus stehen möglicherweise nicht auf allen Geräten dedizierte Hardwarekomponenten zur Verfügung, unter Umständen wird die CPU genutzt. Hierdurch steigt die Dauer, sowie die Energie- und Speicherintensität des Betriebs durch die geringe Parallelisierung. Die mathematische Optimierung, welche zum Training des Modells notwendig ist, stellt nochmals einen höheren Rechenaufwand dar. Daher ist es auch unüblich, größere Modelle auf Smartphones zu trainieren.

\subsubsection{Software-Limitationen}

In den vorigen Abschnitten wurde gezeigt, dass Machine-Learning-Modelle in der Cloud ausgeführt und trainiert werden können, oder dies auch vollständig auf dem Smartphone durchgeführt werden kann, mit den entsprechenden Hardware-Limitationen. Auch die Software-Limitationen hängen direkt von der Wahl des Deployment-Modells ab.

Die Portierung eines Machine-Learning-Modells ist an einen technologischen Flaschenhals gebunden, der durch die Heterogenität der serverseitigen Implementationen und deren clientseitiges Pendant entsteht. Grundsätzlich besteht zu Beginn der Portierung als zentraler Teil des Lösungskonzepts daher nach aktuellem Stand immer die Frage, ob das Modell mithilfe von Frameworks implementiert und über deren Funktionalitäten portiert werden soll, oder selbst mit hardwarenahen Schnittstellen reimplementiert wird. Die Nutzung von Frameworks ist typischerweise vom Standpunkt des Entwicklers die effizienteste Lösung, Modelle lassen sich mit Frameworks in wenigen Codezeilen implementieren. Dies ist jedoch auch mit Nachteilen verbunden. Es besteht das Risiko, dass durch die hohe Abstraktion der Frameworks bestimmte Details übersehen werden, die bei einer manuellen Implementation aktiv codiert werden müssten, wie beispielsweise Aktivierungsfunktionen von Neuronen. Je nach Framework sind die resultierenden Modelle darüberhinaus auf dem Smartphone unter Umständen nicht mehr trainierbar oder nur als \textit{Black Box}\footnote{Black Box bezeichnet in diesem Fall, dass das Machine-Learning-Modell bei der Framework-gestützten Portierung fest definierte Schnittstellen erhält und die internen Funktionsweisen nach außen versteckt werden.} verfügbar. Diese Nachteile sollten bei der Auswahl der Technologien und der Portierung berücksichtigt werden.

Neben dieser Problematik müssen App-Entwickler auch die Gegebenheiten des Betriebssystems und der App-Nutzung mit einbeziehen. Während Cloud-basierte Machine-Learning-Modelle dediziert und kontinuierlich für die vorgesehene Aufgabe ausgeführt werden können, ist die Ausführung von Machine-Learning-Modellen auf Smartphones an häufige Unterbrechungen durch den Nutzer (z.B. Wechsel in andere App) gebunden. Gleichzeitig kann die App permanent in den Hintergrund gelangen, entweder durch Wechsel der App oder durch Aktivierung des Stand-By-Modus. Die Verfügbarkeit der Rechenressourcen kann also zu jedem Zeitpunkt entzogen werden, langfristige Operationen wie das Training sind dann unter Umständen nicht mehr möglich. In diesem Fall besteht grundsätzlich die Möglichkeit, die notwendigen Operationen in den Hintergrund zu verschieben. Um eine Kompetetivität mit anderen Apps zu verhindern, sollte dies bei rechenintensitiven Operationen wie dem Training vermieden oder mit den entsprechenden Systemschnittstellen in Zeitbereiche mit geringer Auslastung verschoben werden\footnote{\url{https://developer.apple.com/documentation/backgroundtasks/choosing_background_strategies_for_your_app} (Abgerufen am 21.5.2021)}. Versucht eine App, sich dennoch selbst durch unsensibles und ressourcenverbrauchendes Hintergrundverhalten zu übervorteilen, so könnte sie bei der App-Prüfung (vor Veröffentlichung im jeweiligen App-Store) zurückgewiesen oder vom Betriebssystem aktiv benachteiligt werden. Je nach Betriebssystem und App-Store gelten hierfür unterschiedliche Entwicklerrichtlinien, wichtig ist bei der Entwicklung also auch die Beachtung der \textit{Compliance}.

\subsection{Modelloptimierung}

Um die Rechen-, Speicher- und Energieintensität von Machine-Learning-Modellen zu optimieren, können verschiedene Konzept angewandt werden. Da es sich hierbei um ein aktuelles Forschungsthema handelt und die Konzepte in verschiedenste Richtungen gehen, bietet sich ein Blick in spezifische Metaanalysen an. \cite{deng_model_2020} und \cite{choudhary_comprehensive_2020} vermitteln einen detaillierten Überblick über den Stand der Forschung in der Modelloptimierung, \cite[S. 315ff]{sosnovshchenko_machine_2018} gibt einen praxisorientierten Überblick. \cite{nan_deep_2019} und \cite{dai_toward_2021} analysieren verschiedene Optimierungsmethoden für mobile Plattformen unter anderem bezüglich ihrer konkreten Auswirkungen auf die Modellpräzision, die Inferenzzeit, die CPU-Last und die damit verbundenen thermischen Auswirkungen. Die zentralen Ideen der Modelloptimierung sollen in den nachfolgenden Abschnitten im Überblick vorgestellt werden.

\subsubsection{Limitierung des Anwendungsfalles}

Eine für alle Machine-Learning-Systeme anwendbare Methode zur Modelloptimierung ist durch die Limitierung des Anwendungsfalles realisierbar. Je mehr Aufgaben ein Modell erfüllen soll, beispielsweise durch die Unterscheidung von zahlreichen Mustern, desto komplexer wird normalerweise auch die interne Repräsentation. Neuronale Netzwerke müssen also zum Beispiel mehr Neuronen und Schichten erhalten, im Umkehrschluss wird das Modell größer und der Rechen-, Energie- und Speicheraufwand steigt. Außerdem steigt die Trainingsdauer und die Menge an benötigten Trainingsdaten. Es bietet sich also an, den Anwendungsfall des Modells soweit möglich zu reduzieren, beispielsweise könnte bei einer Verkehrsmittelklassifikation die Anzahl der Klassen auf \enquote{Fahrrad fahren} und \enquote{Nicht Fahrrad fahren} limitiert werden. Dasselbe Prinzip kann auf die Entropie der Trainingsdaten angewandt werden. Sind die Trainingsdaten variabler als es der Anwendungsfall erfordert, bedingt dies auch wieder eine größere interne Repräsentation des Modells. Bei einer Klassifikation von handgeschriebenen Buchstaben kann beispielsweise der fotografierte Winkel der Buchstaben beim Training variiert werden, um neue Trainingsdaten zu erzeugen (\textit{Data Augmentation}), jedoch nicht mehr als $\pm 30^\circ$, wenn das inferierende Smartphone später erwartungsgemäß später nicht mehr als $30^\circ$ geneigt wird \cites{otavio_good_how_2015}. Zusammenfassend kann also allein durch die gezielte Konfiguration der zu unterscheidenden Klassen sowie der Trainingsdaten die für eine hinreichend gute Klassifikation benötigte Modellgröße reduziert werden \cite[S. 319]{sosnovshchenko_machine_2018}.

\subsubsection{Architekturelle Restrukturierung des Modells}

Die Reduktion der Modellgröße durch Elimination von internen Parametern muss nicht wahllos geschehen. Auch hierfür können gezielte Strategien angewandt werden \cite[S. 317ff]{sosnovshchenko_machine_2018}. Speziell bei \acrshort{cnn}s bietet sich die Reduktion von einzelnen Convolution-Schichten an. Wie \cite{iandola_squeezenet_2016} zeigen, können allein durch die Reduktion einer im Ausgaben-nahen Teil des Netzwerkes liegenden $3\times 3$-Convolution-Schicht zu einer $1\times 1$-Convolution-Schicht die gesamten transitiven Parameter eines \acrshort{cnn}s um das Neunfache reduziert werden (\textit{SqueezeNet}), bei einem geringfügigen Informationsverlust in Relation zum nicht verkleinerten Modell. Eine weitere Idee bei der Optimierung der Convolution-Schichten wurde erstmals in \cite{howard_mobilenets_2017} (\textit{MobileNet}) vorgestellt. Im Forschungsbereich der Computergrafik und \textit{Computer Vision} ist es ein weit verbreitetes Verfahren, separierbare Filter wie den Gaußschen Weichzeichner in zwei oder mehrere Teilfilter zu zerlegen, um die absolut benötigte Anzahl an Rechenoperationen zu reduzieren. Beim Weichzeichner resultiert so ein horizontales und ein vertikales Filter. Die Convolution-Operationen eines \acrshort{cnn}s können analog hierzu, da sie lediglich spezielle Matrixoperationen sind, auch als mathematisches Filter betrachtet und deren Berechnung durch tiefenweise Separation optimiert werden. Bei $3\times 3$-Convolution-Schichten kann die Berechnung so neunfach effizenter (und schneller) ausgeführt werden\footnote{\url{https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/} (Abgerufen am 22.5.2021)}. Weitere Strategien für die architekturelle Optimierung von \acrshort{cnn}s sind die Verwendung von speziellen \textit{Shuffle}-Schichten \cite{zhang_shufflenet_2018} (\textit{ShuffleNet}) oder die gezielte Gruppierung von Convolution-Schichten in \textit{Grouped Convolutions} \cite{huang_condensenet_2018} (\textit{CondenseNet}). Auch für \acrshort{rnn}s bestehen Ideen zur architekturellen Optimierung, beispielsweise durch eine effizientere Repräsentation der Matrix-Vektor-Multiplikationen \cite{wang_accelerating_2017} oder der Ersetzung der typischen \acrshort{lstm}-Neuronen durch kompaktifizierbarere Alternativen \cite{kusupati_fastgrnn_2019, luo_neural_2019}. Neben \acrshort{cnn}s und \acrshort{rnn}s lassen sich auch FFNs hinsichtlich ihrer Architektur optimieren. Beispielsweise lassen sich spezielle \textit{Multiplexing}-Verbindungen in das Netzwerk einfügen, um (unter leichter Verschlechterung des Zeitverhaltens) bis zu $50\%$ der sonst benötigten Hidden Layers und den dazugehörigen Neuronen einzusparen \cite{khalil_efficient_2018}.

\subsubsection{Verlustfreie Kompressionsverfahren}

Neben einer architekturellen Restrukturierung von Machine-Learning-Modellen wie neuronalen Netzwerken ist es auch möglich, diese für den Netzwerktransport und die persistente Speicherung verlustfrei zu komprimieren. Diese Technologie ist altbewährt, verlustfreie Kompressionsprogramme wie \texttt{gzip}\footnote{\url{https://www.gzip.org/} (Abgerufen am 22.5.2021)} existieren bereits seit mehreren Jahrzehnten und basieren häufig auf der theoretischen Grundlage der \textit{Huffman-Kodierung}\footnote{\url{https://de.wikipedia.org/wiki/Huffman-Kodierung} (Abgerufen am 22.5.2021)}. Die Machine-Learning-Modelle müssen dabei nicht zwingend mit plattformspezifischen Kompressionsverfahren verkleinert werden, sowohl iOS\footnote{\url{https://developer.apple.com/documentation/compression/compression_zlib} (Abgerufen am 22.5.2021)} als auch Android\footnote{\url{https://developer.android.com/reference/java/util/zip/Deflater} (Abgerufen am 22.5.2021)} unterstützen zum Beispiel das weit verbreitete \texttt{zlib}-Verfahren \cite[S. 315]{sosnovshchenko_machine_2018}.

\subsubsection{Verlustbehaftete Kompressionsverfahren}

Während bei der verlustfreien Kompression alle Parameter des Machine-Learning-Modells erhalten bleiben, werden bei der verlustbehafteten Kompression gezielt Parameter verändert oder entfernt. Ziel ist es hierbei, so viele Parameter wie möglich zu reduzieren, bei einer möglichst gleichbleibenden (oder in manchen Fällen sogar verbesserten) Präzision.

\paragraph{Quantisierung:} Die Parameter eines herkömmlichen Machine-Learning-Modells liegen typischerweise in Fließkommazahlen nach dem IEEE 754\footnote{\url{https://de.wikipedia.org/wiki/IEEE_754} (Abgerufen am 23.5.2021)} $32 bit$-Format vor. Diese können zur Reduktion der Modellgröße in Ganzzahlen nach dem $8 bit$ Integer-Format überführt werden. Rechnerisch lässt sich hierüber die Größe des Modells auf bis zu ein Viertel der Ausgangsgröße reduzieren. Gleichzeitig müssen die Fließkommazahlen über eine skalare Multiplikation geeignet in den Integer-Wertebereich $[-128, 127]$ (signed) bzw. $[0, 255]$ (unsigned) überführt werden\footnote{\url{https://de.wikipedia.org/wiki/Integer_(Datentyp)} (Abgerufen am 23.5.2021)}. Hierzu können eine Min-Max-Skalierung oder verschiedene andere Skalierungsmethoden dienen. Da sich die Skalierung bei Änderung der Modellparameter mit ändert, bietet sich die Quantisierung in der Regel für eine Komprimierung \textit{nach} dem Training an. Es bestehen aber auch Möglichkeiten, bereits beim Training eine laufende Quantisierung durchzuführen. Trotz des signifikanten Verlustes der durch die binäre Repräsentation darstellbaren Werten verringert eine Quantisierung die Präzision eines Modells normalerweise nur geringfügig, daher eignet sie sich sehr gut für die verlustbehaftete Komprimierung \cite[S. 14ff]{liang_pruning_2021}.

\paragraph{Pruning:} Nicht alle Parameter eines Machine-Learning-Modells sind für die interne Wissensrepräsentation gleich wichtig. Es besteht die Möglichkeit, dass bestimmte Parameter nur einen geringen Einfluss auf die Ausgabe des Modells haben. Bei künstlichen neuronalen Netzwerken können so beispielsweise gezielt einzelne Neuronen, Gruppen von Neuronen oder ganze Schichten entfernt werden, wenn sie als unwichtig identifiziert werden können. Letzteres ist jedoch ein nichttriviales Problem, welches eine differenzierte Herangehensweise erfordert. Es ist möglich, über die zufällige Elimination von Neuronen und Verbindungen zu testen, ob diese die gesamte Präzision signifikant reduziert (\textit{Brute-Force}-Methode). Durch Heuristiken lässt sich dieses Verfahren jedoch weiter verbessern. Bei einer sehr einfachen Variante des \textit{Magnitude-based Pruning} werden beispielsweise Parameter innerhalb eines Machine-Learning-Modells entfernt, welche sehr kleine Werte nahe $0$ annehmen. Die Annahme hierhinter ist, dass Parameter mit sehr kleinen Werten in der Regel weniger wichtig für die Ausgabe des Modells sind, als Parameter mit größeren Werten. Ähnliche Methoden wie \textit{Optimal Brain Damage} \cite{lecun_optimal_1990} und \textit{Optimal Brain Surgeon} \cite{hassibi_second_1993} nutzen den Gradienten der Kostenfunktion für die Identifikation einzelner zu eliminierender Neuronen. Bei der Elimination von Neuronen ist auch die Regularisierung von Bedeutung, durch Regularisierungsmethoden (\Cref{sec:regularisierung}) wie Dropout oder $l_1$- und $l_2$-Regularisierung kann diese beschleunigt werden \cites[S. 7ff]{liang_pruning_2021}{chang_prune_2019}.

\paragraph{Knowledge Distillation:} Methoden wie Quantisierung und Pruning können auf einem bestehenden Modell direkt angewandt werden, um dieses durch die Elimination von potenziell unwichtigen Parametern zu komprimieren. Beim Verfahren \textit{Knowledge Distillation} wird ein bestehendes (trainiertes) Modell genutzt, um dessen Verhaltensweise durch ein kleineres Modell nachzustellen. Ziel ist es, eine effizientere Wissensrepräsentation durch das kleinere Modell zu erstellen, welche weniger Parameter bei einer annähernd gleichen Präzision besitzt.

Die diskutierten Methoden sind in dieser Form für die meisten Machine-Learning-Modelle flexibel anwendbar und dadurch auch weit verbreitet \cite{nan_deep_2019, choudhary_comprehensive_2020}. Die aufgezeigten Grundlagen erfassen jedoch nicht das gesamte Spektrum an potenziellen Ansätzen. Weitere Ansätze sind beispielsweise:

\begin{itemize}
\item Optimierung des Machine-Learning-Modells für die Inferenz durch die Elimination von Komponenten, die lediglich für das Training benötigt werden.
\item Approximation der Modellparameter durch \textit{Low Rank Tensor Approximation} und die daraus resultierende Verkleinerung der Matrixoperationen\footnote{\url{https://en.wikipedia.org/wiki/Low-rank_approximation} (Abgerufen am 23.5.2021)}.
\item Fusion von bestimmten Modell-Operationen (\textit{Layer/Tensor-Fusion})\footnote{\url{https://www.tensorflow.org/lite/convert/operation_fusion} (Abgerufen am 23.5.2021)} zur weiteren Optimierung und Komprimierung der Berechnungen.
\item Verschiedene hardwareorientierte Ansätze wie die Einführung einer dynamischen Speicherverwaltung, um größere Machine-Learning-Modelle effizient und parallelisiert auf Grafikeinheiten und dedizierten Koprozessoren ausführen zu können \cite{sb_dynamic_2019}.
\end{itemize}

Im Rahmen der Portierung bietet es sich in Anbetracht der verschiedensten Methoden an, eine konkrete Kompressionsstrategie zu entwickeln, welche einen oder mehrere dieser Schritte inkludiert. Dabei kann auch eine Selektion spezifischerer Varianten und Derivate der gezeigten Optimierungsmethoden gewählt werden.

\subsection{Profiling}

Die Ergebnisqualität eines Machine-Learning-Modells lässt sich anhand der in \Cref{sec:modellvalidierung} diskutierten Metriken beurteilen. Die Messung von Metriken wie dem $F1$-Score oder die Erstellung einer Konfusionsmatrix ist nicht zwingend direkt auf dem Smartphone notwendig, sofern das Modell in seiner Funktionsweise nicht zwischen Test und Deployment abgewandelt wird. Auch Parameter wie die statische Größe des Modells auf dem Festspeicher sind ohne ein Test-Smartphone messbar. Für andere Ressourcenparameter wiederum ist ein \textit{Profiling} des Modells direkt auf dem Test-Smartphone notwendig. Insbesondere sind dies der dynamische Speicherverbrauch im Arbeitsspeicher während der Ausführung, die Auslastung der CPU-Kerne und eventuell genutzter Koprozessoren bzw. GPU-Kerne und unmittelbar daraus entstehende Faktoren wie der Energieverbrauch und die Messung der Hardwaretemperatur. Um dies zu realisieren, können die in eine App eingebetteten Machine-Learning-Modelle in Kooperation mit entsprechenden Entwicklertools wie \textit{XCode}, \textit{Instruments} (iOS)\footnote{\url{https://developer.apple.com/library/archive/documentation/Performance/Conceptual/EnergyGuide-iOS/MonitorEnergyWithInstruments.html} (Abgerufen am 23.5.2021)} oder dem \textit{Android Profiler}\footnote{\url{https://developer.android.com/studio/profile/android-profiler} (Abgerufen am 23.5.2021)} ausgeführt und die genannten Ressourcenparameter ausgelesen werden. Insbesondere mit Hinblick auf den Vergleich zwischen mehreren Modellen ist es hierbei jedoch wichtig, dass die Testbedingungen möglichst konstant sind, dies muss bei der Evaluation des Tradeoffs zwischen Ressourcenbedarf und Ergebnisqualität später unbedingt einbezogen werden. Beispielsweise sollte eine konstante Temperatur und Energieversorgung sichergestellt werden, um eine zu starke Variation der Prozessorgeschwindigkeit im Smartphone-internen Regelsystem zu vermeiden. Über die diskutierten Parameter hinaus können noch zwei weitere Parameter evaluiert werden, genauer die Dauer und die Latenz der Inferenz. Dies kann über zeitlich messbare Unit-Tests\footnote{\url{https://developer.apple.com/documentation/xctest/xctestcase/1496290-measureblock} (Abgerufen am 23.5.2021)} des Machine-Learning-Modells geschehen, wobei diese Unit-Tests auch wieder unter möglichst konstanten Testbedingungen auf dem Smartphone ausgeführt werden sollten, um ein möglichst aussagekräftiges Ergebnis zu erhalten.
