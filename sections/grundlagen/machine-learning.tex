\section{Machine Learning}\label{sec:machine-learning}

In \Cref{sec:datenerfassung-und-vorverarbeitung} wurde detailliert beschrieben, wie sich die Aktivitätsdaten innerhalb der STADTRADELN-App strukturieren und welche Herausforderungen bei der Weiterverarbeitung der Daten zu beachten sind. In diesem Abschnitt sollen nun Methoden vorgestellt werden, mithilfe derer eine Interpretation der vorverarbeiteten Daten möglich ist.

\subsection{Verkehrsmittelerkennung}

Die Erkennung des Verkehrsmittels anhand der beschriebenen Daten, wie sie \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} umgesetzt haben, lässt sich als ein Problem der \textit{Human Activity Recognition (HAR)} interpretieren. Hierbei liegen Aktivitätsdaten einer jeweiligen Person vor, anhand dessen die ausgeführte Aktivität klassifiziert werden soll. Die formelle Definition der Verkehrsmittelerkennung ist wie folgt. Sei $D = \{d_0, \dots, d_n\} \subseteq \mathbf{R}^n$ die chronologisch sortierte Menge aller vorverarbeiteten und anhand einer Interpolation auf dieselbe Abtastrate synchronisierten Datenpunkte. Wähle ein zu klassifizierendes Segment $S \subseteq D$ der Länge $n_S$. Dann ist die Abbildung $VME(S): D^{n_S} \rightarrow L$ zu finden, wobei $L$ die Menge der möglichen \textit{Labels} von Verkehrsmitteln wie \textit{Fahrrad}, \textit{Bahn}, \textit{Bus}, \textit{zu Fuß} und weiteren ist. Durch die Aufteilung von $D$ in Segmente fester Länge können anschließend mithilfe der Abbildung $VME(S)$ entlang $D$ die genutzten Verkehrsmittel rekonstruiert werden.

\subsubsection{Segmentierung}

In der obigen Problemdarstellung wurde bereits vorweggenommen, dass die Datenpunkte gleichmäßig in Segmente unterteilt werden sollen, die sich auch überlappen können. Dieses Verfahren wird als Gleitfenstermethode bezeichnet. Über die zeitliche Kombination von mehreren aufeinanderfolgenden Datenpunkten (auch als Zeitlinie bezeichnet) können wertvolle Informationen erhalten werden, wie sich zeitlich wiederholende Bewegungsabläufe \cite[S. 4]{banos_window_2014}. Gleichzeitig sollen Wechsel zwischen den Verkehrsmitteln innerhalb der Datenpunktmenge erkannt werden, was mit einer ganzheitlichen Klassifikation der Datenpunktmenge nicht möglich wäre. \\

\todo{Abbildung der Segmente + Datenpunktvektoren einfügen}

\noindent Die Segmentlänge $n_S$ ist hierbei für die Berechnung der Erkennung von entscheidender Bedeutung. \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020} betrachtet Segmente von $10s$ bis $180s$ Länge. Je kleiner $n_S$ gewählt wird, desto höher ist die zeitliche Granularität der Erkennung. Gleichzeitig lässt sich vermuten, dass die Präzision der Klassifikation bei kürzeren Segmentlängen geringer ist, wie die experimentellen Daten aus \cite{matusek_anwendung_2019} suggerieren. Wie in \Cref{sec:datenerfassung-und-vorverarbeitung} gezeigt, ist es durch Interpolation der verschiedenen Spuren (GNSS, Akzelerometer, Gyrosensor, Magnetometer) möglich, diese im Rahmen der Synchronisation zu beliebigen Raten abzutasten. Bei näherer Betrachtung fällt hierbei auf, dass \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020} Datenpunkte mit $1Hz$ (Abtastrate des GNSS-Systems) abtasten, die mit deutlich höheren Frequenzen im dreistelligen Hertz-Bereich aufgenommenen Beschleunigungs-, Auslenkungs- und Orientierungsdaten also stark reduziert werden (Downsampling). Vermutlich bietet dies eine Möglichkeit zur Verbesserung der Konzepte aus \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020}, unter der Berücksichtigung, dass mit $1Hz$ Abtastrate lediglich Bewegungsabläufe mit Frequenzen $f \leq 0.5Hz$ nach dem Nyquist-Shannon-Abtasttheorem\footnote{\url{https://de.wikipedia.org/wiki/Nyquist-Shannon-Abtasttheorem} (Abgerufen am 14.4.2021)} erhalten bleiben, Frequenzen $f > 0.5Hz$ tragen zum Rauschen der abgetasteten Daten bei. Alternativ wäre daher auch ein Upsampling der korrigierten GNSS-Daten auf eine höhere Frequenz von ca. $100 Hz$ möglich. \cite{banos_window_2014} konnten zeigen, dass sich unter dieser Voraussetzung insbesondere kürzere Segmente von $0.25s$ bis $2s$ Länge für die Aktivitätserkennung eignen, gegenüber den von \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020} verwendeten Segmentlängen von $10s$ bis $180s$.

\subsubsection{Feature-Engineering}

In den vorigen Sektionen wurde bereits diskutiert, wie Daten entsprechender Smartphone-Sensorsysteme akquiriert werden können, um diese schließlich bezüglich ihrer Fehlercharakteristika vorzuverarbeiten, mithilfe von Methoden zur Filtrierung, Korrektur und Standardisierung. Hieraus werden, wie in der vorigen Sektion erläutert, Segmente der Länge $n_S$ gebildet. Zur besseren Verständlichkeit wurde bisher davon ausgegangen, dass diese Segmente analog zur Abbildungsdefinition $VME(S): D^{n_S} \rightarrow L$ direkt als Input für das Machine-Learning-Modell dienen, um schließlich die Abbildung auf die Label-Menge $L$ zu realisieren. Je nach Architektur des Modells ist es jedoch üblich, einen weiteren Zwischenschritt zu integrieren, der als \textit{Feature-Extraktion} bezeichnet wird. Hierbei werden die im Segment enthaltenen Datenpunkte $d \in D$ auf einen \textit{Feature-Space} $F \in \mathbf{R}^{n_F}$ abgebildet, welcher dann nachfolgend als Input für das Machine-Learning-Modell dient.

\begin{equation}\label{eq:feature-extraktion}
d_{t=0} =
%
  \begin{bmatrix}
  ACC(t=0)_x \\ \vdots \\ MAG(t=0)_z
  \end{bmatrix}
%
   \xrightarrow{\text{Feature-Extraktion}}
%
  \begin{bmatrix}
  | ACC(t=0) | \\ \vdots \\ | MAG(t=0) | \\
  \vdots \\ FEATURE_{n_F}(S, t=0)
  \end{bmatrix}
%
\end{equation}
wobei:
\begin{conditions}
  d_{t=0} & Datenpunkt an der Stelle $t=0$ des Segments $S$
\end{conditions}

\noindent \Cref{eq:feature-extraktion} zeigt das Schema einer solchen Feature-Extraktion. Die bestehenden Daten können zusammengefasst werden, wie beispielsweise $|ACC(t=0)|$ die absolute Länge des Beschleunigungsvektors zusammenfasst. Dies stellt bereits ein simples Feature dar. Gleichzeitig können neue Informationen in Relation zum Segment ergänzt werden, in \Cref{eq:feature-extraktion} als $FEATURE_{n_F}(S, t=0)$ symbolisiert. Der Zweck dieser Abbildung ist die semantische Aufbereitung der Datenpunkte, mit dem Ziel einer Verbesserung der Erkennung. Welche Features hierbei konkret berechnet werden sollen, wird im Prozess des \textit{Feature-Engineering} bestimmt \cite{matusek_anwendung_2019}. Für den Kontext der Verkehrsmittelerkennung lassen sich prinzipiell zwei Gruppen von Features unterscheiden.

\paragraph{Shallow Features \cite{ravi_deep_2017}:} Diese Features werden direkt aus dem vorliegenden Segment abgeleitet und sind typischerweise für alle Datenpunktvektoren eines Segments identisch. Häufig verwendete shallow Features sind der Durchschnitt und die Standardabweichung bestimmter Werte eines Segments \cite{chen_deep_2015, banos_window_2014, kwapisz_activity_2011, jahangiri_applying_2015, nurhanim_classification_2017}. Diese Features beziehen sich direkt auf die Zeitlinie. Sie werden daher auch als \textit{Time Domain Features} eingeordnet \cite{chen_deep_2015}. Alternativ ist es auch üblich, sogenannte \textit{Frequency Domain Features} durch eine vorige Fourier-Transformation des Segments zu berechnen \cite{zeng_convolutional_2014, chen_deep_2015}.

\paragraph{Non-Shallow Features \cite{ravi_deep_2017}:} Während shallow Features verhältnismäßig einfach zu implementieren sind, können sie wiederum möglicherweise keine komplexen Zusammenhänge innerhalb des Segments repräsentieren \cite{abu_alsheikh_deep_2015}. Bei \textit{Non-Shallow Features}, auch bezeichnet als \textit{Data-Driven Features} oder \textit{Deep Learning Features} \cite{abu_alsheikh_deep_2015}, erfolgt die Berechnung durch Zwischenschaltung eines Machine-Learning-Modells, welches bestimmte Eingangswerte des Segments in einen Feature-Vektor überführt (Enkodierung). \cite{ravi_deep_2017} überführen beispielsweise die Segmentdaten in ein Spektrogramm, um dieses über ein Machine-Learning-Modell zu enkodieren. \cite{zeng_convolutional_2014} verwenden sogar mehrere Machine-Learning-Modelle verschiedener Art. \\

\noindent Die obigen Features lassen sich hierbei auch miteinander kombinieren, um weitere Features zu erzeugen. Für die auf Machine-Learning basierte Aktivitätserkennung nutzen \cite{kwapisz_activity_2011} beispielsweise insgesamt 43 verschiedene erzeugte Features. Auch hier ist jedoch wieder zwischen dem potenziellen Informationszugewinn und der erhöhten Dimensionalität des Feature-Vektors und der damit verbundenen Berechnungskomplexität der Aktivitätserkennung abzuwägen. Auch bei den berechneten Features ist zu beachten, dass diese je nach Architektur des Machine-Learning-Modells normalisiert oder standardisiert werden sollten (siehe \Cref{sec:datenerfassung-und-vorverarbeitung}).

\subsubsection{Einordnung in klassische Probleme des Machine Learnings}

Neben mannigfaltigen Fehlerursachen (in \Cref{sec:datenerfassung-und-vorverarbeitung} erläutert) ist auch die kontextbedingte Komplexität der ermittelten Features denkbar hoch. Smartphones können zum Beispiel in verschiedenen Positionen (Hand, Hosentasche, Rucksack, ...) am Körper bzw. am/im Verkehrsmittel getragen werden. Weitere Ursachen für zwischen Messungen gleicher Aktivität abweichenden Daten sind unterschiedlich gebaute Verkehrsmittel (zum Beispiel Rennrad vs. Mountainbike), unterschiedliche anatomische Gegebenheiten und Bewegungsmuster der jeweiligen Person, sowie die Art des Untergrundes (zum Beispiel Schotterweg vs. asphaltierte Straße) oder die allgemeine Beschaffenheit des Geländes. Daher besteht ein weiteres zentrales Problem in der Realisation der Abbildung $VME(S): D^{n_S} \rightarrow L$. Eine einfache zustandslose Implementation dieser Abbildung wäre denkbar. Hierfür könnten beispielsweise Geschwindigkeitsbereiche auf $L$ abgebildet werden, wie $\varnothing v \geq 40\frac{km}{h} \rightarrow$ \enquote{Auto fahren} oder $\varnothing v \leq 5\frac{km}{h} \rightarrow$ \enquote{zu Fuß}. Bei der Betrachtung dieses Ansatzes lassen sich jedoch schnell Fehlerszenarien konstruieren, in welchen diese Abbildung fehlschlägt, beispielsweise eine dichte Verkehrslage mit ständigem Abbremsen und Beschleunigen, oder eine besonders schnelle Fahrradfahrt bergab. Bei der Behandlung dieser Fehlerszenarien werden bereits komplexere Überlegungen notwendig, welche möglicherweise neue Fehlerszenarien erzeugen. Je näher die zu erreichende Präzision des Systems an $100\%$ liegen soll, desto komplexer würde ein solches System vermutlich werden. Für solche Probleme, die auch durch hochkomplexe algorithmische Lösungen nicht hinreichend gelöst werden können, haben sich verschiedene Methoden aus dem Forschungsspektrum der künstlichen Intelligenz etabliert.

\begin{itemize}
\item Beim \textbf{Clustering} steht die Partitionierung einer Menge von Werten in geeignete Untergruppen im Vordergrund. Verfahren für das Clustering sind beispielsweise k-Means-Clustering \cite{likas_global_2003}, t-SNE \cite{van_der_maaten_laurens_and_hinton_geoffrey_visualizing_2008} oder PCA \cite{ringner_what_2008}.
\item Die \textbf{Regression} behandelt Probleme, deren Hauptgegenstand die Nachbildung eines Kausalzusammenhangs oder einer Korrelation in einer Menge von Werten ist. Im Zentrum steht hierbei das Ziel, die Abweichung der Regression von den Daten zu minimieren.
\item Bei der \textbf{Generation} sollen neue Daten anhand eines Vorbildes erzeugt werden. Ziel ist es, bei einer Eingabe von bestimmten Daten möglichst plausible weitere Daten zu erzeugen. Klassische Anwendungsbereiche umfassen KI-Systeme für die Chat-basierte Texterzeugung \cite{brown_language_2020} oder auch das Supersampling von Bildern \cite{burgess_rtx_2020, park_semantic_2019}.
\item Möglich ist auch die \textbf{Transformation}, wenn statt ähnlichen Daten (wie bei der Generation) andersartige Daten erzeugt werden sollen. Ein charakteristischer Anwendungsbereich ist die Realisation von \textit{Text-to-Speech}- oder \textit{Speech-to-Text}-Systemen \cite{wang_tacotron_2017}.
\item Im Rahmen der \textbf{Klassifikation} werden Eingabedaten in bestimmte Klassen eingeteilt, bei einer Maximierung der Anzahl korrekter Einordnungen.
\end{itemize}

\noindent Zusammenfassend handelt es sich hierbei um Verfahren, bei denen mathematische Modelle anhand vorliegender Daten angelernt werden (Induktion, Training), um schließlich zur Lösung des vorliegenden Problems verwendet zu werden (Deduktion, Inferenz). Daher werden diese Verfahren auch als \textit{Machine-Learning}-Methoden bezeichnet. Statt algorithmische Verfahren zu entwickeln, welche ein komplexes Problem (wie oben illustriert) möglicherweise nur begrenzt lösen können, ist also die Grundidee der Machine-Learning-Methoden die Repräsentation einer Abbildung von \textit{Input} zu \textit{Output} über ein angelerntes mathematisches Modell. \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} konnten zeigen, dass solche Machine-Learning-Methoden zur Implementation der eingangs gezeigten Abbildung $VME(S): D^{n_S} \rightarrow L$ geeignet sind. Die Realisation der Abbildung von Input zu Output geschieht bei Machine-Learning-Modellen über ein mehr oder weniger komplexes System von mathematischen Parametern und Funktionen. Die Berechnung der Ausgabe (Inferenz) eines Machine-Learning-Modells lässt sich somit einfach realisieren, indem die Eingabewerte mithilfe der Parameter und Funktionen zum Ausgang überführt werden. Zentrale Probleme bestehen jedoch darin, eine geeignete Architektur für die Parameter und Funktionen zu wählen, sowie das so erstellte Modell zu trainieren. Diese zwei Probleme sollen in den folgenden Sektionen näher erläutert werden.

\subsection{Training von Machine-Learning-Modellen}

\todo{In dieser Sektion fehlen Einzelnachweise. Diese müssen noch ergänzt werden.}

In Abhängigkeit von der zu lösenden Aufgabe bieten sich verschiedene Lernverfahren an. Ziel ist es hierbei jedoch stets, das Machine-Learning-Modell so anhand der Eingangsdaten anzupassen, dass die Aufgabe innerhalb des Systems bestmöglich erfüllt werden kann. Typische Lernverfahren sind das \textit{Reinforcement Learning}, das \textit{Supervised Learning} und das \textit{Unsupervised Learning}. Diese Verfahren sollen nun vorgestellt werden.

\subsubsection{Reinforcement Learning}

Beim Reinforcement Learning werden auf Grundlage der Entscheidungen eines Machine-Learning-Modells \enquote{Belohnungen} und \enquote{Bestrafungen} definiert. Ziel ist die Maximierung eines Scores, der sich aus den gesammelten Belohnungen und Bestrafungen bildet. Dazu werden mehrere Modelle gebildet, welche sich in ihrer Parametrisierung leicht unterscheiden. Die Modelle werden getestet (meist durch Simulation) und anhand der definierten Belohnungen und Bestrafungen bewertet. Die besten Modelle werden ausgewählt und bieten die Grundlage für die nächste Generation. Hierbei wird angenommen, dass die Modelle analog zur Evolutionstheorie durch Zufall gewünschte Strategien durch Mutation hinzu erlernen. Für Klassifikationssysteme wie eine Verkehrsmittelerkennung ist dieses Lernverfahren jedoch eher unüblich.

\subsubsection{Unsupervised Learning}

Beim Unsupervised Learning wird ein Machine-Learning-Modell auf Daten trainiert, mit dem Ziel, innerhalb der Daten Muster zu erkennen bzw. diese zu reproduzieren. Das Training erfolgt ohne bekannte Labels für die Daten. Das Modell wird iterativ so angepasst, dass es eine abstrakte interne Repräsentation der Daten erlernt. Das Verfahren wird daher typischerweise für die Komprimierung von Daten (\textit{Sparse Coding}) oder die Segmentierung von Daten (durch Unterteilung der Daten in Gruppen mit ähnlichen Mustern) angewandt. Das Modell entwickelt hierbei durch Anpassung der Modellparameter Vermutungen über die Muster eines Datensatzes. Die Modellparameter werden anhand der beobachteten Daten iterativ so angepasst, dass das Modell die Daten möglichst präzise nachbildet oder segmentiert.

\subsubsection{Supervised Learning}

Ähnlich zum Unsupervised Learning wird beim Training von Machine-Learning-Modellen durch Supervised Learning eine zu minimierenden Kostenfunktion definiert. Eine typische Kostenfunktion ist der \textit{Root-Mean-Square Error}\footnote{\url{https://en.wikipedia.org/wiki/Root-mean-square_deviation} (Abgerufen am 6.5.2021)} (RMSE). Die Kostenfunktion soll beschreiben, wie stark das aktuelle Modell (anhand dessen Vorhersage) von dem Zielwert abweicht. Die Kostenfunktion wird minimiert, indem die mathematischen Parameter des Machine-Learning-Modells iterativ in Abhängigkeit von ihrem Beitrag zum Fehler angepasst werden.

\subsubsection{Erweiterte Lernverfahren}

Die oben erläuterten Lernverfahren bilden die drei wesentlichen Herangehensweisen beim Training von Machine-Learning-Modellen. Je nach der zu erfüllenden Aufgabe und den vorliegenden Daten ist eine geeignete Auswahl dieser drei Grundverfahren zum Training des Machine-Learning-Modells zu treffen. Für eine Verkehrsmittelerkennung bietet sich in erster Linie das Supervised Learning an, welches auch in \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} zum Einsatz kommt. Hierfür müssen Datensätze im Voraus händisch mit den zu erkennenden Verkehrsmittel-Klassen ausgestattet werden. Für die zahlreichen bestehenden Datensätze aus dem Movebis-Projekt ist dies jedoch unter Umständen nicht möglich. Hierbei gilt grundsätzlich, dass die Menge der Daten beim Supervised Learning je nach Komplexität und Architektur des Machine-Learning-Modells ausreichend umfangreich sein muss. Um die benötigte Menge an gelabelten Daten zu reduzieren, kann ein erweitertes Lernverfahren eingesetzt werden, das \textit{Semi-Supervised Learning}. Bei diesem Lernverfahren wird zunächst Unsupervised Learning auf den nicht gelabelten Daten durchgeführt, um das Machine-Learning-Modell auf möglichen Mustern innerhalb der Daten vorzutrainieren. Das Machine-Learning-Modell soll so bereits ein abstraktes Grundverständnis der Eingaben erhalten, um anschließend auf den händisch erstellten Daten durch Supervised Learning auf die zu erkennenden Klassen \enquote{umtrainiert} zu werden. Das \enquote{Umtrainieren} von Machine-Learning-Modellen wird im breiteren Sinn auch als \textit{Transfer Learning} bezeichnet.

\subsubsection{Gradientenabstiegsverfahren}

Je nach Art des Machine-Learning-Modells und des Lernverfahrens existieren zahlreiche Möglichkeiten der iterativen Anpassung, um das Modell anzulernen. Von zentraler Bedeutung für Architekturen, wie sie in \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} verwendet werden, sind Gradientenabstiegsverfahren zur Minimierung einer Kostenfunktion, vor allem beim Supervised Learning. Die Minimierung einer Kostenfunktion ist ein mathematisches Optimierungsproblem. Ziel ist es, die Parameter des Machine-Learning-Modells so anzupassen, dass die Kosten minimal werden. Bei einem kleinen Zustandsraum (aufgespannt durch die Modellparameter) ist dies durch einfache analytische Verfahren realisierbar. Bei vielen Tausenden oder Millionen von Modellparametern eignen sich wegen der zunehmenden Komplexität der Berechnung sogenannte Gradientenabstiegsverfahren. Diesen zugrundeliegend ist die Idee, dass sich bei der numerischen Variation der einzelnen Modellparameter die Kosten vergrößern oder verkleinern. Dieser lineare Zusammenhang kann als Gradient betrachtet werden. Beim Gradientenabstiegsverfahren werden die Modellparameter iterativ so angepasst, dass sich zur Verringerung der Kosten entlang des steilsten Gefälles am Gradient nach unten bewegt wird.

\todo{Symbolbild einfügen}

\noindent Die Intensität (Schrittweite auf dem Gradienten), mit der die Modellparameter bei jedem neuen Datenpunkt angepasst werden, wird als \textit{Learning Rate} bezeichnet. Die Learning Rate ist ein wichtiger Hyperparameter von Machine-Learning-Systemen. Ein zentrales Problem von Gradientenabstiegsverfahren ist, dass Gradienten neben einem zu erreichenden globalem Minimum auch zahlreiche lokale Minima besitzen können. Ist die Learning Rate zu klein, so lernt das Modell verhältnismäßig langsam und läuft gleichzeitig Gefahr, in ein lokales Minimum des Gradienten zu fallen (Underfitting). Ist die Learning Rate jedoch zu hoch, besteht das Risiko, dass aus Minima \enquote{herausgesprungen} wird und das Modell nicht mehr ausreichend auf neuen Daten generalisiert (Overfitting). Zentraler Teil des Trainings von Machine-Learning-Modellen ist es daher, das Lernverhalten zu beobachten und ggf. ein \textit{Hyperparameter Tuning} durchzuführen. Diese Betrachtung ist jedoch vereinfacht und dient nur zur Veranschaulichung des Funktionsprinzips. Bei erweiterten Optimierungsverfahren wie dem \textit{ADAM-Optimizer} wird die Learning Rate beispielsweise pro Modellparameter individuell angepasst.

\subsection{Modellvalidierung}

Da Machine-Learning-Modelle durch das Erlernen einer abstrakten internen Repräsentation der Abbildung von Eingaben auf Ausgaben trainiert werden, ist die Erklärung und Validierung von deren Funktion ein nichttriviales Problem, mit dem sich unter anderem der Forschungsbereich \textit{Explainable AI} (XAI) auseinandersetzt. Durch direkte Beobachtung der Modellparameter ist es in Abhängigkeit der Modellkomplexität schwer, zu determinieren, ob ein Machine-Learning-Modell erwünschte oder unerwünschte Muster lernt. Um ein mögliches Overfitting oder Underfitting zu erkennen, muss das Modell während des Trainings kontinuierlich mithilfe von konkreten Metriken validiert werden. Lassen diese Metriken erkennen, dass das Modell ein solches unerwünschtes Verhalten zeigt, können die relevanten Hyperparameter angepasst werden. Zeigt sich beim Training, dass dessen Fortführung zu einem unerwünschten Rückschritt führen würde, kann das Training vorzeitig beendet werden. Dies ist auch bekannt unter dem Fachbegriff \textit{Early Stopping}.

% \subsubsection{Kreuzvalidierung}

% \subsubsection{Metriken}

% \todo{FN, FP, Recall, Precision, F1-Score, Activation Matrix, Confusion Matrix}

% \subsection{Modellarchitekturen}

% \subsubsection{Decision Trees und Random Forests}

% \subsubsection{Support Vector Machines}

% \subsubsection{Feed-Forward Neural Networks}

% \subsubsectino{Recurrent Neural Networks}

% \subsubsection{Convolutional Neural Networks}
