\section{Machine Learning}\label{sec:machine-learning}

In \Cref{sec:datenerfassung-und-vorverarbeitung} wurde detailliert beschrieben, wie sich die Aktivitätsdaten innerhalb der STADTRADELN-App strukturieren und welche Herausforderungen bei der Weiterverarbeitung der Daten zu beachten sind. In diesem Abschnitt sollen nun Methoden vorgestellt werden, mithilfe derer eine Interpretation der vorverarbeiteten Daten möglich ist.

\subsection{Verkehrsmittelerkennung}

Die Erkennung des Verkehrsmittels anhand der beschriebenen Daten, wie sie \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} umgesetzt haben, lässt sich als ein Problem der \textit{Human Activity Recognition (HAR)} interpretieren. Hierbei liegen Aktivitätsdaten einer jeweiligen Person vor, anhand dessen die ausgeführte Aktivität klassifiziert werden soll. Die formelle Definition ist wie folgt. Sei $D = \{d_0, \dots, d_n\} \subseteq \mathbf{R}^n$ die chronologisch sortierte Menge aller vorverarbeiteten und anhand einer Interpolation auf dieselbe Abtastrate synchronisierten Datenpunkte. Wähle ein zu klassifizierendes Segment $S \subseteq D$ der Länge $n_S$. Dann ist die Abbildung $HAR(S): D^{n_S} \rightarrow L$ zu finden, wobei $L$ die Menge der möglichen \textit{Labels} von Verkehrsmitteln wie \textit{Fahrrad}, \textit{Bahn}, \textit{Bus}, \textit{zu Fuß} und weiteren ist. Durch die Aufteilung von $D$ in Segmente fester Länge können anschließend mithilfe der Abbildung $HAR(S)$ entlang $D$ die genutzten Verkehrsmittel rekonstruiert werden.

\subsubsection{Segmentierung}

In der obigen Problemdarstellung wurde bereits vorweggenommen, dass die Datenpunkte gleichmäßig in Segmente unterteilt werden sollen, die sich auch überlappen können. Dieses Verfahren wird als Gleitfenstermethode bezeichnet. Über die zeitliche Kombination von mehreren aufeinanderfolgenden Datenpunkten (auch als Zeitlinie bezeichnet) können wertvolle Informationen erhalten werden, wie sich zeitlich wiederholende Bewegungsabläufe \cite[S. 4]{banos_window_2014}. Gleichzeitig sollen Wechsel zwischen den Verkehrsmitteln innerhalb der Datenpunktmenge erkannt werden, was mit einer ganzheitlichen Klassifikation der Datenpunktmenge nicht möglich wäre. \\

\todo{Abbildung der Segmente + Datenpunktvektoren einfügen}

\noindent Die Segmentlänge $n_S$ ist hierbei für die Berechnung der Erkennung von entscheidender Bedeutung. \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020} betrachtet Segmente von $10s$ bis $180s$ Länge. Je kleiner $n_S$ gewählt wird, desto höher ist die zeitliche Granularität der Erkennung. Gleichzeitig lässt sich vermuten, dass die Präzision der Klassifikation bei kürzeren Segmentlängen geringer ist, wie die experimentellen Daten aus \cite{matusek_anwendung_2019} suggerieren. Wie in \Cref{sec:datenerfassung-und-vorverarbeitung} gezeigt, ist es durch Interpolation der verschiedenen Spuren (GNSS, Akzelerometer, Gyrosensor, Magnetometer) möglich, diese im Rahmen der Synchronisation zu beliebigen Raten abzutasten. Bei näherer Betrachtung fällt hierbei auf, dass \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020} Datenpunkte mit $1Hz$ (Abtastrate des GNSS-Systems) abtasten, die mit deutlich höheren Frequenzen im dreistelligen Hertz-Bereich aufgenommenen Beschleunigungs-, Auslenkungs- und Orientierungsdaten also stark reduziert werden (Downsampling). Vermutlich bietet dies eine Möglichkeit zur Verbesserung der Konzepte aus \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020}, unter der Berücksichtigung, dass mit $1Hz$ Abtastrate lediglich Bewegungsabläufe mit Frequenzen $f \leq 0.5Hz$ nach dem Nyquist-Shannon-Abtasttheorem\footnote{\url{https://de.wikipedia.org/wiki/Nyquist-Shannon-Abtasttheorem} (Abgerufen am 14.4.2021)} erhalten bleiben, Frequenzen $f > 0.5Hz$ tragen zum Rauschen der abgetasteten Daten bei. Alternativ wäre daher auch ein Upsampling der korrigierten GNSS-Daten auf eine höhere Frequenz von ca. $100 Hz$ möglich. \cite{banos_window_2014} konnten zeigen, dass sich unter dieser Voraussetzung insbesondere kürzere Segmente von $0.25s$ bis $2s$ Länge für die Aktivitätserkennung eignen, gegenüber den von \cite{matusek_anwendung_2019} und \cite{werner_kontinuierliche_2020} verwendeten Segmentlängen von $10s$ bis $180s$.

\subsubsection{Feature-Engineering}

In den vorigen Sektionen wurde bereits diskutiert, wie Daten entsprechender Smartphone-Sensorsysteme akquiriert werden können, um diese schließlich bezüglich ihrer Fehlercharakteristika vorzuverarbeiten, mithilfe von Methoden zur Filtrierung, Korrektur und Standardisierung. Hieraus werden, wie in der vorigen Sektion erläutert, Segmente der Länge $n_S$ gebildet. Zur besseren Verständlichkeit wurde bisher davon ausgegangen, dass diese Segmente analog zur Abbildungsdefinition $HAR(S): D^{n_S} \rightarrow L$ direkt als Input für das Machine-Learning-Modell dienen, um schließlich die Abbildung auf die Label-Menge $L$ zu realisieren. Je nach Architektur des Modells ist es jedoch üblich, einen weiteren Zwischenschritt zu integrieren, der als \textit{Feature-Extraktion} bezeichnet wird. Hierbei werden die im Segment enthaltenen Datenpunkte $d \in D$ auf einen \textit{Feature-Space} $F \in \mathbf{R}^{n_F}$ abgebildet, welcher dann nachfolgend als Input für das Machine-Learning-Modell dient.

\begin{equation}\label{eq:feature-extraktion}
d_{t=0} =
%
  \begin{bmatrix}
  ACC(t=0)_x \\ \vdots \\ MAG(t=0)_z
  \end{bmatrix}
%
   \xrightarrow{\text{Feature-Extraktion}}
%
  \begin{bmatrix}
  | ACC(t=0) | \\ \vdots \\ | MAG(t=0) | \\
  \vdots \\ FEATURE_{n_F}(S, t=0)
  \end{bmatrix}
%
\end{equation}
wobei:
\begin{conditions}
  d_{t=0} & Datenpunkt an der Stelle $t=0$ des Segments $S$
\end{conditions}

\noindent \Cref{eq:feature-extraktion} zeigt das Schema einer solchen Feature-Extraktion. Die bestehenden Daten können zusammengefasst werden, wie beispielsweise $|ACC(t=0)|$ die absolute Länge des Beschleunigungsvektors zusammenfasst. Dies stellt bereits ein simples Feature dar. Gleichzeitig können neue Informationen in Relation zum Segment ergänzt werden, in \Cref{eq:feature-extraktion} als $FEATURE_{n_F}(S, t=0)$ symbolisiert. Der Zweck dieser Abbildung ist die semantische Aufbereitung der Datenpunkte, mit dem Ziel einer Verbesserung der Erkennung. Welche Features hierbei konkret berechnet werden sollen, wird im Prozess des \textit{Feature-Engineering} bestimmt \cite{matusek_anwendung_2019}. Für den Kontext HAR lassen sich prinzipiell zwei Gruppen von Features unterscheiden.

\paragraph{Shallow Features \cite{ravi_deep_2017}:} Diese Features werden direkt aus dem vorliegenden Segment abgeleitet und sind typischerweise für alle Datenpunktvektoren eines Segments identisch. Häufig verwendete shallow Features sind der Durchschnitt und die Standardabweichung bestimmter Werte eines Segments \cite{chen_deep_2015, banos_window_2014, kwapisz_activity_2011, jahangiri_applying_2015, nurhanim_classification_2017}. Diese Features beziehen sich direkt auf die Zeitlinie. Sie werden daher auch als \textit{Time Domain Features} eingeordnet \cite{chen_deep_2015}. Alternativ ist es auch üblich, sogenannte \textit{Frequency Domain Features} durch eine vorige Fourier-Transformation des Segments zu berechnen \cite{zeng_convolutional_2014, chen_deep_2015}.

\paragraph{Non-Shallow Features \cite{ravi_deep_2017}:} Während shallow Features verhältnismäßig einfach zu implementieren sind, können sie wiederum möglicherweise keine komplexen Zusammenhänge innerhalb des Segments repräsentieren \cite{abu_alsheikh_deep_2015}. Bei \textit{Non-Shallow Features}, auch bezeichnet als \textit{Data-Driven Features} oder \textit{Deep Learning Features} \cite{abu_alsheikh_deep_2015}, erfolgt die Berechnung durch Zwischenschaltung eines Machine-Learning-Modells, welches bestimmte Eingangswerte des Segments in einen Feature-Vektor überführt (Enkodierung). \cite{ravi_deep_2017} überführen beispielsweise die Segmentdaten in ein Spektrogramm, um dieses über ein Machine-Learning-Modell zu enkodieren. \cite{zeng_convolutional_2014} verwenden sogar mehrere Machine-Learning-Modelle verschiedener Art. \\

\noindent Die obigen Features lassen sich hierbei auch miteinander kombinieren, um weitere Features zu erzeugen. Für die auf Machine-Learning basierte HAR nutzen \cite{kwapisz_activity_2011} beispielsweise insgesamt 43 verschiedene erzeugte Features. Auch hier ist jedoch wieder zwischen dem potenziellen Informationszugewinn und der erhöhten Dimensionalität des Feature-Vektors und der damit verbundenen Berechnungskomplexität der Aktivitätserkennung abzuwägen. Auch bei den berechneten Features ist zu beachten, dass diese je nach Architektur des Machine-Learning-Modells normalisiert oder standardisiert werden sollten (siehe \Cref{sec:datenerfassung-und-vorverarbeitung}).

\subsubsection{Einordnung in klassische Probleme des Machine Learnings}

Neben mannigfaltigen Fehlerursachen (in \Cref{sec:datenerfassung-und-vorverarbeitung} erläutert) ist auch die kontextbedingte Komplexität der ermittelten Features denkbar hoch. Smartphones können zum Beispiel in verschiedenen Positionen (Hand, Hosentasche, Rucksack, ...) am Körper bzw. am/im Verkehrsmittel getragen werden. Weitere Ursachen für zwischen Messungen gleicher Aktivität abweichenden Daten sind unterschiedlich gebaute Verkehrsmittel (zum Beispiel Rennrad vs. Mountainbike), unterschiedliche anatomische Gegebenheiten und Bewegungsmuster der jeweiligen Person, sowie die Art des Untergrundes (zum Beispiel Schotterweg vs. asphaltierte Straße) oder die allgemeine Beschaffenheit des Geländes. Daher besteht ein weiteres zentrales Problem in der Realisation der Abbildung $HAR(S): D^{n_S} \rightarrow L$. Eine einfache zustandslose Implementation dieser Abbildung wäre denkbar. Hierfür könnten beispielsweise Geschwindigkeitsbereiche auf $L$ abgebildet werden, wie $\varnothing v \geq 40\frac{km}{h} \rightarrow$ \enquote{Auto fahren} oder $\varnothing v \leq 5\frac{km}{h} \rightarrow$ \enquote{zu Fuß}. Bei der Betrachtung dieses Ansatzes lassen sich jedoch schnell Fehlerszenarien konstruieren, in welchen diese Abbildung fehlschlägt, beispielsweise eine dichte Verkehrslage mit ständigem Abbremsen und Beschleunigen, oder eine besonders schnelle Fahrradfahrt bergab. Bei der Behandlung dieser Fehlerszenarien werden bereits komplexere Überlegungen notwendig, welche möglicherweise neue Fehlerszenarien erzeugen. Je näher die zu erreichende Präzision des Systems an $100\%$ liegen soll, desto komplexer würde ein solches System vermutlich werden. Für solche Probleme, die auch durch hochkomplexe algorithmische Lösungen nicht hinreichend gelöst werden können, haben sich verschiedene Methoden aus dem Forschungsspektrum der künstlichen Intelligenz etabliert.

\begin{itemize}
\item Beim \textbf{Clustering} steht die Partitionierung einer Menge von Werten in geeignete Untergruppen im Vordergrund. Verfahren für das Clustering sind beispielsweise k-Means-Clustering \cite{likas_global_2003}, t-SNE \cite{van_der_maaten_laurens_and_hinton_geoffrey_visualizing_2008} oder PCA \cite{ringner_what_2008}.
\item Die \textbf{Regression} behandelt Probleme, deren Hauptgegenstand die Nachbildung eines Kausalzusammenhangs oder einer Korrelation in einer Menge von Werten ist. Im Zentrum steht hierbei das Ziel, die Abweichung der Regression von den Daten zu minimieren.
\item Bei der \textbf{Generation} sollen neue Daten anhand eines Vorbildes erzeugt werden. Ziel ist es, bei einer Eingabe von bestimmten Daten möglichst plausible weitere Daten zu erzeugen. Klassische Anwendungsbereiche umfassen KI-Systeme für die Chat-basierte Texterzeugung \cite{brown_language_2020} oder auch das Supersampling von Bildern \cite{burgess_rtx_2020, park_semantic_2019}.
\item Möglich ist auch die \textbf{Transformation}, wenn statt ähnlichen Daten (wie bei der Generation) andersartige Daten erzeugt werden sollen. Ein charakteristischer Anwendungsbereich ist die Realisation von \textit{Text-to-Speech}- oder \textit{Speech-to-Text}-Systemen \cite{wang_tacotron_2017}.
\item Im Rahmen der \textbf{Klassifikation} werden Eingabedaten in bestimmte Klassen eingeteilt, bei einer Maximierung der Anzahl korrekter Einordnungen.
\end{itemize}

\noindent Zusammenfassend handelt es sich hierbei um Verfahren, bei denen mathematische Modelle anhand vorliegender Daten angelernt werden (Induktion, Training), um schließlich zur Lösung des vorliegenden Problems verwendet zu werden (Deduktion, Inferenz). Daher werden diese Verfahren auch als \textit{Machine-Learning}-Methoden bezeichnet. Statt algorithmische Verfahren zu entwickeln, welche ein komplexes Problem (wie oben illustriert) möglicherweise nur begrenzt lösen können, ist also die Grundidee der Machine-Learning-Methoden die Repräsentation einer Abbildung von \textit{Input} zu \textit{Output} über ein angelerntes mathematisches Modell. \cite{matusek_anwendung_2019}, \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} konnten zeigen, dass solche Machine-Learning-Methoden zur Implementation der eingangs gezeigten Abbildung $HAR(S): D^{n_S} \rightarrow L$ geeignet sind. In den folgenden Sektionen soll daher zunächst diskutiert werden, welche Konzepte diesen Modellen zugrunde liegen.


\todo{Gradientenabstiegsverfahren behandeln}
\todo{Labels und Feature Set erläutern}
