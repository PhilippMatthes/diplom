\chapter{Konzept}\label{ch:konzept}

Ziel der Konzepts ist die Ermittlung eines Pareto-optimalen Modells bezüglich der Relation zwischen Ressourcenverbrauch und Ergebnisqualität auf Smartphones. Zunächst muss hierfür ein Datensatz akquiriert werden, welcher zum Training und zur Evaluation genutzt werden kann. Im Rahmen der Forschungsanalyse konnte kein konkreter Modellansatz isoliert werden, der übergreifend die besten Ergebnisse erzielt. Es existieren stattdessen mehrere mögliche Herangehensweisen für die Konfiguration einer Datenvorverarbeitungspipeline und des nachfolgenden Modells. Die Konfiguration ist hierbei maßgeblich von drei Dimensionen abhängig. Bereits durch die initiale Selektion einer Basisarchitektur spannt sich eine große Menge von möglichen Modellen auf. Insbesondere für traditionelle Modelle muss ein extensives Feature-Engineering durchgeführt werden. Die so entstehenden Modellvarianten wiederum können in ihren Hyperparametern und applizierbaren Optimierungsstrategien variiert werden. Damit ist es zumindest im Rahmen dieser Arbeit nicht möglich, alle Kombinationen und Variationen auszutesten. Die zu testenden Modelle sollen stattdessen auf Grundlage empirischer Entscheidungen selektiert werden. Dies beinhaltet eine schrittweise Exploration jeder der genannten Dimensionen, wobei nach jedem Schritt das vielversprechendste Modell heuristisch selektiert wird.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 645 221]{prozedur.pdf}
  \caption{Prozedurale Ermittlung eines selektiv Pareto-optimalen Modells in drei Teilschritten durch Tradeoff-Analyse.}\label{fig:prozedur}
\end{figure}

Das konzipierte Vorgehensmodell ist in \Cref{fig:prozedur} gezeigt. Mit der Akquise des Datensatzes zusammenhängend werden in Schritt 1 des Konzepts verschiedene Feature-Extraktionsverfahren erstellt, welche anschließend genutzt werden, um eine initiale Selektion einer Basisarchitektur durch Test verschiedener Kombinationen durchzuführen. In Schritt 2 wird die Basis-Architektur durch Variation verschiedener Hyperparameter bezüglich des Tradeoffs analysiert. Schritt 3 schließt sich hieran an und analysiert den Tradeoff verschiedener Optimierungsstrategien. Das resultierende Modell soll bezüglich des Tradeoffs Pareto-optimal sein. Die schrittweise Limitation des Betrachtungsspektrums ist notwendig, um die Tradeoff-Analyse in einem realisierbaren Spektrum zu halten. Sie bedingt jedoch auch, dass unter Umständen kein global Pareto-optimales Modell unter Berücksichtigung aller Möglichkeiten gefunden werden kann. Möglich wäre es beispielsweise, dass ein initial bei der Selektion der Basisarchitektur als weniger vielversprechend geltendes und somit eliminiertes Modell durch Variation der Hyperparameter oder Optimierung einen besseren Tradeoff bietet. Dies ist bei der Evaluation zu berücksichtigen. Die einzelnen Schritte des gezeigten Verfahrens werden in den nachfolgenden Sektionen näher diskutiert.

\nocite{chollet_keras_2015,omalley_keras_2019,pedregosa_scikit-learn_2011,martin_abadi_tensorflow_2015}

\section{Explorative Datensatzanalyse zur Konfiguration der Vorverarbeitung}

Nachfolgend wird ein Datensatz für das Training der Modelle in dieser Arbeit akquiriert und anschließend exploriert. Durch die Exploration der Struktur des Datensatzes sollen sich wertvolle Erkenntnisse ergeben, welche direkt in die Strukturierung der Datenvorverarbeitung und damit auch in die erreichbare Ergebnisqualität einfließen.

\subsection{Akquise eines Datensatzes}

Als Ausgangspunkt für die datenorientierte Verkehrsmittelerkennung durch Machine-Learning-Systeme muss ein Datensatz akquiriert werden. Dieser kann selbstständig erzeugt werden, beispielsweise durch Aufzeichnung über eine prototypische Applikation, oder aus verwandten Arbeiten übernommen werden. Da bereits zahlreiche öffentliche Datensätze zur Verfügung stehen und die Nutzung eines solchen Datensatzes die Vergleichbarkeit der Ergebnisse verbessert, wird ein öffentlicher Datensatz zum Training und zur Evaluation der Modelle selektiert.

Die Sussex-Huawei-Locomotion-Challenge, kurz SHL-Challenge, ist ein jährlicher Wettbewerb, bei dem Forschungsteams Verkehrsmittelklassifikationen mit Fokus auf eine bestmögliche Accuracy entwickeln. Im Rahmen des Wettbewerbs werden mehrere Datensätze zur Verfügung gestellt, mit unterschiedlichen Strukturen. Außerdem liegen die Ergebnisse der SHL-Challenge aus 2020 bereits vor und können als Vergleichspunkt zu den Ergebnissen dieser Arbeit genutzt werden \cite{wang_summary_2020}. Die bei dieser Challenge veröffentlichten Datensätze enthalten mehrere Pakete, jeweils aufgezeichnet in einer unterschiedlichen Position am Körper (\textit{Torso}, \textit{Bag}, \textit{Hips} und \textit{Hand}). Insgesamt wurden die Daten von drei Personen aufgezeichnet.

\begin{figure}[h]
\includegraphics[width=0.5\linewidth, bb=0 0 513 277]{shl/label-quantities.pdf}
\caption{Klassenverteilungen im Hand-Datensatz der SHL-Challenge aus 2020.}\label{fig:shl-label-quantities}
\end{figure}

Die Daten bestehen aus Samples von jeweils 500 Datenpunkten, welche mit einer Frequenz von 100Hz aufgezeichnet wurden. Dies ergibt eine Sample-Länge von 5 Sekunden, welche im Unterschied zu den in \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} verwendeten Fensterlängen eine verzögerungsarme Klassifikation ermöglicht (\nameref{qa:inf}). Verfügbar sind hierbei triaxiale Messwerte zu Akzelerometer, Gyrosensor und Magnetometer, sowie weitere, zum Teil bereits vorverarbeitete Messwerte wie die Orientierung als Quaternion und die lineare Beschleunigung. Die Verteilungen der einzelnen aufgezeichneten Klassen sind hierbei nicht gleich, dies zeigt \Cref{fig:shl-label-quantities} für den Hand-Datensatz. Außerdem ist eine Klasse \texttt{Null} im Datensatz enthalten, welche als Platzhalter bei Beginn, Wechsel und Ende der Aufzeichnungen genutzt und im veröffentlichten Datensatz entfernt wurde. Der SHL-Datensatz stellt außerdem keine GNSS-Daten zur Verfügung, dies ist jedoch in Einklang mit dem Konzept. Eine GNSS-Dimension wird bewusst nicht verwendet, um den Energieverbrauch der Datenaufzeichnung zu minimieren und mögliche Fehlerquellen mit der dazugehörigen energieintensiven Vorverarbeitung zu eliminieren (\nameref{qa:r}). Die empirische Annahme ist hierbei, dass die lokalen kinematischen Messparameter für eine hinreichende Klassifikationsqualität genügen. Hieraus resultiert gleichzeitig der Vorteil, dass kein Downsampling der lokalen kinematischen Messdaten wie in \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} durchgeführt werden muss, verbunden mit einem potenziell höheren Informationsgehalt des Ausgangsdatensatzes durch Erhalt der Struktur der Zeitlinien.

\subsection{Speicheroptimiertes Laden von Samples}

Der SHL-Datensatz wird unterteilt in mehrere Pakete, welche jeweils alle vier aufgezeichneten Positionen am Körper beinhalten. Die Datensätze werden geladen und in einen Trainings- und Validierungsteil aufgespalten. Der Validierungsteil verbleibt in Isolation zum Modell und wird zur Ermittlung der Generalisierungsfähigkeit verwendet. Die Größe des Trainingsdatensatzes (insgesamt ca. 100GB) stellt ein konzeptuelles Problem dar. Während traditionelle Modelle auf einem kleineren Teil des Trainingsdatensatzes trainiert werden können, profitieren Deep-Learning-Modelle von der Bereitstellung des gesamten Datensatzes. Das vollumfängliche Laden der Datensätze kann jedoch die Limitationen des Trainingscomputers übersteigen und ist mit einer langen Wartezeit vor Start des Trainings verbunden. Hierfür wird ein Lösungskonzept vorgestellt.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 455 138]{streaming.pdf}
  \caption{Verfahren zum verzögerungsarmen und speicheroptimierten Laden von Samples für das Training und die Validierung von Deep-Learning-Modellen.}\label{fig:streaming}
\end{figure}

Der konzeptionelle Prozess ist in \Cref{fig:streaming} gezeigt. Die Trainings- und Testdaten werden speicheroptimiert geladen. Hierzu werden die Teildatensätze zunächst segmentiert und anschließend in Teilen geladen. Damit im finalen Datensatz nicht zunächst alle Samples einer einzigen jeweiligen Körperposition auftreten, werden die Segmente über die Datensätze verteilt der Reihe nach geladen (ähnlich zum \textit{Round-Robin}-Verfahren). Aus den Segmenten werden kleine Pakete von Samples (Batches) generiert, welche anschließend vorverarbeitet (siehe nachfolgende Sektionen) und insbesondere auch vermischt werden können, um eine möglichst alternierende Verteilung der Positionen am Körper zu realisieren und das Overfitting des Modells auf einer Position zu reduzieren. Die resultierenden Batches können anschließend zum Training und zum Test eines Deep-Learning-Modells genutzt werden, wobei durch partielles Laden der Datensätze der Speicher des Trainingscomputers nicht überbeansprucht wird und ein schneller Beginn des Trainings möglich ist.

\subsection{Feature-Engineering}

Ziel des Feature-Engineerings ist es, Inputs für die verschiedenen möglichen Modellarchitekturen bereitzustellen. Daher werden mehrere Varianten der Bereitstellung von Features betrachtet, um anschließend jeweils die bestmögliche Feature-Pipeline für jedes individuelle Modell zur sich anschließenden initialen Selektion einer Basisarchitektur bereitzustellen. Dies inkludiert die Berechnung von transformierten Zeitliniendaten, Shallow Features und Non-Shallow Features.

\subsubsection{Vorverarbeitung von Zeitliniendaten}

Die Zeitliniendaten des SHL-Datensatzes können direkt geladen und anschließend für Training und Test genutzt werden. Unter anderem sind hierbei jedoch viele zusätzliche Zeitlinien-Informationen enthalten, welche bezüglich des potenziellen Informationszugewinns zu prüfen sind. Daher werden zwei mögliche Bildungen von direkten Zeitlinien-Features untersucht. Zum einen die Verwendung aller bereitgestellten Informationen mit der damit verbundenen relativen Erhöhung der Operationen zur Berechnung (energieintensiver) und zum anderen die Verwendung einer Selektion aus den möglichen Informationen.

Als Grundlage der selektiven Berechnung sollen die im Datensatz enthaltenen Akzelerometer-, Magnetometer- und Gyrosensordaten verwendet werden. Durch die Berechnung des punktweisen Vektorbetrags über diese Zeitlinien lässt sich die Dimensionalität des Inputs für das Machine-Learning-Modell zusammen mit den notwendigen Operationen reduzieren. Die Gleichungen sind $\left| ACC \right| = \sqrt{ACC_x^2 + ACC_y^2 + ACC_z^2}$, $\left| MAG \right| = \sqrt{MAG_x^2 + MAG_y^2 + MAG_z^2}$ und $\left| GYR \right| = \sqrt{GYR_x^2 + GYR_y^2 + GYR_z^2}$. Die statistischen Auswirkungen dieser Berechnung sind in den nachfolgenden Grafiken dargestellt.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1396 386]{shl/akzelerometer-boxplot.pdf}
\caption{Statistische Verteilung der Sensorkomponenten im Akzelerometer-Signal anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-akzelerometer-boxplot}
\end{figure}

In \Cref{fig:shl-akzelerometer-boxplot} wird die in den Grundlagen erläuterte Beobachtung deutlich, dass der Gravitationsvektor in den Teilachsen einen einheitlichen Verschiebungsfaktor darstellt. Durch Berechnung des Vektorbetrags zentriert sich die Verteilung deutlich vertikal um die Erdbeschleunigung, welche bei ungefähr $9,81 \frac{m}{s^2}$ liegt. Durch Subtraktion dieser Beschleunigung, beispielsweise durch Standardisierung, ließe sich der Gravitationsvektor zumindest aus dem Vektorbetrag effizient und verlustfrei entfernen, im Unterschied zum schätzungsbedingt fehlerbehafteten und berechnungsintensiveren AHRS-Verfahren. Es liegt nahe, dass Informationen aus dem SHL-Datensatz wie der aktuelle Gravitationsvektor ($GRA_{x,y,z}$), die lineare Beschleunigung ($LACC_{x,y,z}$) und die Ausrichtung des Geräts in Quaternionen ($ORI_{w,x,y,z}$) durch die Elimination des Gravitationsvektors für die Klassifikation an Bedeutung verlieren, da das Modell den gravitativen Faktor gegebenenfalls nicht selbstständig eliminieren muss. Umgekehrt lässt sich vermuten, dass ein geeignetes Machine-Learning-Modell quantitative Zusammenhänge zwischen $GRA_{x,y,z}$, $ORI_{w, x,y,z}$, $LACC_{x,y,z}$ und $MAG_{x,y,z}$, $ACC_{x,y,z}$, $GYR_{x,y,z}$ erlernen und kombinieren kann. Das Risiko des Overfittings ist bei Letzterem jedoch vermutlich höher, die Transformation in $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ kann als Regularisierung auf den Inputs interpretiert werden.

Zurück auf die Daten aus \Cref{fig:shl-akzelerometer-boxplot} schauend wird deutlich, dass die größten Auslenkungen der Amplitude erwartungsgemäß beim Rennen zu messen sind. Diese Aktivität lässt sich vermutlich leicht von den Verkehrsmittel Bus oder Zug unterscheiden, allein durch Betrachtung der Akzelerometerdaten.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1403 386]{shl/magnetometer-boxplot.pdf}
\caption{Statistische Verteilung der Sensorkomponenten im Magnetometer-Signal anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-magnetometer-boxplot}
\end{figure}

Ähnliche Beobachtungen lassen sich ergänzend anhand von \Cref{fig:shl-magnetometer-boxplot} feststellen. Der Vektorbetrag des Magnetometer-Signals zentriert sich um die erwartbare Magnetfeldstärke der Erde und schwankt in bestimmten Verkehrsmitteln durch Anwesenheit von artifiziellen Magnetfeldquellen nach oben und unten. Deutlich wird die starke Variabilität des Magnetfeldes in den einzelnen Teilachsen, bedingt durch die Ausrichtung des Geräts. Auch hier kann durch den Vektorbetrag die Ausrichtung des Geräts als transformativer Faktor eliminiert werden.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1399 386]{shl/gyrosensor-boxplot.pdf}
\caption{Statistische Verteilung der Sensorkomponenten im Gyrosensor-Signal anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-gyrosensor-boxplot}
\end{figure}

\Cref{fig:shl-gyrosensor-boxplot} zeigt die Anwendung des Vektorbetrags auf die Winkelauslenkung in $\frac{rad}{s}$. Die größten Auslenkungen sind erwartungsgemäß beim Rennen messbar. Auch die Klasse Fahrradfahren zeigt eine erhöhte Winkelauslenkung. Eine vertikale Verschiebung der Messdaten, wie sie beim Akzelerometer und beim Magnetometer beobachtet werden kann, ist hierbei nicht beobachtbar. Stattdessen sind die Daten durch die Messung der zeitlichen Änderung des Signals um den Nullpunkt zentriert, da auf jede ins System eingebrachte Rotation auch zwangsläufig eine entgegengesetzte Rotation folgt. $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ betrachtend lässt sich abschließend also anhand der explorativen Datenanalyse feststellen, dass trotz Reduktion der Signale $MAG_{x,y,z}$, $ACC_{x,y,z}$ und $GYR_{x,y,z}$ von insgesamt 9 Dimensionen auf 3 Dimensionen eine für die Klassifikation wichtige Variabilität zwischen den Klassen weiterhin klar erkennbar bleibt, wenn auch abhängig von der Unterschiedlichkeit der inhärenten Bewegungsmodalitäten. Ob die Bildung der Vektorbeträge jedoch wirklich vielversprechender als die energieintensivere Auswahl aller Zeitlinien ist, wird erst zu einem späteren Punkt in der Arbeit in Verbindung mit einem dazugehörigen Machine-Learning-Modell getestet.

\subsubsection{Selektion einer Skalierungsmethode}\label{sec:skalierungsmethode}

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1057 277]{shl/sensor-histograms.pdf}
\caption{Statistische Verteilung der Signalstärken von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ im SHL-Hand-Datensatz.}\label{fig:shl-sensor-histograms}
\end{figure}

Die Auswahl der Skalierungsmethode für die Eingaben ist für einige Modelle wichtig, um eine bestmögliche Ergebnisqualität zu erzielen. Hierzu wird die statistische Verteilung der Eingaben näher betrachtet. In \Cref{fig:shl-sensor-histograms} sind die Verteilungen von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ für den SHL-Hand-Datensatz illustriert, die Prinzipien lassen sich jedoch auch auf alle partiellen Zeitlinien $GRA_{x,y,z}$, $ORI_{w, x,y,z}$, $LACC_{x,y,z}$, $MAG_{x,y,z}$, $ACC_{x,y,z}$ und $GYR_{x,y,z}$ ableiten.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1057 277]{shl/scaler-histograms.pdf}
\caption{Anwendung der Standardisierung, Yeo-Johnson-Transformation und Box-Cox-Transformation auf $\left| ACC \right|$, Markierung bei $-1$ und $1$.}\label{fig:shl-scaler-histograms}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1058 277]{shl/yeo-johnson-histograms.pdf}
  \caption{Anwendung der Yeo-Johnson-Transformation auf $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$, Markierung bei $-1$ und $1$.}\label{fig:shl-yeo-johnson-histograms}
\end{figure}

\Cref{fig:shl-scaler-histograms} zeigt zunächst die bereits erläuterte Standardisierung. Die Standardisierung verändert die Verteilung der Daten nicht morphologisch, stattdessen werden die Daten mit Erhaltung von Ausreißern um den horizontalen Nullpunkt skaliert. In manchen Fällen ist es zur Vorverarbeitung sinnvoll, die Daten zusätzlich morphologisch um den horizontalen Nullpunkt symmetrisch anzugleichen, indem spezielle Power-Transformationen angewandt werden. \cite{werner_kontinuierliche_2020} nutzt die Yeo-Johnson-Transformation, welche in \Cref{fig:shl-scaler-histograms} und \Cref{fig:shl-yeo-johnson-histograms} gezeigt ist. Die Yeo-Johnson-Transformation ist auf einem Datenpunkt $x$ wie folgt definiert.

\begin{equation}
\begin{aligned}
  yeojohnson(x;\lambda) &=
  \boldsymbol 1 _{(\lambda \neq 0, x \geq 0)} \frac{(x+1)^\lambda-1}{\lambda} \\
  &+ \boldsymbol 1_{(\lambda = 0, x \geq 0)} \log (x+1) \\
  &+ \boldsymbol 1_{(\lambda \neq 2, x < 0)} \frac{(1-x)^{2-\lambda}-1}{\lambda - 2} \\
  &+ \boldsymbol 1_{(\lambda = 2, x < 0)} -\log (1-x) \\
\end{aligned}
\end{equation}

Der Parameter $\lambda$ wird durch die Maximum-Likelihood-Methode\footnote{\url{https://de.wikipedia.org/wiki/Maximum-Likelihood-Methode} (Abgerufen am 5.8.2021)} geschätzt. Ziel der Anwendung einer solchen Power-Transformation ist, die Abschrägung (\textit{skew}) der Daten zu eliminieren, um den Bias des Modells zu verringern und den Lernerfolg zu steigern. Die Yeo-Johnson-Transformation ist eine Erweiterung der in \Cref{fig:shl-scaler-histograms} gezeigten Box-Cox-Transformation und dient als grundlegende Skalierungsmethode für die Machine-Learning-Modelle in dieser Arbeit.

\subsubsection{Bildung von Shallow Features}

Würden nur die skalierten Zeitliniendaten anhand der bisherigen Beschreibungen bereitgestellt, so könnten traditionelle Machine-Learning-Ansätze nicht effektiv in die Erstauswahl einer Basisarchitektur mit einbezogen werden, da sie analog zur Forschungsanalyse auf reinen Zeitliniendaten nicht hinreichend generalisieren können. Es sollen zusätzlich noch statistische und frequenzbezogene Features ermittelt werden, welche insbesondere für diese Modelle herangezogen werden können. Gleichzeitig muss die Anzahl und Berechnungskomplexität von Features so gering wie möglich gehalten werden, um die Energieintensität zu reduzieren (\nameref{qa:r}). Die Auswahl von Features kann durch automatisierte Analyse der statistischen Signifikanz durchgeführt werden, hierfür ist das in \cite{christ_time_2018} beschriebene Framework eine attraktive Lösung. Für die Entscheidung, ob traditionelle Modelle generell konzeptuell weiter verfolgt werden sollen, genügt jedoch zunächst eine Auswahl typischer Features, wie sie in verwandten Arbeiten beschrieben werden. Aussagekräftige und effizient berechenbare statistische Features sind beispielsweise das arithmetische Mittel, die Standardverteilung, sowie das Maximum und Minimum des jeweiligen Zeitliniensignals. Hinzu kommen frequenzbezogene Features. Das Frequenzspektrum kann über eine Fast-Fourier-Transformation ermittelt werden, mit der gegebenen Abtastrate von 100Hz ist so eine direkte Zuordnung der Signalstärken zur Frequenz möglich.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1039 443]{shl/sensor-fft.pdf}
\caption{Statistische Verteilung der Frequenzen von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-sensor-fft}
\end{figure}

\Cref{fig:shl-sensor-fft} zeigt einen Ausschnitt der hieraus resultierenden Verteilung der Signalstärken für $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$. Sichtbar wird hierbei, dass die verschiedenen Verkehrsmittelklassen unterschiedlich zu den einzelnen Frequenzbereichen beitragen. Dies lässt vermuten, dass die Transformation der Zeitliniendaten in die Frequenzdomäne weitere Unterscheidungsmerkmale bereitstellen kann. Aus dem Frequenzspektrum lassen sich somit weitere Features ableiten, hierunter das Maximum und Minimum der Signalstärke, die Entropie des Signals, sowie die Durchschnittsfrequenz.

\subsubsection{Bildung von Non-Shallow Features}

Neben Shallow Features und den vorverarbeiteten Zeitliniendaten sollen auch Non-Shallow Features in das Konzept einfließen. Für die Realisation wird ein Autoencoder erstellt. Hierbei handelt es sich um ein künstliches neuronales Netzwerk, welches eine effiziente Kodierung der Zeitliniendaten erlernt. Das Netzwerk besteht aus einem enkodierenden Teil (Encoder), welcher die Eingangsdaten auf eine kompakte interne Repräsentation (Coding) abbildet, sowie einem dekodierenden Teil (Decoder), welcher die kompakte Repräsentation in die ursprünglichen Zeitlinien zurückverfolgen soll. Der Decoder wird hierbei nur zum Training des Modells benötigt. Nach dem Training wird der Encoder vom Decoder isoliert. Die kompakte Repräsentation des Encoders auf den Eingabedaten dient als Ausgabe von Non-Shallow Features.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 704 235]{shl/autoencoder.pdf}
  \caption{Resultate eines 1D Deep Convolutional Autoencoders auf Akzelerometerdaten des SHL-Testdatensatzes.}\label{fig:autoencoder}
\end{figure}

In \Cref{fig:autoencoder} sind die Resultate eines aus dem Prototyping entstandenen Autoencoders gezeigt. Der Autoencoder besteht aus mehreren, konzentrisch kleiner werdenden Schichten von eindimensionalen Convolution-Schichten mit zwischengeschalteten Dropout-Schichten zur Regularisierung. Diese Architektur eignet sich für die komplexe Repräsentation von Zeitlinien wesentlich besser als LSTM-Schichten oder Dense-Schichten \cite{martinez_autoencoders_2019}. Auch durch wiederholte Variation der Hyperparameter des Autoencoders konnte jedoch keine hinreichende Generalisierungsfähigkeit erreicht werden. Diese Ergebnisse werden in \Cref{fig:autoencoder} deutlich. Die Kurvenverläufe können teils rekonstruiert werden, bei einigen Samples jedoch kann keine hinreichende interne Repräsentation gefunden werden und die Rekonstruktion fällt zurück auf ein wiederkehrendes Muster (zweite Spalte von rechts). Zu beobachten ist auch, dass der Autoencoder stets zur Minimierung der Kosten dazu bestrebt ist, die Ausgangszeitlinie ähnlich wie beim Moving-Average-Verfahren direkt abzubilden, statt eine abstraktere Repräsentation zu etablieren. Wird die Dimensionalität des Encoders zu groß gewählt, so konnte beobachtet werden, dass das Modell lediglich die Eingabedaten in transformierter Form im Encoding abspeichert. Dies würde jedoch vermutlich keinen Mehrwert für ein nachgeschaltetes Klassifikationsmodell gegenüber den reinen Eingangsdaten bieten.

Non-Shallow Features im Sinne der rekonstruierbaren Repräsentation einer Zeitlinie werden vor dem Hintergrund dieser Ergebnisse nicht weiter betrachtet. Das Potenzial dieser Methode ist jedoch durch die prototypische Vorbetrachtung nicht ausgeschöpft und kann in einer späteren Arbeit nochmals betrachtet werden. Als Ausgangspunkt hierfür könnten beispielsweise Regularisierungen gesucht werden, welche das Bestreben des Autoencoders zur direkten Abspeicherung der Zeitlinie im Encoding mitigieren. Dass Non-Shallow Features in diesem Rahmen als nicht vielversprechend bewertet wurden, bedeutet jedoch nicht, dass Deep-Learning-Klassifikationsmodelle keine hinreichende interne Repräsentation der Klassen erlernen können, denn hierbei wird im Unterschied zum Autoencoder nicht die Identitätsfunktion auf den Trainingsdaten erlernt.

\subsubsection{Gewichtung der Klassen}

In \Cref{sec:skalierungsmethode} wurden spezielle Skalierungsmethoden selektiert, um die Verzerrung der Wertebereichsverteilung möglichst zu reduzieren. Ein weiteres Problem stellen die ungleich verteilten Klassen im Datensatz dar (siehe \Cref{fig:shl-label-quantities}). Trainiert das Modell auf überdurchschnittlich vielen Samples zu einer Klasse, so lernt das Modell, diese Klasse zur Reduktion der Kostenfunktion häufiger vorherzusagen. Dies ist mit einem weiteren Problem verbunden. Angenommen, der Trainings-, Test- und Validierungsdatensatz werden über demselben ungleich gewichteten Datensatz gebildet. So wird ein eventueller Bias nicht durch eine Analyse der Metriken auf dem Testdatensatz sichtbar. Es ist also zu erwarten, dass das Modell in diesem Fall eine künstlich erhöhte Accuracy verzeichnet und auf neuen Datensätzen signifikant schlechter generalisiert. Auch aus Gründen der Vergleichbarkeit der Metriken ist es somit unbedingt notwendig, dass die Klassen zum Training des Modells gleich gewichtet werden.

Bei traditionellen Modellen bietet es sich an, die Gleichgewichtung durch Transformation des Datensatzes selbst zu realisieren. Die unterrepräsentierten Klassen können durch das Verfahren \textit{Synthetic Minority Over-sampling Technique} (SMOTE) auf Grundlage eines randomisierten Auswahlverfahrens repliziert werden \cite{chawla_smote_2002}. Dies ist jedoch mit dem Problem verbunden, dass auf diese Weise synthetische Duplikate im Datensatz angefertigt werden und das Modell somit keine neuen Informationen erhält. Stattdessen trainiert das Modell gegebenenfalls mehrere Male auf demselben Sample, bei zu starkem Oversampling neigt das Modell anschließend zum Overfitting. Um den Anteil der synthetischen Samples zu reduzieren, werden daher Samples der überrepräsentierten Klassen randomisiert eliminiert (\textit{Undersampling}). Das traditionelle Modell trainiert anschließend auf einem balancierten Datensatz.

Für Modelle aus dem Deep Learning lässt sich eine weitere Technik anwenden, um eine relative Gleichgewichtung des Datensatzes beim Lernen zu erzielen. Statt den Datensatz selbst anzugleichen, wird die Verlustfunktion relativ zur betrachteten Klasse gewichtet. Im übertragenen Sinn widmet sich das Modell den überrepräsentierten Klassen anschließend weniger als den unterrepräsentierten Klassen \cite{cui_class-balanced_2019}.

\subsection{Initiale Datenvorverarbeitungspipeline}

Die Vorverarbeitung muss so sparsam wie möglich gehalten werden (\nameref{qa:r}). Die Implementation eines Schätzverfahrens wie AHRS zur Transformation der lokalen kinematischen Messdaten in ein Referenzkoordinatensystem wird durch Bildung des Vektorbetrags und damit einer sparsameren Alternative ersetzt. Eine verbleibende vertikale Verschiebung wie die der Akzelerometerdaten wird durch die Yeo-Johnson-Transformation eliminiert, die Schrägheit der Verteilung in den resultierenden Vektorbetragsdaten in eine annähernd symmetrisch verteilte Repräsentation überführt.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 735 173]{vorverarbeitung.pdf}
  \caption{Initiale Vorverarbeitungspipeline.}\label{fig:vorverarbeitungspipeline}
\end{figure}

\Cref{fig:vorverarbeitungspipeline} zeigt diese Schritte. Im Anschluss hieran werden die Daten entweder für das Training von traditionellen Modellen in Shallow Features überführt und durch SMOTE gewichtet, oder für das Training von Deep-Learning-Modellen als direkte Zeitliniendaten bereitgestellt, zusammen mit einer Auflistung der Klassenverteilungen zur Gewichtung der Kostenfunktion beim Training.

\section{Empirische Selektion einer Basisarchitektur}

Zur Selektion der Basisarchitektur wird auf die zwei Feature-Schnittstellen der Vorverarbeitungspipeline zurückgegriffen. Es werden typische Metaarchitekturen und Modellkonfigurationen aus verwandten Arbeiten extrahiert und auf dem SHL-Datensatz prototypisch trainiert. Die Selektion erfolgt argumentativ auf Grundlage der so ermittelten Parameter, der Ergebnisse verwandter Arbeiten und unter Berücksichtigung des Potenzials für die nachfolgenden Selektionsschritte.

\subsection{Test von traditionellen Modellen}

Die Forschunganalyse hat ergeben, dass traditionelle Modelle bei der Verkehrsmittelerkennung gegenüber Deep-Learning-Ansätzen nicht zu vernachlässigen sind. Gleichzeitig gelten Deep-Learning-Ansätze speziell bei der Verfügbarkeit von großen Datensätzen wie dem SHL-Datensatz als die vielversprechenderen Ansätze. Die Ergebnisqualität der traditionellen Modelle ist außerdem maßgeblich von der Auswahl und Anzahl der berechneten Shallow Features abhängig, wobei deren Berechnung zur Energieintensität beiträgt. Mit Betrachtung der von Forschungsteams erzielten Ergebnisse bei der SHL Challenge aus 2020 zeigt sich, dass der beste Deep-Learning-Ansatz auf Zeitliniendaten mit einem F1-Score von $88,5\%$ dem besten traditionellen Ansatz ($77,9\%$) mit Abstand überlegen ist \cite{wang_summary_2020}.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1063 262]{shl/traditional-models.pdf}
  \caption{Erreichte Accuracy-Werte und F1-Scores traditioneller Machine-Learning-Modelle auf dem SHL-Datensatz mit 24 Features pro Sample.}\label{fig:shl-traditional-models}
\end{figure}

Die vielversprechendste Architektur lässt sich anhand dessen jedoch nicht zwangsweise vorhersagen, hierfür ist die Abhängigkeit des Modells von der Datenvorverarbeitung zu stark. Daher werden mehrere traditionelle Modelle auf den ermittelten Shallow Features des Datensatzes trainiert. Auf den Shallow Features konnten traditionelle Verfahren einen F1-Score auf dem Testdatensatz von maximal $70,42\%$ sowie eine Accuracy von $70,32\%$ ohne Postprocessing erreichen. Im Rahmen der SHL-Challenge würde sich diese Ergebnisqualität auf Platz 5 von 16 einordnen \cite[S. 3]{wang_summary_2020}. Das MLP ergibt sich aus der Analyse als das am besten klassifizierende traditionelle Modell, während die Speichergröße des MLP wesentlich unter dem betrachteten KNN- und SVM-Modell liegt.

\subsection{Test von Deep-Learning-Modellen}

\paragraph{Problematik händisch erstellter Features:} Die Ermittlung von Shallow Features ist mit mehreren Problemen verbunden. Da die Features über jeden Datenpunkt für jedes Segment berechnet werden müssen, ist die Energieintensität der Vorverarbeitung gleichzeitig von der Art und Menge der Features abhängig. Mit Hinblick auf den Ressourcenverbrauch wurde zum Test der traditionellen Modelle daher die Anzahl der Shallow Features auf 24 aussagekräftige Werte beschränkt (\nameref{qa:r}). Auch hinsichtlich der Portierbarkeit (\nameref{qa:p}) stellt eine größere Anzahl von Features ein Problem dar. Während zur Selektion der Features bereits automatisierte Frameworks wie \cite{christ_time_2018} verwendet werden können, ist deren Portierung mit einem vergleichsweise hohen Aufwand verbunden, da bisher keine Möglichkeit existiert, die Feature-Pipelines direkt auf Smartphones zu portieren, ohne die Feature-Berechnungen nachzuimplementieren. Für einfache statistische Features ist dies denkbar, in komplexere Features müsste hingegen großer Zeitaufwand durch Einarbeitung in die mathematischen Grundlagen, Implementation und Unit-Testing fließen.

Vor diesem Hintergrund scheint es attraktiv, statt Shallow Features händisch zu erstellen, die Zeitliniendaten direkt durch Deep-Learning-Modelle klassifizieren zu lassen. Dessen Plausibilität wurde durch verwandte Arbeiten gezeigt, und der Aufwand der Portierung würde um ein Vielfaches reduziert. Um dies näher zu betrachten, werden verschiedene Deep-Learning-Modelle, wieder prototypisch, jedoch nun direkt auf den Zeitliniendaten getestet. Die Auswahl kann nicht alle möglichen Architekturen einbeziehen, soll jedoch auch nicht ausschließlich arbiträr erfolgen, sondern bestimmten Heuristiken folgen. Hierzu gehört, dass die initiale Auswahl der Architekturen nach dem Schema einer Breitensuche durchgeführt werden soll, um sich noch nicht zu stark auf einen konkreten Ansatz zu konzentrieren. Dies kann realisiert werden, indem eine breite Auswahl von Grundarchitekturen abgetastet wird. Hierzu werden typische Architekturen aus verwandten Arbeiten wiederverwendet. Unter Umständen kann es auch förderlich sein, neuartige Architekturen in die Breitensuche mit einzubeziehen. Insgesamt wurden vor diesem Hintergrund die folgenden fünf Architekturtypen durch Integration in die zuvor erläuterte Vorverarbeitungspipeline getestet:

\begin{itemize}
\item \textbf{Einfaches CNN:} Ein CNN mit ReLU-Aktivierungsfunktion und Regularisierung durch Batch Normalization nach \cite{wang_time_2016}
\item \textbf{ResNet:} Ein CNN mit ReLU-Aktivierungsfunktion und Regularisierung durch Batch Normalization, welches durch seine besondere Tiefe und durch Shortcuts zwischen tiefen Schichten gekennzeichnet ist, nach \cite{he_deep_2015, ismail_fawaz_deep_2019}
\item \textbf{CNN-LSTM:} Ein LSTM, welches einem einschichtigen CNN mit ReLU-Aktivierungsfunktion zur Kodierung der Eingabewerte nachgeschaltet ist
\item \textbf{Einfaches LSTM:} Ein LSTM mit Dropout-Regularisierung, welches die Zeitliniendaten als direkte Inputs erhält
\item \textbf{Transformer:} Ein \textit{attention}\footnote{\url{https://en.wikipedia.org/wiki/Attention_(machine_learning)} (Abgerufen am 16.8.2021)}-basiertes Transformer-Netzwerk nach \cite{vaswani_attention_2017, theodoros_ntakouris_keras_2021}
\end{itemize}

\paragraph{Hyperparameter des prototypischen Trainingsprozesses:} Beim Training der Deep-Learning-Modelle werden folgende Regeln angewandt. Die Modelle werden in die zuvor erläuterte Vorverarbeitungspipeline eingebunden und auf Batches mit einer Größe von 32 Samples (\textit{Batch-Size}) durch Supervised Learning trainiert, die Samples werden einem Shuffling unterzogen. Die Batch-Size von 32 hat sich für die gewählten Modelle auf dem Testsystem als performantere Konfiguration gegenüber größeren Batch-Sizes von 64 oder 128 erwiesen. Die Validierung des Modells geschieht auf einem separierten Datensatz. Als Variante des Gradientenabstiegsverfahren wird der Adam-Optimizer genutzt. Als Alternativen stehen unter anderem der SGD-Optimizer und der RMSprop-Optimizer zur Verfügung. Der Adam-Optimizer zeigte im prototypischen Trainingsprozess gegenüber den anderen Optimierungsverfahren jedoch eine schnellere Konvergenz. Als zugrundeliegende Verlustfunktion wird die \textit{Sparse Categorical Cross Entropy} verwendet, ein Derivat der Kreuzentropie, welche die Notwendigkeit eines One-Hot-Encodings der Labels erübrigt. Da die Labels des SHL-Datensatzes in Form von Integer-Encodings vorliegen, muss somit keine weitere Vorverarbeitung auf den Labels erfolgen. Auf Grundlage der Validierungsmetriken wird Early Stopping verwendet, sowie eine automatisierte Reduktion der Learning Rate bei einer stagnierenden Verbesserung der Accuracy auf den Validierungsdaten.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 856 207]{shl/deep-learning-models.pdf}
  \caption[Erreichte Accuracy-Werte von Deep-Learning-Modellen auf Zeitliniendaten des SHL-Datensatzes.]{Erreichte Accuracy-Werte von Deep-Learning-Modellen auf Zeitliniendaten des SHL-Datensatzes. Legende: Accuracy auf Trainingsdaten (obere Linie), Accuracy auf Testdaten (untere Linie), bestes Ergebnis traditioneller Modelle auf händischen Features (horizontale Gerade), Early Stopping (vertikale Gerade).}\label{fig:shl-deep-learning-models}
\end{figure}

Die besten Ergebnisqualitäten wurden hiermit durch das ResNet-Modell und das LSTM-Modell erreicht. Beide Modelle erreichen auf Zeitliniendaten bessere Ergebnisqualitäten als das MLP auf Shallow Features. Die Trainingsverläufe sind in \Cref{fig:shl-deep-learning-models} dargestellt. Aus den Verläufen lassen sich mehrere wertvolle Informationen ableiten. Das bezüglich der Ergebnisqualität beste Modell ist das ResNet (CNN). Hierbei ist es unerheblich, ob auf Vektorbeträgen oder auf allen Zeitlinien trainiert wird. Während das ResNet auf den Vektorbetragsdaten wesentlich kürzer zum Training für eine Epoche als das LSTM benötigt, trainiert es auch insgesamt für weniger Epochen bis zum Erreichen der bestmöglichen Ergebnisqualität in dieser Konfiguration. Die erreichte Accuracy von $78,94\%$ ist wesentlich besser als die des besten traditionellen Modells. Dennoch zeigt sich ein Overfitting auf den Trainingsdaten, auf welchen zeitweise mehr als $93,5\%$ Accuracy erreicht werden konnte.

\subsection{Selektion der ResNet-Architektur}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 514 241]{resnet.pdf}
  \caption[Initiale Konfiguration eines residualen Netzwerks (ResNet) als Basisarchitektur.]{Initiale Konfiguration eines residualen Netzwerks (ResNet) als Basisarchitektur. Input ist ein Sample mit 500 Datenpunkten und 3 Sensordimensionen $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$. Das CNN besteht aus 3 Blöcken mit je 3 Schichten aus Convolution, Batch Normalization und ReLU-Aktivierungsfunktion. Die Größe des Convolution-Kernels ist in jedem Block die Abfolge 8, 5, 3. Zwischen den Blöcken sind Shortcuts integriert, sogenannte \textit{Residual Connections}, welche sich an der biologischen Komposition pyramidaler Neuronen orientieren. Zur Klassifikation wird nach dem letzten Block ein \textit{Global Average Pooling} durchgeführt und mithilfe einer Dense-Schicht und der Softmax-Aktivierungsfunktion in die Verkehrsmittelklassen überführt.}\label{fig:resnet}
\end{figure}

Die Accuracy der in \Cref{fig:resnet} gezeigten ResNet-Architektur auf den Vektorbeträgen der Zeitliniendaten von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ ist im prototypischen Vergleich mit den besten traditionellen Modellen und anderen Deep-Learning-Modellen als signifikant höher festzustellen. Es wurde gezeigt, dass eine Klassifikation der Zeitliniendaten gegenüber einer problematischen Erstellung von Shallow Features hinreichende Ergebnisse verspricht. Obwohl LSTM-Netzwerke für die Verarbeitung von entlang einer Zeitlinie verteilten Daten konzeptioniert wurden, konnte das ResNet-Modell die Ergebnisse der getesteten LSTM-Netzwerke übertreffen. Darüber hinaus trainiert es anhand der Tests auch wesentlich schneller und benötigt weniger Epochen für das Erreichen der bestmöglichen Ergebnisqualität. Dies ist mit Hinblick auf Schritt 2 des Konzepts (\Cref{fig:prozedur}), welcher eine Rastersuche inkludiert, förderlich, da auf diese Weise in derselben Zeit mehr Modellvariationen evaluiert werden können. Insbesondere ist hierbei eine stärkere Regularisierung des ResNet attraktiv, welche das beobachtete Overfitting reduzieren und gleichzeitig die bisher noch nicht betrachteten Ressourcenparameter verbessern kann. Auch mit Hinblick auf Schritt 3 des Konzepts bieten sich generell bei CNN-Architekturen attraktive Möglichkeiten zur Optimierung, wie die Einführung von separierbaren Convolution-Schichten, welche sich bei Verwendung von LSTM-Netzwerken nicht ergeben. Daher wird die ResNet-Architektur auf empirischer Grundlage als Basisarchitektur für die weiteren Betrachtungen selektiert.

\section{Bilaterale Rastersuche}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 482 155]{rastersuche.pdf}
  \caption{Bilaterale Rastersuche unter Einbeziehung von Ressourcenparametern neben der Accuracy.}\label{fig:rastersuche}
\end{figure}

Die ResNet-Architektur hat sich aus den vorigen Sektionen als Basisarchitektur mit den für diese Arbeit günstigsten Eigenschaften ergeben und dient als Grundlage für die bilaterale Tradeoff-Analyse zwischen Ressourcenverbrauch und Ergebnisqualität. Der konzipierte Ablauf des Verfahrens ist in \Cref{fig:rastersuche} gezeigt. Zunächst wird ein Hyperparameter-Metamodell über der Basisarchitektur gebildet. Anschließend wird dieses genutzt, um Variationen der Basisarchitektur bezüglich ihrer Accuracy zu testen und die besten $N$ Modelle auszuwählen. Diese werden auf ein Smartphone portiert und einem Profiling unterzogen. Die ermittelten Ressourcenparameter werden zur statistischen Modellierung eines Pareto-Optimums genutzt, wodurch ein bilateral (bezüglich Ergebnisqualität und Ressourcenverbrauch) Pareto-optimales Modell resultieren soll. Die konzeptuellen Einzelheiten sind in den nachfolgenden Sektionen aufgeschlüsselt.

\subsection{Definition eines Hyperparameter-Metamodells}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 475 145]{hypermodell.pdf}
  \caption{Hyperparameter-Metamodell der ResNet-Basisarchitektur.}\label{fig:hypermodell}
\end{figure}

Ziel der Definition eines Hyperparameter-Metamodells ist die parametrisierte Variation der initialen Basisarchitektur. Die Parameter sind hierbei ausgewählte Hyperparameter der Basisarchitektur. Dies können Hyperparameter im engeren Sinn wie die Learning Rate oder die Anzahl der tiefen Schichten sein, oder auch abstraktere Variationsmöglichkeiten wie die Art der Aktivierungsfunktion oder die Strukturierung der Schichten (ResNetV1 nach \cite{he_deep_2015} und ResNetV2 nach \cite{he_identity_2016}). \Cref{fig:hypermodell} zeigt das für die ResNet-Basisarchitektur erstellte Hyperparameter-Metamodell.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 799 257]{shl/stft-layer-output.pdf}
  \caption{STFT-Repräsentation des Eingabefrequenzspektrums von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$.}\label{fig:stft-layer-output}
\end{figure}

Darüber hinaus können auch die eindimensionalen Convolution-Schichten der Basisarchitektur auf zweidimensionale Convolution-Schichten übertragen werden. Dies wird in Betracht gezogen, da ResNets ursprünglich nicht für die Verarbeitung von 1D-Sequenzen entwickelt wurden, sondern zur Klassifikation von Bildern in Form von zweidimensionalen RGB- bzw. BGR-Pixeldaten. Hierfür wird eine portierbare STFT-Transformationsschicht (\textit{Short-Time-Fourier-Transform}) vorgeschaltet, deren Ausgabe analog zu \Cref{fig:stft-layer-output} als Eingabe für diese Architekturen dienen können. Im Zuge dessen könnten auch auf dem Imagenet-Datensatz aus \cite{deng_imagenet_2009} vortrainierte ResNet-Modelle (ResNet50, ResNet101, ResNet152) in die Rastersuche einbezogen werden, um Transfer Learning als konzeptuellen Bestandteil mit einzubeziehen und potenziell von den bereits erlernten visuellen Features zu profitieren.

\subsection{Metamodell-Optimierung}

\begin{table}[h]
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{@{\extracolsep{\fill}}llp{8cm}@{}}
  \toprule
  Hyperparameter & Beschreibung & Suchraum \\
  \midrule
  \midrule
  \multicolumn{3}{c}{Variable Hyperparameter} \\
  \midrule
  Modelltyp
    & Input-Dimensionalität und Vorschaltung einer STFT-Schicht
    & 1D, 2D mit STFT \\
  Version
    & Architektur-Version nach \cite{he_deep_2015} und \cite{he_identity_2016}
    & ResNetV1, ResNetV2 \\
  Stacks
    & Anzahl der Convolution-Stacks nach dem Input-Stack
    & 1, 2, 3, 4 \\
  Tiefe
    & Anzahl der Convolution-Schichten in Convolution-Stacks
    & 4, 8 für Stack 3 (falls existent) und 6, 23, 36 für Stack 4 (falls existent) \\
  Pooling
    & Variante der Pooling-Schichten
    & Average-Pooling, Max-Pooling \\
  Optimizer
    & Variante des Gradientenabstiegsverfahrens
    & Adam, RMSprop, SGD \\
  Learning Rate
    & Schrittweite des Gradientenabstiegsverfahrens
    & 0,1 über 0,01 bis 0,001 \\
  \midrule
  \multicolumn{3}{c}{Durch Architektur-Version definierte Hyperparameter} \\
  \midrule
  Aktivierungsfunktion
    & Art der neuronalen Aktivierungsfunktion
    & ReLU (Softmax für letzte Dense-Schicht) \\
  Regularisierung
    & Intermediäre Regularisierung zwischen Convolution-Schichten
    & Kein Dropout, keine L2/L1-Kernel-Regularisierung, jedoch Batch-Normalization und Layer-Normalization nach STFT (falls existent) \\
  \midrule
  \multicolumn{3}{c}{Weitere, fest definierte Hyperparameter} \\
  \midrule
  Batch-Size
    & Anzahl gleichzeitig verarbeiteter Trainingssamples
    & 64 \\
  Loss
    & Art der Verlustfunktion
    & Sparse Categorical Cross Entropy \\
  Early Stopping
    & Frühzeitiges Beenden des Trainings
    & Nach 5 Epochen ohne Verbesserung \\
  Learning Rate Decay
    & Automatische Reduktion der Learning Rate
    & Halbierung nach 2 Epochen ohne Verbesserung \\
  \bottomrule
  \end{tabular}%
  }
  \caption{Definierter Hyperparameter-Suchraum der Metamodell-Optimierung.}
  \label{tab:hyperparameter-suchraum}
\end{table}

Das deklarierte Metamodell ermöglicht die Erzeugung von ResNet-Modellen für den SHL-Datensatz, indem die Hyperparameter eines Modells als Parameter in das Metamodell eingegeben werden. Die genauen Hyperparameter-Variationsmöglichkeiten sind in \Cref{tab:hyperparameter-suchraum} dargestellt. Zusätzlich wird die Batch-Size gegenüber dem Prototyping von 32 auf 64 angehoben, um eine optimale Auslastung der GPU zu ermöglichen und das Rauschen des Gradientenabstiegsverfahrens zu reduzieren\footnote{Durch die Inklusion wesentlich größerer Modelle bei der Rastersuche als bei der Selektion der Basisarchitektur ist es möglich, dass die GPU das Training auf einer Batch-Size von 128 durch Überlauf des VRAM nicht durchführen kann.}. Anschließend können die erzeugten Modelle trainiert und bezüglich deren Accuracy getestet werden.

\subsubsection{Hyperband-Orakel}

Die Erzeugung der Modelle kann zufallsbasiert erfolgen oder alternativ mithilfe eines Metamodell-Orakels. Aufgabe des Hyperband-Orakels ist die Bereitstellung von Kombinationen der Hyperparameter zur Exploration des durch diese aufgespannten Hyperparameter-Suchraums, wobei gegenüber einer randomisierten Suche die Hyperparameter auf Grundlage der beobachteten Accuracy-Werte erraten werden und somit die Dauer der Suche nach einer bestmöglichen Konfiguration reduziert werden soll. Für diese Arbeit wird als Metamodell-Orakel der in \cite{li_hyperband_2018} vorgestellte \textit{Hyperband}-Algorithmus verwendet. Bei diesem Algorithmus werden die Modelle während der Hyperparameter-Exploration nicht zwangsweise über eine feste Anzahl von Epochen bis zum Early Stopping trainiert, sondern zunächst wenige Epochen, um die gesamte Dauer der Suche weiter zu reduzieren. Das Hyperband-Orakel sucht also zunächst in der Breite mit limitierter Trainingsdauer, um anschließend die vielversprechendsten Ansätze auszuwählen und diese über mehr Epochen zu trainieren.

\subsubsection{Top-$N$-Selektion}

Ergebnis der so realisierten Metamodell-Optimierung ist eine Rangliste von Architekturen, welche nach der erzielten Accuracy geordnet sind. Die besten $N$ Modelle können nachfolgend ausgewählt werden für die weitere Analyse, während die in der Rangliste darunter liegenden Modelle eliminiert werden. Ziel ist auch hier wieder die empirische Beschränkung des für eine vollumfängliche Weiterverfolgung zu großen Suchraums. $N$ kann hierbei frei gewählt werden, beschränkt sich aber theoretisch durch die maximale Anzahl von Modellen und praktisch durch die verfügbare Zeit zur Durchführung der nachfolgenden Experimente, da die gewählten Modelle je nach Architektur teils auch für mehrere Stunden trainieren können. $N$ sollte jedoch groß genug gewählt werden, um eine sinnvolle Analyse des Tradeoffs zu gewährleisten. Möglich ist beispielsweise auch die Definition einer mindestens zu erreichenden Accuracy, welche sich unter anderem an der erreichten Accuracy der traditionellen Modelle als \textit{Baseline}-Accuracy orientieren kann.

\subsubsection{Finalisierung}

Die bei der Hypermodell-Optimierung gefundenen Accuracy-Werte zu den jeweiligen Modellkonfigurationen sind nicht als final zu betrachten, denn das Training der Modelle kann durch den Hyperband-Algorithmus vor Erreichen der bestmöglichen Ergebnisqualität terminiert werden. Die Accuracy-Werte eignen sich somit zwar, um eine relative Ordnung zwischen den Ergebnisqualitäten der Modelle herzustellen, aber nicht, um den Tradeoff zwischen einer \textit{bestmöglichen} Accuracy und Ressourcenverbrauch zu analysieren. Daher werden alle selektierten Modelle für die nachfolgende Tradeoff-Analyse durch Training bis zum Early Stopping finalisiert.

\subsection{Analyse des Ressourcenverbrauchs durch Portierung}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 671 171]{analyse-portierung.pdf}
  \caption{Ermittlung der Resourcenfaktoren durch Portierung.}\label{fig:analyse-portierung}
\end{figure}

Für die aus der Rastersuche über die selektierte Basisarchitektur resultierenden Top-N-Modelle ist nach Finalisierung bereits bekannt, welche Accuracy-Werte diese erreichen können. Die durch Top-N-Selektion realisierte Elimination der schlechteren Modelle realisiert zusätzlich, dass eine zuvor wählbare Mindest-Accuracy von allen Modellen erreicht wird (\nameref{qa:q}). Für eine Tradeoff-Analyse sind jedoch noch weitere Ressourcenparameter relevant. Dies sind insbesondere der Energieverbrauch (\nameref{qa:e}), der Speicherverbrauch (\nameref{qa:s}) sowie die Inferenzzeit (\nameref{qa:inf}).

Um diese zu ermitteln, werden die Modelle in eine auf Smartphones ausführbare Repräsentation überführt (siehe \Cref{fig:analyse-portierung}). Dies gelingt durch Konversion und verlustfreie Kompression. In welches Format konkret konvertiert wird, hängt von der gewählten High-Level-API zum Training und zur Portierung der Modelle ab und ist Teil der Implementation. Wichtig ist jedoch, dass die Wahl des oder der Frameworks zur Implementation von dessen Fähigkeit zur Portierung der trainierten Modelle abhängt. Anschließend werden die Modelle jeweils auf dem Zielsmartphone eingebettet, zusammen mit der Vorverarbeitungspipeline, welche mit Bezug auf die selektierte Basisarchitektur aus der Aufzeichnung der Sensorwerte und einer nachfolgenden Yeo-Johnson-Transformation besteht. Die Konfiguration der Yeo-Johnson-Transformation ($\lambda$-Werte) wird aus der explorativen Datenanalyse entnommen und auf das Smartphone übertragen.

\begin{figure}[h]
  \includegraphics[width=0.75\linewidth, bb=0 0 390 145]{stream.pdf}
  \caption{Stream-Processing von Sensorwerten auf dem Smartphone zur Klassifikation des Verkehrsmittels.}\label{fig:stream}
\end{figure}

\paragraph{Interpolation und Synchronisation:} Die resultierende Stream-Processing-Pipeline ist in \Cref{fig:stream} gezeigt und orientiert sich an der Gleitfenstermethode, um kontinuierlich klassifizierbare Segmente zu erzeugen. Die Sensorwerte werden in zeitlich festen Abständen mit $100Hz$ abgetastet. Eine Interpolation der Daten ist somit nicht notwendig, da auf ein intensives Downsampling zum Alignment der Sensordaten mit den GNSS-Datenpunkten wie in \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} verzichtet wird. Außerdem muss entschieden werden, ob nach \cite{matusek_anwendung_2019} eine Synchronisation der Sensordaten notwendig ist. Dem steht grundsätzlich \cite{mairittha_improving_2020} im Widerspruch. Jedoch muss deren Argumentation, dass eine Synchronisation bei einer Aufzeichnung der Daten auf demselben Gerät nicht notwendig sei, näher diskutiert werden. Dies gilt nur dann, wenn die Datenpunkte synchron und nicht durch asynchrone Callbacks abgetastet werden, hierbei keine Verzögerungen auftreten und die Systemschnittstelle ebenfalls zum Bezug der Sensormesswerte keine internen Verzögerungen verzeichnet. Während die Synchronität der Sensorabfrage als Kriterium an die Implementation resultiert, kann von der Systemschnittstelle angenommen werden, dass diese keine signifikanten internen Verzögerungen verzeichnet. Diese Annahme resultiert aus den Akzeptanzanforderungen an die Systemschnittstelle, beispielsweise wären durch Verzögerungen auch Usability-Einschränkungen in Smartphone-Spielen gegeben, wenn diese durch Bewegung des Geräts mit dem Nutzer interagieren. Treten trotzdem minimale Verzögerungen auf, so ist anzunehmen, dass die Klassifikation keine signifikant anderen Vorhersagen trifft.

\paragraph{Effizienzsteigerung durch Reduktion der Inferenzrate:} Theoretisch wäre es mit der gezeigten Stream-Processing-Pipeline möglich, pro Sekunde 100 Inferenzprozesse durchzuführen. Dies ist jedoch nicht zielführend. Um den Energieverbrauch zu reduzieren, genügt es, wenn pro Sekunde dem Anwendungsfall entsprechend deutlich weniger Inferenzprozesse durchgeführt werden.

