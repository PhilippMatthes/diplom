\chapter{Konzept}\label{ch:konzept}

Ziel der Konzepts ist die Ermittlung eines Pareto-optimalen Modells bezüglich der Relation zwischen Ressourcenverbrauch und Ergebnisqualität auf Smartphones. Zunächst muss hierfür ein Datensatz akquiriert werden, welcher zum Training und zur Evaluation genutzt werden kann. Im Rahmen der Forschungsanalyse konnte kein konkreter Modellansatz isoliert werden, der übergreifend die besten Ergebnisse erzielt. Es existieren stattdessen mehrere mögliche Herangehensweisen für die Konfiguration einer Datenvorverarbeitungspipeline und des nachfolgenden Modells. Die Konfiguration ist hierbei maßgeblich von drei Dimensionen abhängig. Bereits durch die initiale Selektion einer Basisarchitektur spannt sich eine große Menge von möglichen Modellen auf. Insbesondere für traditionelle Modelle muss ein extensives Feature-Engineering durchgeführt werden. Die so entstehenden Modellvarianten wiederum können in ihren Hyperparametern und applizierbaren Optimierungsstrategien variiert werden. Damit ist es zumindest im Rahmen dieser Arbeit nicht möglich, alle Kombinationen und Variationen auszutesten. Die zu testenden Modelle sollen stattdessen auf Grundlage empirischer Entscheidungen selektiert werden. Dies beinhaltet eine schrittweise Exploration jeder der genannten Dimensionen, wobei nach jedem Schritt das vielversprechendste Modell heuristisch selektiert wird.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 645 221]{prozedur.pdf}
  \caption{Prozedurale Ermittlung eines selektiv Pareto-optimalen Modells in drei Teilschritten durch Tradeoff-Analyse.}\label{fig:prozedur}
\end{figure}

Das konzipierte Verfahren ist in \Cref{fig:prozedur} gezeigt. Mit der Akquise des Datensatzes zusammenhängend werden in Schritt 1 des Konzepts verschiedene Feature-Extraktionsverfahren erstellt, welche anschließend genutzt werden, um eine initiale Selektion einer Basisarchitektur durch Test verschiedener Kombinationen durchzuführen. In Schritt 2 wird die Basis-Architektur durch Variation verschiedener Hyperparameter bezüglich des Tradeoffs analysiert. Schritt 3 schließt sich hieran an und analysiert den Tradeoff verschiedener Optimierungsstrategien. Das resultierende Modell soll bezüglich des Tradeoffs Pareto-optimal sein. Die schrittweise Limitation des Betrachtungsspektrums ist notwendig, um die Tradeoff-Analyse in einem realisierbaren Spektrum zu halten. Sie bedingt jedoch auch, dass unter Umständen kein global Pareto-optimales Modell unter Berücksichtigung aller Möglichkeiten gefunden werden kann. Möglich wäre es beispielsweise, dass ein initial bei der Selektion der Basisarchitektur als weniger vielversprechend geltendes und somit eliminiertes Modell durch Variation der Hyperparameter oder Optimierung einen besseren Tradeoff bietet. Dies ist bei der Evaluation zu berücksichtigen. Die einzelnen Schritte des gezeigten Verfahrens werden in den nachfolgenden Sektionen näher diskutiert.

\nocite{chollet_keras_2015,omalley_keras_2019,pedregosa_scikit-learn_2011,martin_abadi_tensorflow_2015}

\section{Explorative Datensatzanalyse zur Konfiguration der Vorverarbeitung}

Nachfolgend wird ein Datensatz für das Training der Modelle in dieser Arbeit akquiriert und anschließend exploriert. Durch die Exploration der Struktur des Datensatzes sollen sich wertvolle Erkenntnisse ergeben, welche direkt in die Strukturierung der Datenvorverarbeitung und damit auch in die erreichbare Ergebnisqualität einfließen.

\subsection{Akquise eines Datensatzes}

Als Ausgangspunkt für die datenorientierte Verkehrsmittelerkennung durch Machine-Learning-Systeme muss ein Datensatz akquiriert werden. Dieser kann selbstständig erzeugt werden, beispielsweise durch Aufzeichnung über eine prototypische Applikation, oder aus verwandten Arbeiten übernommen werden. Da bereits zahlreiche öffentliche Datensätze zur Verfügung stehen und die Nutzung eines solchen Datensatzes die Vergleichbarkeit der Ergebnisse verbessert, wird ein öffentlicher Datensatz zum Training und zur Evaluation der Modelle selektiert. Die Sussex-Huawei-Locomotion-Challenge, kurz SHL-Challenge, ist ein jährlicher Wettbewerb, bei dem Forschungsteams Verkehrsmittelklassifikationen mit Fokus auf eine bestmögliche Accuracy entwickeln. Im Rahmen des Wettbewerbs werden mehrere Datensätze zur Verfügung gestellt, mit unterschiedlichen Strukturen. Außerdem liegen die Ergebnisse der SHL-Challenge aus 2020 bereits vor und können als Vergleichspunkt zu den Ergebnissen dieser Arbeit genutzt werden \cite{wang_summary_2020}. Die bei dieser Challenge veröffentlichten Datensätze enthalten vier Pakete zum Training, jeweils aufgezeichnet in einer unterschiedlichen Position am Körper (\textit{Torso}, \textit{Bag}, \textit{Hips} und \textit{Hand}), zusätzlich zu einem Validierungsdatensatz, welcher aus kleinen Teildatensätzen zu diesen vier Positionen besteht.

\begin{figure}[h]
\includegraphics[width=0.5\linewidth, bb=0 0 513 277]{shl/label-quantities.pdf}
\caption{Klassenverteilungen im Hand-Datensatz der SHL-Challenge aus 2020.}\label{fig:shl-label-quantities}
\end{figure}

Die Daten bestehen aus Samples von jeweils 500 Datenpunkten, welche mit einer Frequenz von 100Hz aufgezeichnet wurden. Dies ergibt eine Sample-Länge von 5 Sekunden, welche im Unterschied zu den in \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} verwendeten Fensterlängen eine verzögerungsarme Klassifikation ermöglicht (\nameref{qa:inf}). Verfügbar sind hierbei triaxiale Messwerte zu Akzelerometer, Gyrosensor und Magnetometer, sowie weitere, zum Teil bereits vorverarbeitete Messwerte wie die Orientierung als Quaternion und die lineare Beschleunigung. Die Verteilungen der einzelnen aufgezeichneten Klassen sind hierbei nicht gleich, dies zeigt \Cref{fig:shl-label-quantities} für den Hand-Datensatz. Außerdem ist eine Klasse \texttt{Null} im Datensatz enthalten, welche als Puffer bei Beginn, Wechsel und Ende der Aufzeichnungen genutzt und im veröffentlichten Datensatz entfernt wurde. Der SHL-Datensatz stellt außerdem keine GNSS-Daten zur Verfügung. Dies ist jedoch in Einklang mit dem Konzept. Eine GNSS-Dimension wird bewusst nicht verwendet, um den Energieverbrauch der Datenaufzeichnung zu minimieren und mögliche Fehlerquellen mit der dazugehörigen energieintensiven Vorverarbeitung zu eliminieren (\nameref{qa:r}). Die empirische Annahme ist hierbei, dass die lokalen kinematischen Messparameter für eine hinreichende Klassifikationsqualität genügen. Hieraus resultiert gleichzeitig der Vorteil, dass kein Downsampling der lokalen kinematischen Messdaten wie in \cite{matusek_anwendung_2019,werner_kontinuierliche_2020,stojanov_continuous_2020} durchgeführt werden muss, verbunden mit einem potenziell höheren Informationsgehalt des Ausgangsdatensatzes durch Erhalt der Struktur der Zeitlinien.

\subsection{Speicheroptimiertes Laden von Samples}

Der SHL-Datensatz wird unterteilt in zwei separierte Pakete zum Training und zur Validierung, welche jeweils alle vier aufgezeichneten Positionen am Körper beinhalten. Der Validierungsdatensatz wird für dieses Konzept als Testdatensatz betrachtet. Der Testdatensatz sollte hierbei bis zur Ermittlung der Metriken zur Ergebnisqualität nicht mit dem Trainingsprozess des Modells in Kontakt kommen, um eine verlässliche Aussage über die Generalisierungsfähigkeit treffen zu können. Somit soll eine signifikante Überschätzung der Ergebnisqualität vermieden werden, wie sie in der SHL-Challenge bei einigen Forschungsteams aufgetreten ist \cite{wang_summary_2020}.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 491 138]{streaming.pdf}
  \caption{Verfahren zum verzögerungsarmen und speicheroptimierten Laden von Samples für das Training und die Validierung von Deep-Learning-Modellen.}\label{fig:streaming}
\end{figure}

Die Größe des Trainingsdatensatzes (ca. 100GB) stellt ein konzeptuelles Problem dar. Während traditionelle Modelle auf einem kleineren Teil des Trainingsdatensatzes trainiert werden können, profitieren Deep-Learning-Modelle von der Bereitstellung des gesamten Datensatzes. Das vollumfängliche Laden der Datensätze kann die Limitationen des Trainingscomputers übersteigen und ist mit einer langen Wartezeit vor Start des Trainings verbunden. Daher werden die Trainings- und Testdaten speicheroptimiert geladen. Der konzeptionelle Prozess ist in \Cref{fig:streaming} gezeigt. Die Teildatensätze werden zunächst segmentiert und anschließend in Teilen geladen. Aus den Segmenten werden kleine Pakete von Samples (Batches) generiert, welche anschließend vorverarbeitet (siehe nachfolgende Sektionen) und insbesondere auch vermischt werden, um eine möglichst alternierende Verteilung der Positionen am Körper zu realisieren und das Overfitting des Modells auf einer Position zu reduzieren. Die resultierenden Batches können anschließend zum Training und zum Test eines Deep-Learning-Modells genutzt werden, wobei durch partielles Laden der Datensätze der Speicher des Trainingscomputers nicht überbeansprucht wird und ein schneller Beginn des Trainings möglich ist.

\subsection{Feature-Engineering}

Ziel des Feature-Engineerings ist es, Inputs für die verschiedenen möglichen Modellarchitekturen bereitzustellen. Daher werden mehrere Varianten der Bereitstellung von Features betrachtet, um anschließend jeweils die bestmögliche Feature-Pipeline für jedes individuelle Modell zur sich anschließenden initialen Selektion einer Basisarchitektur bereitzustellen. Dies inkludiert die Berechnung von transformierten Zeitliniendaten, Shallow-Features und Non-Shallow-Features.

\subsubsection{Vorverarbeitung von Zeitliniendaten}

Die Zeitliniendaten des SHL-Datensatzes können direkt geladen und anschließend für Training und Test genutzt werden. Unter anderem sind hierbei jedoch viele zusätzliche Zeitlinien-Informationen enthalten, welche bezüglich des potenziellen Informationszugewinns zu prüfen sind. Daher werden zwei mögliche Bildungen von direkten Zeitlinien-Features untersucht. Zum einen die Verwendung aller bereitgestellten Informationen mit der damit verbundenen relativen Erhöhung der Operationen zur Berechnung (energieintensiver) und zum anderen die Verwendung einer Selektion aus den möglichen Informationen.

Als Grundlage der selektiven Berechnung sollen die im Datensatz enthaltenen Akzelerometer-, Magnetometer- und Gyrosensordaten verwendet werden. Durch die Berechnung des punktweisen Vektorbetrags über diese Zeitlinien lässt sich die Dimensionalität des Inputs für das Machine-Learning-Modell zusammen mit den notwendigen Operationen reduzieren. Die Gleichungen sind $\left| ACC \right| = \sqrt{ACC_x^2 + ACC_y^2 + ACC_z^2}$, $\left| MAG \right| = \sqrt{MAG_x^2 + MAG_y^2 + MAG_z^2}$ und $\left| GYR \right| = \sqrt{GYR_x^2 + GYR_y^2 + GYR_z^2}$. Die statistischen Auswirkungen dieser Berechnung sind in den nachfolgenden Grafiken dargestellt.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1396 386]{shl/akzelerometer-boxplot.pdf}
\caption{Statistische Verteilung der Sensorkomponenten im Akzelerometer-Signal anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-akzelerometer-boxplot}
\end{figure}

In \Cref{fig:shl-akzelerometer-boxplot} wird die in den Grundlagen erläuterte Beobachtung deutlich, dass der Gravitationsvektor in den Teilachsen einen vertikalen Verschiebungsfaktor darstellt. Durch Berechnung des Vektorbetrags zentriert sich die Verteilung deutlich vertikal um die Erdbeschleunigung, welche bei ungefähr $9,81 \frac{m}{s^2}$ liegt. Durch Subtraktion dieser Beschleunigung, beispielsweise durch Standardisierung, ließe sich der Gravitationsvektor zumindest aus dem Vektorbetrag effizient und verlustfrei entfernen, im Unterschied zum schätzungsbedingt fehlerbehafteten und berechnungsintensiveren AHRS-Verfahren. Es liegt nahe, dass Informationen aus dem SHL-Datensatz wie der aktuelle Gravitationsvektor ($GRA_{x,y,z}$), die lineare Beschleunigung ($LACC_{x,y,z}$) und die Ausrichtung des Geräts in Quaternionen ($ORI_{w,x,y,z}$) durch die Elimination des Gravitationsvektors für die Klassifikation an Bedeutung verlieren. Umgekehrt lässt sich vermuten, dass ein geeignetes Machine-Learning-Modell quantitative Zusammenhänge zwischen $GRA_{x,y,z}$, $ORI_{w, x,y,z}$, $LACC_{x,y,z}$ und $MAG_{x,y,z}$, $ACC_{x,y,z}$, $GYR_{x,y,z}$ erlernen und kombinieren kann. Das Risiko des Overfittings ist bei Letzterem jedoch vermutlich höher, die Transformation in $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ kann als Regularisierung auf den Inputs interpretiert werden.

Zurück auf die Daten aus \Cref{fig:shl-akzelerometer-boxplot} schauend wird deutlich, dass die größten Auslenkungen der Amplitude erwartungsgemäß beim Rennen zu messen sind. Diese Aktivität lässt sich vermutlich leicht von den Verkehrsmittel Bus oder Zug unterscheiden, allein durch Betrachtung der Akzelerometerdaten.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1403 386]{shl/magnetometer-boxplot.pdf}
\caption{Statistische Verteilung der Sensorkomponenten im Magnetometer-Signal anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-magnetometer-boxplot}
\end{figure}

Ähnliche Beobachtungen lassen sich ergänzend anhand von \Cref{fig:shl-magnetometer-boxplot} feststellen. Der Vektorbetrag des Magnetometer-Signals zentriert sich um die erwartbare Magnetfeldstärke der Erde und schwankt in bestimmten Verkehrsmitteln durch Anwesenheit von artifiziellen Magnetfeldquellen nach oben und unten. Deutlich wird die starke Variabilität des Magnetfeldes in den einzelnen Teilachsen, bedingt durch die Ausrichtung des Geräts. Auch hier kann durch den Vektorbetrag die Ausrichtung des Geräts als transformativer Faktor eliminiert werden.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1399 386]{shl/gyrosensor-boxplot.pdf}
\caption{Statistische Verteilung der Sensorkomponenten im Gyrosensor-Signal anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-gyrosensor-boxplot}
\end{figure}

\Cref{fig:shl-gyrosensor-boxplot} zeigt die Anwendung des Vektorbetrags auf die Winkelauslenkung in $\frac{rad}{s}$. Die größten Auslenkungen sind erwartungsgemäß beim Rennen messbar. Auch die Klasse Fahrradfahren zeigt eine erhöhte Winkelauslenkung. Eine vertikale Verschiebung der Messdaten, wie sie beim Akzelerometer und beim Magnetometer beobachtet werden kann, ist hierbei nicht beobachtbar. Stattdessen sind die Daten durch die Messung der zeitlichen Änderung des Signals um den Nullpunkt zentriert, da auf jede ins System eingebrachte Rotation auch zwangsläufig eine entgegengesetzte Rotation folgt. $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ betrachtend lässt sich abschließend also anhand der explorativen Datenanalyse feststellen, dass trotz Reduktion der Signale $MAG_{x,y,z}$, $ACC_{x,y,z}$ und $GYR_{x,y,z}$ von insgesamt 9 Dimensionen auf 3 Dimensionen eine für die Klassifikation wichtige Variabilität zwischen den Klassen weiterhin klar erkennbar bleibt, wenn auch abhängig von der Unterschiedlichkeit der inhärenten Bewegungsmodalitäten. Ob die Bildung der Vektorbeträge jedoch wirklich vielversprechender als die energieintensivere Auswahl aller Zeitlinien ist, wird erst zu einem späteren Punkt in der Arbeit in Verbindung mit einem dazugehörigen Machine-Learning-Modell getestet.

\subsubsection{Selektion einer Skalierungsmethode}\label{sec:skalierungsmethode}

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1057 277]{shl/sensor-histograms.pdf}
\caption{Statistische Verteilung der Signalstärken von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ im SHL-Hand-Datensatz.}\label{fig:shl-sensor-histograms}
\end{figure}

Die Auswahl der Skalierungsmethode für die Eingaben ist für einige Modelle wichtig, um eine bestmögliche Ergebnisqualität zu erzielen. Hierzu wird die statistische Verteilung der Eingaben näher betrachtet. In \Cref{fig:shl-sensor-histograms} sind die Verteilungen von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ für den SHL-Hand-Datensatz illustriert, die Prinzipien lassen sich jedoch auch auf alle partiellen Zeitlinien $GRA_{x,y,z}$, $ORI_{w, x,y,z}$, $LACC_{x,y,z}$, $MAG_{x,y,z}$, $ACC_{x,y,z}$ und $GYR_{x,y,z}$ ableiten.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1057 277]{shl/scaler-histograms.pdf}
\caption{Anwendung der Standardisierung, Yeo-Johnson-Transformation und Box-Cox-Transformation auf $\left| ACC \right|$, Markierung bei $-1$ und $1$.}\label{fig:shl-scaler-histograms}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1058 277]{shl/yeo-johnson-histograms.pdf}
  \caption{Anwendung der Yeo-Johnson-Transformation auf $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$, Markierung bei $-1$ und $1$.}\label{fig:shl-yeo-johnson-histograms}
\end{figure}

\Cref{fig:shl-scaler-histograms} zeigt zunächst die bereits erläuterte Standardisierung. Die Standardisierung verändert die Verteilung der Daten nicht morphologisch, stattdessen werden die Daten mit Erhaltung von Ausreißern um den horizontalen Nullpunkt skaliert. In manchen Fällen ist es zur Vorverarbeitung sinnvoll, die Daten zusätzlich morphologisch um den horizontalen Nullpunkt symmetrisch anzugleichen, indem spezielle Power-Transformationen angewandt werden. \cite{werner_kontinuierliche_2020} nutzt die Yeo-Johnson-Transformation, welche in \Cref{fig:shl-scaler-histograms} und \Cref{fig:shl-yeo-johnson-histograms} gezeigt ist. Die Yeo-Johnson-Transformation ist auf einem Datenpunkt $x$ wie folgt definiert.

\begin{equation}
\begin{aligned}
  yeojohnson(x;\lambda) &=
  \boldsymbol 1 _{(\lambda \neq 0, x \geq 0)} \frac{(x+1)^\lambda-1}{\lambda} \\
  &+ \boldsymbol 1_{(\lambda = 0, x \geq 0)} \log (x+1) \\
  &+ \boldsymbol 1_{(\lambda \neq 2, x < 0)} \frac{(1-x)^{2-\lambda}-1}{\lambda - 2} \\
  &+ \boldsymbol 1_{(\lambda = 2, x < 0)} -\log (1-x) \\
\end{aligned}
\end{equation}

Der Parameter $\lambda$ wird durch die Maximum-Likelihood-Methode\footnote{\url{https://de.wikipedia.org/wiki/Maximum-Likelihood-Methode} (Abgerufen am 5.8.2021)} geschätzt. Ziel der Anwendung einer solchen Power-Transformation ist, die Abschrägung (\textit{skew}) der Daten zu eliminieren, um den Bias des Modells zu verringern und den Lernerfolg zu steigern. Die Yeo-Johnson-Transformation ist eine Erweiterung der in \Cref{fig:shl-scaler-histograms} gezeigten Box-Cox-Transformation und dient als grundlegende Skalierungsmethode für die Machine-Learning-Modelle in dieser Arbeit.

\subsubsection{Bildung von Shallow-Features}

Würden nur die skalierten Zeitliniendaten anhand der bisherigen Beschreibungen bereitgestellt, so könnten traditionelle Machine-Learning-Ansätze nicht effektiv in die Erstauswahl einer Basisarchitektur mit einbezogen werden, da sie analog zur Forschungsanalyse auf reinen Zeitliniendaten nicht hinreichend generalisieren können. Es sollen zusätzlich noch statistische und frequenzbezogene Features ermittelt werden, welche insbesondere für diese Modelle herangezogen werden können. Gleichzeitig muss die Anzahl und Berechnungskomplexität von Features so gering wie möglich gehalten werden, um die Energieintensität zu reduzieren (\nameref{qa:r}). Die Auswahl von Features kann durch automatisierte Analyse der statistischen Signifikanz durchgeführt werden, hierfür ist das in \cite{christ_time_2018} beschriebene Framework eine attraktive Lösung. Für die Entscheidung, ob traditionelle Modelle generell konzeptuell weiter verfolgt werden sollen, genügt jedoch zunächst eine Auswahl typischer Features, wie sie in verwandten Arbeiten beschrieben werden. Aussagekräftige und effizient berechenbare statistische Features sind beispielsweise das arithmetische Mittel, die Standardverteilung, sowie das Maximum und Minimum des jeweiligen Zeitliniensignals. Hinzu kommen frequenzbezogene Features. Das Frequenzspektrum kann über eine Fast-Fourier-Transformation ermittelt werden, mit der gegebenen Abtastrate von 100Hz ist so eine direkte Zuordnung der Signalstärken zur Frequenz möglich.

\begin{figure}[h]
\includegraphics[width=\linewidth, bb=0 0 1039 443]{shl/sensor-fft.pdf}
\caption{Statistische Verteilung der Frequenzen von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ anhand der Klassen im SHL-Hand-Datensatz.}\label{fig:shl-sensor-fft}
\end{figure}

\Cref{fig:shl-sensor-fft} zeigt einen Ausschnitt der hieraus resultierenden Verteilung der Signalstärken für $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$. Sichtbar wird hierbei, dass die verschiedenen Verkehrsmittelklassen unterschiedlich zu den einzelnen Frequenzbereichen beitragen. Dies lässt vermuten, dass die Transformation der Zeitliniendaten in die Frequenzdomäne weitere Unterscheidungsmerkmale bereitstellen kann. Aus dem Frequenzspektrum lassen sich somit weitere Features ableiten, hierunter das Maximum und Minimum der Signalstärke, die Entropie des Signals, sowie die Durchschnittsfrequenz.

\subsubsection{Bildung von Non-Shallow-Features}

Neben Shallow-Features und den vorverarbeiteten Zeitliniendaten sollen auch Non-Shallow-Features in das Konzept einfließen. Für die Realisation wird ein Autoencoder erstellt. Hierbei handelt es sich um ein künstliches neuronales Netzwerk, welches eine effiziente Kodierung der Zeitliniendaten erlernt. Das Netzwerk besteht aus einem enkodierenden Teil (Encoder), welcher die Eingangsdaten auf eine kompakte interne Repräsentation (Coding) abbildet, sowie einem dekodierenden Teil (Decoder), welcher die kompakte Repräsentation in die ursprünglichen Zeitlinien zurückverfolgen soll. Der Decoder wird hierbei nur zum Training des Modells benötigt. Nach dem Training wird der Encoder vom Decoder isoliert. Die kompakte Repräsentation des Encoders auf den Eingabedaten dient als Ausgabe von Non-Shallow-Features.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 704 235]{shl/autoencoder.pdf}
  \caption{Resultate eines 1D Deep Convolutional Autoencoders auf Akzelerometerdaten des SHL-Testdatensatzes.}\label{fig:autoencoder}
\end{figure}

In \Cref{fig:autoencoder} sind die Resultate eines aus dem Prototyping entstandenen Autoencoders gezeigt. Der Autoencoder besteht aus mehreren, konzentrisch kleiner werdenden Schichten von eindimensionalen Convolution-Schichten mit zwischengeschalteten Dropout-Schichten zur Regularisierung. Diese Architektur eignet sich für die komplexe Repräsentation von Zeitlinien wesentlich besser als LSTM-Schichten oder Dense-Schichten \cite{martinez_autoencoders_2019}. Auch durch wiederholte Variation der Hyperparameter des Autoencoders konnte jedoch keine hinreichende Generalisierungsfähigkeit erreicht werden. Diese Ergebnisse werden in \Cref{fig:autoencoder} deutlich. Die Kurvenverläufe können teils rekonstruiert werden, bei einigen Samples jedoch kann keine hinreichende interne Repräsentation gefunden werden und die Rekonstruktion fällt zurück auf ein wiederkehrendes Muster (zweite Spalte von rechts). Zu beobachten ist auch, dass der Autoencoder stets zur Minimierung der Kosten dazu bestrebt ist, die Ausgangszeitlinie ähnlich wie beim Moving-Average-Verfahren direkt abzubilden, statt eine abstraktere Repräsentation zu etablieren. Wird die Dimensionalität des Encoders zu groß gewählt, so konnte beobachtet werden, dass das Modell lediglich die Eingabedaten in transformierter Form im Encoding abspeichert. Dies würde jedoch vermutlich keinen Mehrwert für ein nachgeschaltetes Klassifikationsmodell gegenüber den reinen Eingangsdaten bieten.

Non-Shallow-Features im Sinne der rekonstruierbaren Repräsentation einer Zeitlinie werden vor dem Hintergrund dieser Ergebnisse nicht weiter betrachtet. Das Potenzial dieser Methode ist jedoch durch die prototypische Vorbetrachtung nicht ausgeschöpft und kann in einer späteren Arbeit nochmals betrachtet werden. Als Ausgangspunkt hierfür könnten beispielsweise Regularisierungen gesucht werden, welche das Bestreben des Autoencoders zur direkten Abspeicherung der Zeitlinie im Encoding mitigieren. Dass Non-Shallow-Features in diesem Rahmen als nicht vielversprechend bewertet wurden, bedeutet jedoch nicht, dass Deep-Learning-Klassifikationsmodelle keine hinreichende interne Repräsentation der Klassen erlernen können, denn hierbei wird im Unterschied zum Autoencoder nicht die Identitätsfunktion auf den Trainingsdaten erlernt.

\subsubsection{Gewichtung der Klassen}

In \Cref{sec:skalierungsmethode} wurden spezielle Skalierungsmethoden selektiert, um die Verzerrung der Wertebereichsverteilung möglichst zu reduzieren. Ein weiteres Problem stellen die ungleich verteilten Klassen im Datensatz dar (siehe \Cref{fig:shl-label-quantities}). Trainiert das Modell auf überdurchschnittlich vielen Samples zu einer Klasse, so lernt das Modell, diese Klasse zur Reduktion der Kostenfunktion häufiger vorherzusagen. Dies ist mit einem weiteren Problem verbunden. Angenommen, der Trainings-, Test- und Validierungsdatensatz werden über demselben ungleich gewichteten Datensatz gebildet. So wird ein eventueller Bias nicht durch eine Analyse der Metriken auf dem Testdatensatz sichtbar. Es ist also zu erwarten, dass das Modell in diesem Fall eine künstlich erhöhte Accuracy verzeichnet und auf neuen Datensätzen signifikant schlechter generalisiert. Auch aus Gründen der Vergleichbarkeit der Metriken ist es somit unbedingt notwendig, dass die Klassen zum Training des Modells gleich gewichtet werden.

Bei traditionellen Modellen bietet es sich an, die Gleichgewichtung durch Transformation des Datensatzes selbst zu realisieren. Die unterrepräsentierten Klassen können durch das Verfahren \textit{Synthetic Minority Over-sampling Technique} (SMOTE) auf Grundlage eines randomisierten Auswahlverfahrens repliziert werden \cite{chawla_smote_2002}. Dies ist jedoch mit dem Problem verbunden, dass auf diese Weise synthetische Duplikate im Datensatz angefertigt werden und das Modell somit keine neuen Informationen erhält. Stattdessen trainiert das Modell gegebenenfalls mehrere Male auf demselben Sample, bei zu starkem Oversampling neigt das Modell anschließend zum Overfitting. Um den Anteil der synthetischen Samples zu reduzieren, werden daher Samples der überrepräsentierten Klassen randomisiert eliminiert (\textit{Undersampling}). Das traditionelle Modell trainiert anschließend auf einem balancierten Datensatz.

Für Modelle aus dem Deep Learning lässt sich eine weitere Technik anwenden, um eine relative Gleichgewichtung des Datensatzes beim Lernen zu erzielen. Statt den Datensatz selbst anzugleichen, wird die Verlustfunktion relativ zur betrachteten Klasse gewichtet. Im übertragenen Sinn widmet sich das Modell den überrepräsentierten Klassen anschließend weniger als den unterrepräsentierten Klassen \cite{cui_class-balanced_2019}.

\subsection{Initiale Datenvorverarbeitungspipeline}

Die Vorverarbeitung muss so sparsam wie möglich gehalten werden (\nameref{qa:r}). Die Implementation eines Schätzverfahrens wie AHRS zur Transformation der lokalen kinematischen Messdaten in ein Referenzkoordinatensystem wird durch Bildung des Vektorbetrags und damit einer sparsameren Alternative ersetzt. Eine verbleibende vertikale Verschiebung wie die der Akzelerometerdaten wird durch die Yeo-Johnson-Transformation eliminiert, die Schrägheit der Verteilung in den resultierenden Vektorbetragsdaten in eine annähernd symmetrisch verteilte Repräsentation überführt.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 735 173]{vorverarbeitung.pdf}
  \caption{Initiale Vorverarbeitungspipeline.}\label{fig:vorverarbeitungspipeline}
\end{figure}

\Cref{fig:vorverarbeitungspipeline} zeigt diese Schritte. Im Anschluss hieran werden die Daten entweder für das Training von traditionellen Modellen in Shallow-Features überführt und durch SMOTE gewichtet, oder für das Training von Deep-Learning-Modellen als direkte Zeitliniendaten bereitgestellt, zusammen mit einer Auflistung der Klassenverteilungen zur Gewichtung der Kostenfunktion beim Training.

\section{Empirische Selektion einer Basisarchitektur}

Zur Selektion der Basisarchitektur wird auf die zwei Feature-Schnittstellen der Vorverarbeitungspipeline zurückgegriffen. Es werden typische Metaarchitekturen und Modellkonfigurationen aus verwandten Arbeiten extrahiert und auf dem SHL-Datensatz prototypisch trainiert. Die Selektion erfolgt argumentativ auf Grundlage der so ermittelten Parameter, der Ergebnisse verwandter Arbeiten und unter Berücksichtigung des Potenzials für die nachfolgenden Selektionsschritte.

\subsection{Test von traditionellen Modellen}

Die Forschunganalyse hat ergeben, dass traditionelle Modelle bei der Verkehrsmittelerkennung gegenüber Deep-Learning-Ansätzen nicht zu vernachlässigen sind. Gleichzeitig gelten Deep-Learning-Ansätze speziell bei der Verfügbarkeit von großen Datensätzen wie dem SHL-Datensatz als die vielversprechenderen Ansätze. Die Ergebnisqualität der traditionellen Modelle ist außerdem maßgeblich von der Auswahl und Anzahl der berechneten Shallow-Features abhängig, wobei deren Berechnung zur Energieintensität beiträgt. Mit Betrachtung der von Forschungsteams erzielten Ergebnisse bei der SHL Challenge aus 2020 zeigt sich, dass der beste Deep-Learning-Ansatz auf Zeitliniendaten mit einem F1-Score von $88,5\%$ dem besten traditionellen Ansatz ($77,9\%$) mit Abstand überlegen ist \cite{wang_summary_2020}.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1063 262]{shl/traditional-models.pdf}
  \caption{Erreichte Accuracy-Werte und F1-Scores traditioneller Machine-Learning-Modelle auf dem SHL-Datensatz mit 24 Features pro Sample.}\label{fig:shl-traditional-models}
\end{figure}

Die vielversprechendste Architektur lässt sich anhand dessen jedoch nicht zwangsweise vorhersagen, hierfür ist die Abhängigkeit des Modells von der Datenvorverarbeitung zu stark. Daher werden mehrere traditionelle Modelle auf den ermittelten Shallow-Features des Datensatzes trainiert. Hierfür wurde die Anzahl der Features auf insgesamt 24 pro Sample limitiert, um die Energieintensität des Ansatzes zu minimieren. Auf den Shallow-Features konnten traditionelle Verfahren einen F1-Score auf dem Testdatensatz von maximal $70,42\%$ sowie eine Accuracy von $70,32\%$ ohne Postprocessing erreichen. Im Rahmen der SHL-Challenge würde sich diese Ergebnisqualität auf Platz 5 von 16 einordnen \cite[S. 3]{wang_summary_2020}, womit dieser Score zwar deutlich unter den typischen Ergebnisqualitäten in der Forschungsdomäne läge, jedoch für den SHL-Datensatz ein akzeptables Ergebnis darstellen würde. Das MLP ergibt sich aus der Analyse als das am besten klassifizierende traditionelle Modell, während die Speichergröße des MLP wesentlich unter dem betrachteten KNN- und SVM-Modell liegt.

\subsection{Test von Deep-Learning-Modellen}

Das MLP scheint als neuronales Modell bereits bessere Ergebnisse zu erzielen als andere traditionelle Verfahren. Daher lohnt sich ein Blick auf tiefere neuronale Modelle. Hierfür werden verschiedene Deep-Learning-Modelle, wieder prototypisch, jedoch direkt auf den Zeitliniendaten getestet. Insgesamt wurden die folgenden Modelle durch Integration in die zuvor erläuterte Vorverarbeitungspipeline getestet:

\begin{itemize}
\item Ein 1D-CNN mit ReLU-Aktivierungsfunktion und Regularisierung durch Batch Normalization, wobei der ersten Convolution-Schicht eine FFT-Transformationsschicht vorgeschaltet ist und somit indirekt auf dem Frequenzspektrum klassifiziert wird
\item Ein 1D-CNN mit ReLU-Aktivierungsfunktion und Regularisierung durch Batch Normalization nach \cite{wang_time_2016}
\item Ein 1D-CNN mit ReLU-Aktivierungsfunktion und Regularisierung durch Batch Normalization, welches durch seine besondere Tiefe und durch Shortcuts zwischen tiefen Schichten gekennzeichnet ist (\textit{ResNet}), nach \cite{ismail_fawaz_deep_2019}
\item Ein LSTM, welches einem einschichtigen CNN mit ReLU-Aktivierungsfunktion zur Kodierung der Eingabewerte nachgeschaltet ist (\textit{CNN-LSTM})
\item Ein LSTM mit Dropout-Regularisierung, welches die Zeitliniendaten als direkte Inputs erhält
\item Ein \textit{attention}\footnote{\url{https://en.wikipedia.org/wiki/Attention_(machine_learning)} (Abgerufen am 16.8.2021)}-basierter Transformer-Netzwerk nach \cite{vaswani_attention_2017, theodoros_ntakouris_keras_2021}
\end{itemize}

\paragraph{Hyperparameter des prototypischen Trainingsprozesses:} Beim Training der Deep-Learning-Modelle werden folgende Regeln angewandt. Die Modelle werden in die zuvor erläuterte Vorverarbeitungspipeline eingebunden und auf Batches mit einer Größe von 32 Samples durch Supervised Learning trainiert, die Samples werden einem Shuffling unterzogen. Die Validierung des Modells geschieht auf einem separierten Datensatz. Als Variante des Gradientenabstiegsverfahren wird der Adam-Optimizer genutzt. Als zugrundeliegende Verlustfunktion wird die \textit{Sparse Categorical Cross Entropy} verwendet, ein Derivat der Kreuzentropie\footnote{\url{https://de.wikipedia.org/wiki/Kreuzentropie} (Abgerufen am 16.8.2021)}, welche die Notwendigkeit eines One-Hot-Encodings der Labels erübrigt. Auf Grundlage der Validierungsmetriken wird Early Stopping verwendet, sowie eine automatisierte Reduktion der Learning Rate bei einer stagnierenden Verbesserung der Accuracy auf den Validierungsdaten.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 856 207]{shl/deep-learning-models.pdf}
  \caption[Erreichte Accuracy-Werte von Deep-Learning-Modellen auf Zeitliniendaten des SHL-Datensatzes.]{Erreichte Accuracy-Werte von Deep-Learning-Modellen auf Zeitliniendaten des SHL-Datensatzes. Legende: Accuracy auf Trainingsdaten (obere Linie), Accuracy auf Testdaten (untere Linie), bestes Ergebnis traditioneller Modelle auf händischen Features (horizontale Gerade).}\label{fig:shl-deep-learning-models}
\end{figure}

Die besten Ergebnisqualitäten wurden hiermit durch das ResNet-Modell und das LSTM-Modell erreicht. Beide Modelle erreichen auf Zeitliniendaten bessere Ergebnisqualitäten als das MLP auf Shallow-Features. Die Trainingsverläufe sind in \Cref{fig:shl-deep-learning-models} dargestellt. Aus den Verläufen lassen sich mehrere wertvolle Informationen ableiten. Das bezüglich der Ergebnisqualität beste Modell ist das ResNet (CNN). Hierbei ist es unerheblich, ob auf Vektorbeträgen oder auf allen Zeitlinien trainiert wird. Während das ResNet wesentlich kürzer zum Training für eine Epoche als das LSTM benötigt, trainiert es auch insgesamt für weniger Epochen bis zum Erreichen der bestmöglichen Ergebnisqualität in dieser Konfiguration. Die erreichte Accuracy von $78,94\%$ ist wesentlich besser als die des besten traditionellen Modells. Dennoch zeigt sich ein Overfitting auf den Trainingsdaten, auf welchen zeitweise mehr als $93,5\%$ Accuracy erreicht werden konnte.

\subsection{Selektion der ResNet-Architektur}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 514 241]{resnet.pdf}
  \caption[Initiale Konfiguration eines residualen Netzwerks (ResNet) als Basisarchitektur.]{Initiale Konfiguration eines residualen Netzwerks (ResNet) als Basisarchitektur. Input ist ein Sample mit 500 Datenpunkten und 3 Sensordimensionen $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$. Das CNN besteht aus 3 Blöcken mit je 3 Schichten aus Convolution, Batch Normalization und ReLU-Aktivierungsfunktion. Die Größe des Convolution-Kernels ist in jedem Block die Abfolge 8, 5, 3. Zwischen den Blöcken sind Shortcuts integriert, sogenannte \textit{Residual Connections}, welche sich an der Komposition biologischer Neuronen orientieren. Zur Klassifikation wird nach dem letzten Block ein \textit{Global Average Pooling} durchgeführt und mithilfe einer Dense-Schicht und der Softmax-Aktivierungsfunktion in die Verkehrsmittelklassen überführt.}\label{fig:resnet}
\end{figure}

Die Accuracy der in \Cref{fig:resnet} gezeigten ResNet-Architektur auf den Vektorbeträgen der Zeitliniendaten von $\left| ACC \right|$, $\left| MAG \right|$ und $\left| GYR \right|$ ist im prototypischen Vergleich mit den besten traditionellen Modellen und anderen Deep-Learning-Modellen als signifikant höher festzustellen. Obwohl LSTM-Netzwerke für die Verarbeitung von entlang einer Zeitlinie verteilten Daten konzeptioniert wurden, konnte das ResNet-Modell die Ergebnisse der getesteten LSTM-Netzwerke übertreffen. Darüber hinaus trainiert es anhand der Tests auch wesentlich schneller und benötigt weniger Epochen für das Erreichen der bestmöglichen Ergebnisqualität. Dies ist mit Hinblick auf Schritt 2 des Konzepts (\Cref{fig:prozedur}), welcher eine Rastersuche inkludiert, förderlich, da auf diese Weise in derselben Zeit mehr Modellvariationen evaluiert werden können. Insbesondere ist hierbei eine Verkleinerung des ResNet attraktiv, welche das beobachtete Overfitting reduzieren und gleichzeitig die bisher noch nicht betrachteten Ressourcenparameter verbessern kann. Auch mit Hinblick auf Schritt 3 des Konzepts bieten sich generell bei CNN-Architekturen attraktive Möglichkeiten zur Optimierung, wie die Einführung von separierbaren Convolution-Schichten, welche sich bei Verwendung von LSTM-Netzwerken nicht ergeben. Daher wird die ResNet-Architektur auf empirischer Grundlage als Basisarchitektur für die weiteren Betrachtungen selektiert.
