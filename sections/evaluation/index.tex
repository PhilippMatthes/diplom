\chapter{Evaluation}\label{ch:evaluation}

\section{Ergebnisse der Hyperband-Rastersuche}

Als Teil des Konzeptes wurden bereits verschiedene traditionelle Modelle und Deep-Learning-Modelle untersucht, um zu einem Basismodell zu gelangen, welches durch Variation der Hyperparameter einer Rastersuche unterzogen werden kann, und um schließlich die Tradeoff-Parameter hierauf zu evaluieren. Als Basisarchitektur wurde das ResNet als Erweiterung des in \cite{stojanov_continuous_2020} vorgestellten \acrshort{cnn}s gewählt. Die in \Cref{sec:metamodell-optimierung} definierten Hyperparameterdimensionen wurden implementiert und anschließend einer Rastersuche unterzogen.

\subsection{Trainingsverläufe}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1143 351]{shl/gridsearch-results.pdf}
  \caption{Trainingsverläufe der Hyperband-Rastersuche.}\label{fig:gridsearch-results}
\end{figure}

Die Trainingsverläufe der Hyperband-Rastersuche sind in \Cref{fig:gridsearch-results} gezeigt. Hierbei wird noch einmal deutlich, dass die Modelle im Rahmen des Hyperband-Algorithmus nicht über die gleiche Anzahl von Epochen trainiert wurden. Die Anzahl von Epochen wurde vom Hyperband-Orakel schrittweise selektiv erhöht. Bereits beim Training der verschiedenen Modellvariationen auf dem \acrshort{shl}-Datensatz konnten einige der Modelle die zuvor erreichte Validation-Accuracy von $78,94\%$ um einige Prozentpunkte übertreffen. Durch die Hyperband-Rastersuche konnten somit mehrere Modellvariationen mit besseren Ergebnisqualitäten als die der Basisarchitektur gefunden werden. Die Train-Accuracy liegt häufig höher, das diesbezüglich beste Modell erreichte hierbei über 95\%.

Außerdem lässt sich beobachten, dass die Kreuzentropie als Verlust auf den Validierungsdaten teils stark ansteigt, obwohl die Validation-Accuracy ebenfalls weiterhin ansteigt und nur selten zwischenzeitlich stärker abfällt. Diese Beobachtung lässt sich erklären durch eine stärkere Unsicherheit zwischen den Output-Neuronen in der letzten Dense Schicht mit Softmax-Aktivierungsfunktion. Während die relative Unsicherheit der Output-Neuronen in die Berechnung der kategorischen Kreuzentropie mit einfließt, zählt für die Berechnung der Accuracy nur das am stärksten aktivierte Neuron. Die Validation-Accuracy wird als besser vergleichbares Sortierungskriterium für die trainierten Ansätze selektiert. Zu vernachlässigen ist die teils hohe Unsicherheit der Output-Neuronen jedoch nicht, denn dies kann zu einer stärkeren Fluktuation der Klassen bei einer Echtzeitanwendung führen. Dieses Problem kann mitigiert werden, indem eine der Postprocessing-Techniken aus \cite{werner_kontinuierliche_2020} oder \cite{stojanov_continuous_2020} eingesetzt wird. Trotzdem sollte die Unsicherheit der Modelle mit in die weiteren Auswertungen mit einfließen, daher wurden die größten vier Kreuzentropien in \Cref{fig:gridsearch-results} mit deren Identifikationsnummer hervorgehoben.

\subsection{Analyse der Trainingsdauer zur Dimensionierung der Finalisierung}

\begin{figure}[h]
  \includegraphics[width=0.5\linewidth, bb=0 0 362 290]{shl/gridsearch-finalization-prediction.pdf}
  \caption{Trainings- und Finalisierungszeiten der Modelle.}\label{fig:finalization-prediction}
\end{figure}

Die Hyperband-Rastersuche wurde über einen Zeitraum von 2 Wochen durchgeführt. Als Ergebnis resultiert eine Rangfolge von insgesamt 28 der möglichen ResNet-Konfigurationen. Um zu entscheiden, wie viele der getesteten Hyperparameter-Konfigurationen insgesamt und über wie viele Epochen diese finalisiert werden sollen, wurden das Trainingsverhalten der Modelle statistisch analysiert. Da analog zu \Cref{fig:gridsearch-results} der Trainingsfortschritt (nach Validation-Accuracy) der über 10 oder mehr Epochen\footnote{Eine Epoche beinhaltet ca. 800.000 Samples aus dem \acrshort{shl}-Datensatz.} trainierten Modelle zumeist nach 5 bis 10 Epochen stagnierte, wurde eine maximale Trainingsdauer zur Finalisierung von 15 Epochen festgelegt. Bei dieser Entscheidung musste zusätzlich berücksichtigt werden, dass die Finalisierung in einem für diese Arbeit realisierbaren Zeitrahmen verbleibt. Sowohl die maximale Anzahl von Epochen, sofern kein Early Stopping einsetzt, als auch die Gesamtanzahl der für die Finalisierung selektierten Modelle musste auf Grundlage einer Schätzung eines akzeptablen Zeitaufwandes zum Training erfolgen.

Daher wurden die Trainingsverläufe zusätzlich statistisch evaluiert, um eine Schätzung über die Trainingsdauer pro Epoche zu erhalten und diese auf 15 Epochen zu extrapolieren. Das Ergebnis dieser Analyse ist in \Cref{fig:finalization-prediction} dargestellt. Das Histogramm zeigt, dass sich die meisten Modelle im Bereich von ca. 10 Minuten pro Epoche befinden, teils jedoch auch deutlich länger trainieren können, bis zu einer Trainingszeit von ca. 60 Minuten pro Epoche. Die Finalisierung eines solchen Modells würde somit geschätzt 15 Stunden betragen. Die meisten Modelle können in unter 3 Stunden finalisiert werden, sofern davon ausgegangen wird, dass die selektierte Finalisierungsdauer von 15 Epochen nach den vorigen Beobachtungen genügt. Eine weitere Betrachtung, welche bei der Dimensionierung der zu finalisierten Modelle einfloss, besteht darin, dass aus verschiedenen Gründen kein durchgängiges Training der Modelle mithilfe der gewählten Google-Cloud-Instanz möglich ist. Die Instanz wird beispielsweise, um serverseitig Ressourcen einzusammeln, ungefähr halbtäglich recycelt. Auch das Training musste für die zwischenzeitliche Unterbrechung so konfiguriert werden, dass die Modelle in Google Drive zwischengespeichert werden, um den verlorenen Fortschritt bei der Terminierung der Instanz zu minimieren. Da das Training nach zwischenzeitlicher Terminierung der Instanz, welche sich auch teils nächtlich und ohne Aufsicht ereignete, stets manuell wieder initialisiert werden musste, konnte keine effektive Trainingsdauer von 24 Stunden pro Tag erreicht werden. Technisch wäre es außerdem möglich gewesen, die SHL-Datensätze über Google Drive einzubinden, dazu hätte jedoch ein weiteres kostenpflichtiges Abonnement abgeschlossen werden müssen, um den Speicherplatz in Google Drive zu erhöhen. Nach Evaluation des Kosten-Nutzen-Verhältnisses wurde stattdessen der SHL-Datensatz stets vor Beginn des Trainings auf den Festspeicher der Cloud-Instanz heruntergeladen. Dies resultierte in einer weiteren täglichen Wartezeit von ca. einer Stunde, welche die effektive Trainingsdauer weiter reduziert. Mit Berücksichtigung dieser Faktoren wurden schließlich die 10 besten Modelle der Rastersuche selektiert und Finalisiert.

\subsection{Resultierende Modelle}

Die aus der Rastersuche resultierenden Modellkonfigurationen sind in \Cref{tab:gridsearch-results} gezeigt. Von 10 Modellen bestehen drei Modelle aus 2D-Convolution-Schichten und einer vorgeschalteten \acrshort{stft}. Die überwiegende Mehrheit der Top-10-Modelle klassifiziert direkte Zeitliniendaten mithilfe von 1D-Convolution-Schichten analog zur initial getesteten Basisarchitektur. Außerdem überrepräsentiert ist der Adam-Optimizer, der bereits in der prototypischen Analyse der Basisarchitekturen die besten Ergebnisse erzielte, sowie die ResNetV1-Bauweise.

\input{sections/evaluation/gridsearch-results}

Die Tiefe der Modelle scheint analog zu den Betrachtungen aus \Cref{ch:verwandte-arbeiten} auch von Bedeutung zu sein. Keines der Top-10-Modelle besitzt nur einen Block von Convolution-Schichten. Die meisten der Top-10-Modelle besitzt 2 Convolution-Blöcke, das tiefste Modell mit der ID \texttt{6af8a1} und der insgesamt größtmöglichen Konfiguration erzielt die zweitbeste Validation-Accuracy. Zu beobachten ist außerdem, dass die in \Cref{fig:gridsearch-results} mit der größten Kreuzentropie auf den Validierungsdaten markierten Top-10-Modelle (\texttt{e6c367}, \texttt{6af8a1}, \texttt{752315}) auch gleichzeitig die Modelle mit einer erhöhten Tiefe sind. Die Modelle \texttt{752315}, \texttt{e6c367} zeigen neben \texttt{35d171} gleichzeitig das verhältnismäßig größte Overfitting auf den Trainingsdaten.

\section{Ergebnisse der Top-10-Finalisierung}

\begin{table}[h]
  \resizebox{\textwidth}{!}{%
    \input{sections/evaluation/finalization-results}
    \caption{Ergebnisse der Top-10-Finalisierung.}
  }
\end{table}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1072 567]{shl/finalization-results-accuracy.pdf}
  \caption{Accuracy-Ergebnisse der Top-10-Finalisierung.}\label{fig:finalization-results-accuracy}
\end{figure}

\section{Ergebnisse der Tradeoff-Analyse}

\begin{landscape}
\begin{table}[]
  \resizebox{\textwidth}{!}{%
    \input{sections/evaluation/tradeoff-results}
    \caption[Ergebnisse der Tradeoff-Analyse.]{Ergebnisse der Tradeoff-Analyse. Unter Xcode 12 konnten im Unterschied zum iPhone SE trotz verschiedener Versuche zur Problembehandlung keine Energy Logs (in \acrshort{aeu}) des iPhone XS aufgezeichnet werden. Die Schnittstelle zeigte lediglich den Hinweis \enquote{No Data}, während alle anderen Hardwareparameter problemlos aufgezeichnet werden konnten. Die Instruments-Schnittstelle zur Aufzeichnung von Energy Logs wurde darüber hinaus in der aktuellsten Beta von Xcode komplett entfernt, sodass die genauen \acrshort{aeu}-Werte nur für das iPhone SE bekannt sind.}
  }
\end{table}
\end{landscape}

\section{Ergebnisse der Optimierung}

\subsection{Limitierungen beim Pruning durch \acrshort{pqat} und XNNPACK}

\subsection{Beschränkung des Anwendungsfalls}

\section{Limitationen und Risiken für die Validität}

\subsection{Abhängigkeit von Threads}

\subsection{Abhängigkeit von der Inferenzrate}

\subsection{Real-World-Performance}

\section{Vergleich mit Movebis-Ansätzen}

\section{Zusammenfassung}
