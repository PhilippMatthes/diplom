\chapter{Evaluation}\label{ch:evaluation}

\section{Ergebnisse der Hyperband-Rastersuche}

Als Teil des Konzeptes wurden bereits verschiedene traditionelle Modelle und Deep-Learning-Modelle untersucht, um zu einem Basismodell zu gelangen, welches durch Variation der Hyperparameter einer Rastersuche unterzogen werden kann, und um schließlich die Tradeoff-Parameter hierauf zu evaluieren. Als Basisarchitektur wurde das ResNet als Erweiterung des in \cite{stojanov_continuous_2020} vorgestellten \acrshort{cnn}s gewählt. Die in \Cref{sec:metamodell-optimierung} definierten Hyperparameterdimensionen wurden implementiert und anschließend einer Rastersuche unterzogen.

\subsection{Trainingsverläufe}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1143 351]{shl/gridsearch-results.pdf}
  \caption{Trainingsverläufe der Hyperband-Rastersuche.}\label{fig:gridsearch-results}
\end{figure}

Die Trainingsverläufe der Hyperband-Rastersuche sind in \Cref{fig:gridsearch-results} gezeigt. Hierbei wird noch einmal deutlich, dass die Modelle im Rahmen des Hyperband-Algorithmus nicht über die gleiche Anzahl von Epochen trainiert wurden. Die Anzahl von Epochen wurde vom Hyperband-Orakel schrittweise selektiv erhöht. Bereits beim Training der verschiedenen Modellvariationen auf dem \acrshort{shl}-Datensatz konnten einige der Modelle die zuvor erreichte Validation-Accuracy von $78,94\%$ um einige Prozentpunkte übertreffen. Durch die Hyperband-Rastersuche konnten somit mehrere Modellvariationen mit besseren Ergebnisqualitäten als die der Basisarchitektur gefunden werden. Die Train-Accuracy liegt häufig höher, das diesbezüglich beste Modell erreichte hierbei über 95\%.

Außerdem lässt sich beobachten, dass die Kreuzentropie als Verlust auf den Validierungsdaten teils stark ansteigt, obwohl die Validation-Accuracy ebenfalls weiterhin ansteigt und nur selten zwischenzeitlich stärker abfällt. Diese Beobachtung lässt sich erklären durch eine stärkere Unsicherheit zwischen den Output-Neuronen in der letzten Dense Schicht mit Softmax-Aktivierungsfunktion. Während die relative Unsicherheit der Output-Neuronen in die Berechnung der kategorischen Kreuzentropie mit einfließt, zählt für die Berechnung der Accuracy nur das am stärksten aktivierte Neuron. Die Validation-Accuracy wird als besser vergleichbares Sortierungskriterium für die trainierten Ansätze selektiert. Zu vernachlässigen ist die teils hohe Unsicherheit der Output-Neuronen jedoch nicht, denn dies kann zu einer stärkeren Fluktuation der Klassen bei einer Echtzeitanwendung führen. Dieses Problem kann mitigiert werden, indem eine der Postprocessing-Techniken aus \cite{werner_kontinuierliche_2020} oder \cite{stojanov_continuous_2020} eingesetzt wird. Trotzdem sollte die Unsicherheit der Modelle mit in die weiteren Auswertungen mit einfließen, daher wurden die größten vier Kreuzentropien in \Cref{fig:gridsearch-results} mit deren Identifikationsnummer hervorgehoben.

\subsection{Analyse der Trainingsdauer zur Dimensionierung der Finalisierung}

\begin{figure}[h]
  \includegraphics[width=0.5\linewidth, bb=0 0 362 290]{shl/gridsearch-finalization-prediction.pdf}
  \caption{Trainings- und Finalisierungszeiten der Modelle.}\label{fig:finalization-prediction}
\end{figure}

Die Hyperband-Rastersuche wurde über einen Zeitraum von 2 Wochen durchgeführt. Als Ergebnis resultiert eine Rangfolge von insgesamt 28 der möglichen ResNet-Konfigurationen. Um zu entscheiden, wie viele der getesteten Hyperparameter-Konfigurationen insgesamt und über wie viele Epochen diese finalisiert werden sollen, wurden das Trainingsverhalten der Modelle statistisch analysiert. Da analog zu \Cref{fig:gridsearch-results} der Trainingsfortschritt (nach Validation-Accuracy) der über 10 oder mehr Epochen\footnote{Eine Epoche beinhaltet ca. 800.000 Samples aus dem \acrshort{shl}-Datensatz.} trainierten Modelle zumeist nach 5 bis 10 Epochen stagnierte, wurde eine maximale Trainingsdauer zur Finalisierung von 15 Epochen festgelegt. Bei dieser Entscheidung musste zusätzlich berücksichtigt werden, dass die Finalisierung in einem für diese Arbeit realisierbaren Zeitrahmen verbleibt. Sowohl die maximale Anzahl von Epochen, sofern kein Early Stopping einsetzt, als auch die Gesamtanzahl der für die Finalisierung selektierten Modelle musste auf Grundlage einer Schätzung eines akzeptablen Zeitaufwandes zum Training erfolgen.

Daher wurden die Trainingsverläufe zusätzlich statistisch evaluiert, um eine Schätzung über die Trainingsdauer pro Epoche zu erhalten und diese auf 15 Epochen zu extrapolieren. Das Ergebnis dieser Analyse ist in \Cref{fig:finalization-prediction} dargestellt. Das Histogramm zeigt, dass sich die meisten Modelle im Bereich von ca. 10 Minuten pro Epoche befinden, teils jedoch auch deutlich länger trainieren können, bis zu einer Trainingszeit von ca. 60 Minuten pro Epoche. Die Finalisierung eines solchen Modells würde somit geschätzt 15 Stunden betragen. Die meisten Modelle können in unter 3 Stunden finalisiert werden, sofern davon ausgegangen wird, dass die selektierte Finalisierungsdauer von 15 Epochen nach den vorigen Beobachtungen genügt. Eine weitere Betrachtung, welche bei der Dimensionierung der zu finalisierten Modelle einfloss, besteht darin, dass aus verschiedenen Gründen kein durchgängiges Training der Modelle mithilfe der gewählten Google-Cloud-Instanz möglich ist. Die Instanz wird beispielsweise, um serverseitig Ressourcen einzusammeln, ungefähr halbtäglich recycelt. Auch das Training musste für die zwischenzeitliche Unterbrechung so konfiguriert werden, dass die Modelle in Google Drive zwischengespeichert werden, um den verlorenen Fortschritt bei der Terminierung der Instanz zu minimieren. Da das Training nach zwischenzeitlicher Terminierung der Instanz, welche sich auch teils nächtlich und ohne Aufsicht ereignete, stets manuell wieder initialisiert werden musste, konnte keine effektive Trainingsdauer von 24 Stunden pro Tag erreicht werden. Technisch wäre es außerdem möglich gewesen, die SHL-Datensätze über Google Drive einzubinden, dazu hätte jedoch ein weiteres kostenpflichtiges Abonnement abgeschlossen werden müssen, um den Speicherplatz in Google Drive zu erhöhen. Nach Evaluation des Kosten-Nutzen-Verhältnisses wurde stattdessen der SHL-Datensatz stets vor Beginn des Trainings auf den Festspeicher der Cloud-Instanz heruntergeladen. Dies resultierte in einer weiteren täglichen Wartezeit von ca. einer Stunde, welche die effektive Trainingsdauer weiter reduziert. Mit Berücksichtigung dieser Faktoren wurden schließlich die 10 besten Modelle der Rastersuche selektiert und Finalisiert.

\subsection{Resultierende Modelle}

Die aus der Rastersuche resultierenden Modellkonfigurationen sind in \Cref{tab:gridsearch-results} gezeigt. Von 10 Modellen bestehen drei Modelle aus 2D-Convolution-Schichten und einer vorgeschalteten \acrshort{stft}. Die überwiegende Mehrheit der Top-10-Modelle klassifiziert direkte Zeitliniendaten mithilfe von 1D-Convolution-Schichten analog zur initial getesteten Basisarchitektur. Außerdem überrepräsentiert ist der Adam-Optimizer, der bereits in der prototypischen Analyse der Basisarchitekturen die besten Ergebnisse erzielte, sowie die ResNetV1-Bauweise.

\input{sections/evaluation/gridsearch-results}

Die Tiefe der Modelle scheint analog zu den Betrachtungen aus \Cref{ch:verwandte-arbeiten} auch von Bedeutung zu sein. Keines der Top-10-Modelle besitzt nur einen Block von Convolution-Schichten. Die meisten der Top-10-Modelle besitzt 2 Convolution-Blöcke, das tiefste Modell mit der ID \texttt{6af8a1} und der insgesamt größtmöglichen Konfiguration erzielt die zweitbeste Validation-Accuracy. Zu beobachten ist außerdem, dass die in \Cref{fig:gridsearch-results} mit der größten Kreuzentropie auf den Validierungsdaten markierten Top-10-Modelle (\texttt{e6c367}, \texttt{6af8a1}, \texttt{752315}) auch gleichzeitig die Modelle mit einer erhöhten Tiefe sind. Die Modelle \texttt{752315}, \texttt{e6c367} zeigen neben \texttt{35d171} gleichzeitig das verhältnismäßig größte Overfitting auf den Trainingsdaten.

\section{Ergebnisse der Top-10-Finalisierung}

\begin{table}[h]
  \input{sections/evaluation/finalization-results}
  \caption{Quantitative Ergebnisse der Top-10-Finalisierung.}\label{tab:finalization-results}
\end{table}

Die Resultate der Top-10-Finalisierung sind in \Cref{tab:finalization-results} aufgelistet. Die Ergebnisse sind mit der Rangfolge aus der Rastersuche versehen. Einige der selektierten Modelle konnten durch Finalisierung ein signifikant besseres Ergebnis erreichen, als es die initiale Rangfolge aus der Rastersuche zunächst vermuten lässt. Das Modell \texttt{4ad2e7} auf Rang 4 der Rastersuche konnte das Modell \texttt{968ca3} auf Rang 1 in der Validation-Accuracy übertreffen. Gleichzeitig besitzt das Modell \texttt{4ad2e7} auch die kleinste Speichergröße im Festspeicher von 3,96 MB. Insgesamt lässt sich beobachten, dass die größeren Modelle nicht zwangsweise eine bessere Accuracy erreichen können, als die getesteten kleineren Modellvarianten. Die Rastersuche konnte somit mehrere Modellkonfigurationen identifizieren, welche vergleichsweise klein sind, gleichzeitig jedoch keinen signifikanten Tradeoff in der Validation-Accuracy aufweisen und somit bereits von diesem Standpunkt aus als besonders attraktiv für das Deployment auf Smartphones zu betrachten sind.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1072 567]{shl/finalization-results-accuracy.pdf}
  \caption{Entwicklung der Accuracy bei der Top-10-Finalisierung.}\label{fig:finalization-results-accuracy}
\end{figure}

Ebenfalls in \Cref{tab:finalization-results} hervorgehoben und anhand der Finalisierungsverläufe in \Cref{fig:finalization-results-accuracy} sichtbar ist das Overfitting der Modelle. Während des Trainings erreichten alle Modelle ein Plateau in der Validation-Accuracy. Ein auf Overfitting hindeutender Abfall der Verläufe konnte jedoch nicht beobachtet werden. Der Finalisierungsprozess wurde bei 3 von 10 Modellen durch Early Stopping terminiert. Dass die Kurven der Train-Accuracy im Vergleich mit der Validation-Accuracy voneinander abweichen, ist ein erwartetes Verhalten, bei einigen Modellen jedoch weichen die beiden Kurven stärker ab als bei anderen. Im Vergleich zeigen die getesteten Modelle auf STFT-Eingaben hierbei eine stärkere Differenz als die Modelle auf reinen Eingabedaten. Es wurden jedoch nur wenige STFT-Modelle betrachtet.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1071 567]{shl/finalization-results-loss.pdf}
  \caption{Entwicklung der Kreuzentropie bei der Top-10-Finalisierung.}\label{fig:finalization-results-loss}
\end{figure}

Neben STFT-Modellen zeigen auch die getesteten größeren Modellkonfigurationen eine erhöhte Abweichung zwischen der Train-Accuracy und der Validation-Accuracy. Dies deutet auf Overfitting hin, welches aus der Modellgröße resultiert. Die bei der Rastersuche beobachtete Entwicklung der Kreuzentropie bildet sich auch bei der Finalisierung erneut ab. Dies ist in \Cref{fig:finalization-results-loss} gezeigt. Auch hier zeigen die vergleichsweise großen Modelle eine stärkere Differenz zwischen Trainingsdaten und Validierungsdaten und somit eine erhöhte Unsicherheit beim Test der Generalisierungsfähigkeit. Die im Rahmen der Finalisierung einbezogenen größeren Modelle zeigen also nicht nur ein verstärktes Overfitting und eine stärkere Unsicherheit anhand der Kreuzentropie, sondern konnten auch in der Accuracy keine signifikant besseren Ergebnisse erzielen als vergleichsweise kleine Modellkonfigurationen. Ein Tradeoff allein anhand der Modellgröße konnte somit nicht beobachtet werden.

\section{Ergebnisse der Tradeoff-Analyse}

Die finalisierten Modelle wurden einem Profiling unterzogen, wie es in \Cref{ch:konzept} konzeptuell vorgestellt wurde. Die Ergebnisse sind in \Cref{tab:tradeoff-analyse-ergebnisse} aufgeschlüsselt. Die zwei Tradeoff-Parameter der Validation-Accuracy und der Modellgröße im Festspeicher wurden als Ergebnisse der Finalisierung übertragen. Mit einbezogen sind außerdem Analysewerte, die eine Auskunft über die auf den jeweiligen Hardwarebeschleuniger übertragenen Operationen geben. Dieser Wert ist mitunter sehr unterschiedlich und hängt davon ab, wieviele der verwendeten Operationen innerhalb des ResNet-Modells in Abhängigkeit von der Modellkonfiguration unterstützt werden. Einige Modelle konnten somit nur zu einem geringen Teil auf der GPU oder der \acrshort{ane} ausgeführt werden. Das bezüglich des relativen Anteils der ausgelagerten Operationen am besten auslagerbare Modell ist auch das größte getestete Modell mit 168.53 MB Speicherbedarf auf dem Festwertspeicher. Außerdem konnte beobachtet werden, dass mehr Operationen auf den \acrshort{ane}-Koprozessor ausgelagert werden können als auf die interne GPU.

\subsection{Arbeitsspeicherverbrauch}

Der Arbeitsspeicherverbrauch zeigt, dass die Modelle, zumindest bei einer nicht assistierten Ausführung auf der CPU, keine signifikant unterschiedlichen Anforderungen hieran aufweisen. Die Modelle werden offenbar durch das verwendete Framework TensorFlow Lite speichereffizient geladen und erreichen somit nur marginale Arbeitsspeicherverbräuche. Ein leicht erhöhter Arbeitsspeicherverbrauch ist bei der Ausführung auf der \acrshort{ane} messbar, eine Korrelation mit der Anzahl ausgelagerter Operationen ist nicht erkennbar. Vielmehr scheint der Arbeitsspeicherverbrauch anzusteigen, wenn eine insgesamt große Anzahl von Operationen im Modell vorhanden ist, diese jedoch nur zu einem untergeordneten Teil auf die \acrshort{ane} ausgelagert werden können. Dies lässt vermuten, dass der kausale Zusammenhang hinter der geringfügigen Erhöhung des Arbeitsspeicherverbrauchs durch die Kommunikation zwischen den Operationen auf unterschiedlichen Hardwarekomponenten und den daraus entstehenden Overhead bedingt sein kann. Auch der Arbeitsspeicherverbrauch auf der GPU zeigt einen betrachtenswerten Zusammenhang. Da bei der Messung des Speicherverbrauchs auch der VRAM der GPU mit gemessen wurde, zeigt sich bei dem Modell \texttt{752315} mit den meisten ausgelagerten Operationen ein systemübergreifender Speicherverbrauch von 347 MB und somit der einzige Speicherverbrauch, der bezüglich der Systemlimitationen als erhöht interpretiert werden kann. Plattformübergreifend zeigt sich, dass der beobachtete Speicherverbrauch nur marginal zwischen den getesteten Plattformen abweicht, sofern dies beurteilt werden kann. Unbekannt ist beispielsweise, ob neuere Versionen der \acrshort{ane} andere Messwerte bedingen, beispielsweise durch eine verbesserte Verfügbarkeit von auszulagernden Operationen. Selbst, wenn dies der Fall ist, können ältere Geräte jedoch nicht vernachlässigt werden, als Gegenstand der Betrachtung möglicher zu unterstützender Geräte, auf der eine App später ausgeführt wird.

\begin{landscape}
\begin{table}[]
  \resizebox{\textwidth}{!}{%
    \input{sections/evaluation/tradeoff-results}
    \caption{Ergebnisse der Tradeoff-Analyse.}\label{tab:tradeoff-analyse-ergebnisse}
  }
\end{table}
\end{landscape}

\subsection{Inferenzzeit}

Einen weiteren zentralen Analyseaspekt stellen die Verhältnisse zur Inferenzzeit dar. Durch die Berechnung des Korellationskoeffizienten $R$ zwischen den Ressourcenparametern lassen sich die Verhältnisse quantifizieren. Während die Modellgröße mit der Anzahl der Operationen positiv korreliert ($R\approx0.93$), korreliert auch die Inferenzzeit analog zur Intuition mit der Anzahl von Operationen, der Median der Korrelationen für alle Geräte und Hardwarebeschleuniger liegt bei $R\approx0.87$. Das iPhone XS inferiert im Schnitt 55\% schneller auf der CPU und 60\% schneller auf der GPU als das iPhone SE. Die \acrshort{ane} konnte nur bei zwei Modellen einen nennenswerten Speedup erreichen. Den besten Speedup erreichte die \acrshort{ane} auf dem Modell \texttt{752315}, welches fast alle Operationen auf der \acrshort{ane} ausführen kann. Auf anderen Modellen wiederum war die \acrshort{ane} bis zu 260\% langsamer als die Ausführung auf der CPU. Während die Inferenzzeiten bei der Nutzung der \acrshort{ane} stark von der CPU abweichen, ergeben sich bei der Nutzung der GPU keine signfikanten Änderungen in der Inferenzzeit. Lediglich eine Modellkonfiguration konnte von der Ausführung auf der GPU einen Speedup von 46\% erreichen, der hierbei gemessene Speedup besteht jedoch nur auf dem iPhone SE und nicht auf dem iPhone XS.

\subsection{CPU-Auslastung}

Ein eng mit der Inferenzzeit verknüpfter Ressourcenparameter bildet sich in der CPU-Auslastung. Wird das Modell ausschließlich auf der CPU ausgeführt, korreliert die CPU-Auslastung mit der Inferenzzeit sehr stark positiv mit $R_{iPhoneSE}\approx0.98$ und $R_{iPhoneXS}\approx0.995$. Bei der Auslagerung der Modelle auf Hardwarebeschleuniger verzerrt sich dieses Bild, je nachdem, in welchem Anteil die Modelle noch auf der CPU ausgeführt werden. Das bereits häufiger als Beispiel genannte Modell \texttt{752315} kann die CPU-Auslastung um bis zu 227\% reduzieren. Die Last wird in diesem Fall jedoch nur auf die GPU verlagert. Ob dies in einem Vorteil im Energieverbrauch resultiert, muss also separat geprüft werden. Werden Operationen auf die \acrshort{ane} ausgelagert, so zeigt sich ein ähnliches Bild wie bei der Inferenzzeit. Manche Modelle wie \texttt{752315} können die CPU-Auslastung reduzieren, andere wiederum erfahren einen gegenteiligen Effekt. Beim Modell \texttt{6af8a1} erhöht sich die CPU-Auslastung sogar auf mehr als das Doppelte der Auslastung bei reiner Ausführung auf der CPU. Ursache ist auch hier vermutlich der bei der Kommunikation der Layer-Ergebnisse entstehende Overhead. Analog zur Inferenzzeit bietet der \acrshort{ane}-Koprozessor also nur dann einen Vorteil, wenn das Modell zu einem großen Teil kompatibel mit den hierdurch zur Verfügung gestellten Berechnungsoperatoren ist.

\subsection{Energieverbrauch}

Als verbleibender Ressourcenparameter wurde der Energieverbrauch gemessen. Die Messung konnte allerdings nicht einwandfrei durchgeführt werden. Unter Xcode 12 konnten im Unterschied zum iPhone SE trotz verschiedener Versuche zur Problembehandlung keine Energy Logs (in \acrshort{aeu}) des iPhone XS aufgezeichnet werden. Die Schnittstelle zeigte lediglich den Hinweis \enquote{No Data}, während alle anderen Hardwareparameter problemlos aufgezeichnet werden konnten. Die Instruments-Schnittstelle zur Aufzeichnung von Energy Logs wurde darüber hinaus in der aktuellsten Beta von Xcode komplett entfernt, sodass die genauen \acrshort{aeu}-Werte nur für das iPhone SE bekannt sind. Die Betrachtung des Energieverbrauchs insbesondere der \acrshort{ane} auf dem iPhone XS entfällt somit, vermutlich liegt dieser analog zur Inferenzzeit und zur CPU-Auslastung jedoch im Schnitt höher als die Energieverbräuche auf CPU und GPU. Der Energieverbrauch bildet insgesamt keine Messwerte ab, welche als unvorhergesehen interpretiert werden können. Die Messwerte orientieren sich vielmehr an den bereits beobachteten Relationen zur Inferenzzeit und zum CPU-Verbrauch, analog zu den Beobachtungen aus \cite{brownlee_exploring_2021}. Die Auslagerung der Modelle auf die GPU ist außerdem analog zu den Messwerten weitestgehend unerheblich für den Gesamtenergieverbrauch.

Die beobachteten geringfügigen Abweichungen zwischen CPU und GPU sind nicht signifikant und können rein durch Messungenaugkeiten entstanden sein. Beim T-Test\footnote{\url{https://de.wikipedia.org/wiki/Zweistichproben-t-Test} (Abgerufen am 17.9.2021)} der statistischen Signfikanz ist $p \geq 0.05$. Die gemessenen \acrshort{aeu}-Werte sind jedoch mit Vorsicht zu interpretieren, denn es ist nicht bekannt, ob diese einen linearen, logarithmischen oder einen anderweitigen Zusammenhang abbilden. Eine Messung in mW wäre an dieser Stelle besser, diese ist jedoch auf iOS-Geräten aus technischen Gründen nicht möglich, da hierfür keine Schnittstelle existiert oder diese entfernt wurde\footnote{\url{https://gist.github.com/marcoarment/92d58159943240d6ba47} (Abgerufen am 17.9.2021)}. Um die gemessenen \acrshort{aeu}-Werte besser interpretieren zu können, wurden unter anderem auch die qualitativen Indikatoren von Xcode genutzt, genauer die Energy-Gauge und die abgebildeten prozentualen Verhältnisse des CPU- und GPU-Verbrauchs in Relation zum Komponentenverbrauch des Displays. Die Modelle zeigen hierbei im Schnitt einen wesentlich geringeren CPU- und GPU-Verbrauch als das Display des Smartphones, in der Größenordnung 1 zu 4. Wird GNSS aktiviert, so kann eine Verdopplung der gesamten Komponentenauslastung festgestellt werden. Inwiefern der Komponentenverbrauch jedoch mit dem tatsächlichen Energieverbrauch der Komponenten zusammenhängt, ist weder in Xcode direkt ablesbar, noch wird dies in der Dokumentation von Xcode beschrieben.

\subsection{Tradeoff und Selektion des Pareto-optimalen Modells}

Die betrachteten Modelle zeigen, dass die Modellgröße in den meisten Fällen über Arbeitsspeicherverbrauch, Inferenzzeit, CPU-Verbrauch und Energieverbrauch bestimmt. Sind die Modelle gut auf \acrshort{ane} oder GPU auszulagern, so werden teils weniger Ressourcen verbraucht, eine signifikante Reduktion des Energieverbrauchs konnte jedoch nicht betrachtet werden. Die Größe des Modells scheint bei den betrachteten Konfigurationen jedoch kein Garant für eine gute Ergebnisqualität zu sein. Stattdessen konnte das Modell \texttt{4ad2e7} gefunden werden, welches im Vergleich die beste Accuracy bietet und gleichzeitig den kleinsten Speicherabdruck besitzt. Hierdurch verzeichnet es im Vergleich zu anderen Modellen eine besonders kurze Inferenzzeit sowie einen geringen CPU- und Energieverbrauch. Der Arbeitsspeicherverbrauch des Modells ist nur geringfügig unterschiedlich zu den anderen betrachteten Modellen. Das Modell \texttt{4ad2e7} geht somit als klarer Sieger im Tradeoff-Vergleich der Top-10-Modelle hervor. Im Vergleich zu den anderen Modellkonfigurationen kann das Modell \texttt{4ad2e7} als Pareto-optimal betrachtet werden. Eine weitere Priorisierung oder Metamodellierung anhand der Ressourcenparameter ist somit nicht notwendig, da das Modell \texttt{4ad2e7} in fast allen Ressourcendimensionen die beste Variante darstellt.

\section{Ergebnisse der Optimierung}

Das Modell \texttt{4ad2e7} wurde analog zum finalen Konzeptschritt einer Optimierung unterzogen, um die daraus entstandenen Modellvarianten erneut durch Profiling zu testen. Die ermittelten Tradeoff-Parameter sind in \Cref{tab:optimization-tradeoff-results} dargestellt.

\begin{table}[h]
\input{sections/evaluation/optimization-tradeoff-results}
\caption{Tradeoff-Parameter der Modelloptimierung.}\label{tab:optimization-tradeoff-results}
\end{table}

\subsection{Tiefenweise separierbare Convolution-Schichten}

Die in der Modellkonfiguration vorhandenen Convolution-Schichten sollten analog zur MobileNet-Architektur aus \cite{howard_mobilenets_2017} mit tiefenweise separierbaren Convolution-Schichten substituiert werden, um die Anzahl der notwendigen Operationen zu reduzieren und die Inferenzzeit zu verbessern. Dies konnte nur begrenzt implementiert werden. Die verwendete Keras-Bibliothek bietet zurzeit im \textit{Stable-Release} tiefenweise separierbare Convolution-Schichten nur für zweidimensionale Convolution-Architekturen an\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D} (Abgerufen am 18.9.2021)}. Stattdessen wurden einfache separierbare Schichten verwendet, welche auch für eindimensionale Convolution-Architekturen verfügbar sind\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/SeparableConv1D} (Abgerufen am 18.9.2021)}. Das Modell erreicht in Relation zu anderen Optimierungsvarianten keine signifikant schnellere Inferenzzeit und außerdem keinen geringeren CPU- und Energieverbrauch. Das Modell ist nur geringfügig kleiner als ohne Optimierung, der Arbeitsspeicherverbrauch ist auch reduziert. Im Tradeoff zur um mehr als 2\% reduzierten Accuracy scheint diese Strategie jedoch keine hinreichende Verbesserung zu bieten.

\subsection{Pruning, XNNPACK und PQAT}

Sowohl die Ausgangskonfiguration, als auch die mit separierbaren Schichten ausgestattete Modellkonfiguration wurden einem Pruning unterzogen. Hierbei wurde über 15 Epochen mit einer reduzierten Learning Rate trainiert und die Sparsity der Modelle schrittweise über die Epochen von 25\% auf 75\% polynomiell angehoben\footnote{\url{https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PolynomialDecay?hl=de} (Abgerufen am 18.9.2021)}. Hierbei konnte das Modell ohne separierbare Schichten die initiale Accuracy ohne Verlust beibehalten, während die Modellgröße auf 25\% reduziert wurde. Durch Einfügen von beim Pruning benötigten Zwischenschichten erhöht sich die Anzahl von Operationen innerhalb des Modells leicht auf 108. Die anderweitigen Ressourcenparameter zeigen nur marginale Abweichungen, lediglich der Energieverbrauch ist geringfügig erhöht und die \acrshort{ane} des iPhone XS kann das Modell wesentlich schneller ausführen. Insgesamt kann dieses Modell somit nur mit Bezug auf die Modellgröße und bei einer Ausführung auf Geräten mit \acrshort{ane} überzeugen. Das Modell mit separierbaren Schichten konnte bei demselben Verfahren überraschenderweise eine höhere Accuracy als alle anderen getesteten Modellvarianten erreichen, obwohl das ausschließlich durch separierbare Schichten substituierte Modell eine schlechtere Accuracy erreichte. Dieses Modell ist ebenfalls 25\% kleiner als das Ausgangsmodell und kann wesentlich schneller auf der \acrshort{ane} inferiert werden. Der Energieverbrauch dieses Modells ist nur 0.5 \acrshort{aeu} und somit insignifikant höher als der des Ausgangsmodells, wenn die CPU genutzt wird. Dies bietet sich an, da das Modell nur zu einem kleinen Anteil auf die GPU ausgelagert werden kann und somit keine messbaren Vorteile in den Ressourcenparametern beobachtet werden konnten.

Das Pruning konnte nicht, wie im Konzept vorgeschlagen, mithilfe von effizienten Inferenzoperatoren wie XNNPACK durchgeführt werden. Unter anderem deshalb stellten sich vermutlich auch keine signifikanten Vorteile im Inferenzverhalten ein. Die (teils experimentellen) Schnittstellen hierfür werden nach aktuellem Stand zwar durch das TensorFlow Model Optimization Toolkit bereitgestellt, bei Versuch der Überführung des Modells in eine durch XNNPACK optimierte Repräsentation scheiterte dies jedoch durch die Limitation der verfügbaren Operatoren. XNNPACK stellt nur eine beschränkte Anzahl von möglichen Operatoren zur Verfügung, welche teils auch Convolution-Operationen umfassen. Diese sind jedoch mit konkreten Dimensionierungen versehen, welche einerseits erneut nur für 2D-Convolution-Operationen verfügbar sind, und andererseits bereits Kernel- und Filtergrößen vorgeben, die nicht im Basismodell vorgesehen sind. Um das Modell in eine für XNNPACK kompatible Version zu transformieren sind somit weitere Aufwände notwendig. Ein ähnliches Problem stellte sich heraus bei der Durchführung der Quantisierung unter Berücksichtigung der zuvor durch Pruning getrennten Verbindungen. Das PQAT-Verfahren wird als Schnittstelle ebenfalls vom TensorFlow Model Optimization Toolkit bereitgestellt, diese unterliegt jedoch ebenfalls technischen Limitationen. Das PQAT-Verfahren und die damit verbundene Quantisierung ist nach aktuellem Stand nicht für 1D-Convolution-Operationen unterstützt. Eine Lösung hierfür wird im Moment erarbeitet\footnote{\url{https://github.com/tensorflow/model-optimization/issues/362} (Abgerufen am 18.9.2021)}. Da daher keine Quantisierung durchgeführt werden konnte, ist diese in \Cref{tab:optimization-tradeoff-results} auch nicht mit aufgeführt.

\subsection{Beschränkung des Anwendungsfalls}

Zuletzt soll untersucht werden, inwiefern sich die Beschränkung des Anwendungsfalls auf die erreichbare Accuracy auswirkt. Da bereits identifiziert werden konnte, dass insbesondere die intuitiv ähnlichsten Verkehrsmittel miteinander am häufigsten verwechselt werden, kann diese Erkenntnis in einer Fusion der Verkehrsmittelklassen einfließen.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 995 295]{shl/optimization-class-merge.pdf}
  \caption[Verhalten des durch separierbare Schichten und Pruning optimierten ResNets auf dem Validierungsdatensatz.]{Verhalten des durch separierbare Schichten und Pruning optimierten ResNets auf dem Validierungsdatensatz. Links: Einfluss der Klassen auf die Gesamt-Accuracy. Mitte: Konfusionsmatrix des optimierten Modells. Rechts: Erreichbare Accuracy durch Limitation des Anwendungsfalls.}\label{fig:optimization-class-merge}
\end{figure}

Die statistischen Resultate des Modells, welches dem Pruning auf Basis separierbarer Convolution-Schichten unterzogen wurde, sind in \Cref{fig:optimization-class-merge} gezeigt. Am besten klassifiziert das Modell die Klassen \enquote{Run}, gefolgt von \enquote{Bike}, \enquote{Still} und \enquote{Walking}. Während \enquote{Car} ebenfalls mit einer Accuracy von über 90\% klassifiziert werden kann, werden insbesondere die Verkehrsmittel \enquote{Train} und \enquote{Subway} häufig miteinander verwechselt. An dieser Stelle bietet sich eine empirische Fusion der Klassen an, die sich wie folgt zusammensetzen kann. Die Klassen \enquote{Train} und \enquote{Subway} werden zu \enquote{Rail} verbunden. Die Klassen \enquote{Bus} und \enquote{Car} werden zu \enquote{Road} symbolisch für motorisierte Straßenfahrzeuge verbunden. Außerdem können die Klassen \enquote{Run} und \enquote{Walking} zur Klasse \enquote{Foot} verbunden werden, auch wenn die beiden Klassen bereits sehr gut unterschieden werden können. In \Cref{fig:optimization-class-merge} ist die resultierende Konfusionsmatrix auf demselben Modell gezeigt. Werden nur diese fünf vorgeschlagenen Verkehrsmittel bei der Klassifikation benötigt, so lässt sich auf dem Validierungsdatensatz eine durchschnittliche Accuracy von 93.56\% erreichen.

\subsection{Postprocessing durch Glättung}

Dieses Ergebnis kann anschließend weiter durch Postprocessing wie in \cite{werner_kontinuierliche_2020} oder \cite{stojanov_continuous_2020} verbessert werden. Insbesondere bei den verbleibenden Unsicherheiten des Modells, wie sie bei der Unterscheidung der Klassen \enquote{Train} und \enquote{Subway} zu beobachten ist, bietet sich somit weiteres Verbesserungspotenzial. Der Test des Postprocessing ist auf dem SHL-Datensatz jedoch nicht ohne Probleme realisierbar, da die Samples innerhalb des Datensatzes diskontinuierlich vorliegen. Möglich wäre ein Test auf einem anderen Datensatz, welcher nicht zu dem Zweck gemischt wurde, um Verbesserungen durch Sample-übergreifende Betrachtungen zu unterbinden. Da das Postprocessing und dessen Potenzial bereits ergiebig durch \cite{werner_kontinuierliche_2020} und \cite{stojanov_continuous_2020} analysiert wurde, wird an dieser Stelle lediglich darauf verwiesen, dass das zugrundeliegende Glättungsverfahren ohne großen Implementationsaufwand in das Framework der bestehenden mobilen Pipeline programmatisch integriert werden kann.

\section{Limitationen und Risiken für die Validität}

In den vorigen Sektionen wurde evaluiert, inwiefern sich ein Tradeoff bei der Messung der Ressourcenparameter verschiedener Modelle zur Verkehrsmittelerkennung feststellen lässt. Außerdem wurde ein Modell gefunden, welche anhand der vorliegenden Daten als Pareto-optimal betrachtet werden kann.

\subsection{Beschränkung des Suchraums}

Eine wichtige Limitation dieser Betrachtung ist die Beschränkung des Suchraums. In dieser Arbeit wurde der Suchraum auf den Movebis-Modellen aufgebaut und durch weitere traditionelle sowie neuartige Modelle ergänzt. Hierzu gehören jeweils verschiedene Möglichkeiten des Feature-Engineerings. In Angesicht der großen Bandbreite der möglichen Ansätze konnten diese nur über einen begrenzten Zeitraum getestet werden. Bei der sich hieran anschließenden Selektion des Basismodells steht die Vergleichbarkeit der betrachteten Modelle und deren Portierbarkeit über eine einheitliche Datenverarbeitungspipeline im Vordergrund. Die Selektion eines Basismodells schließt jedoch ein, dass andere Basismodelle in der Tradeoff-Analyse nicht mit betrachtet werden konnten. Hierdurch und auch durch die weiteren Selektionsschritte im vorgestellten Vorgehensmodell ist es wahrscheinlich, dass eine oder mehrere andere Kombinationen von Vorverarbeitung und Klassifikation existieren, welche übergreifend einen noch besseren Tradeoff bieten. Die Schwierigkeit besteht hierbei jedoch eher darin, diese Kombinationen zu finden.

Das gefundene Modell ist somit zwar anhand der Daten als Pareto-optimal zu betrachten, durch Ausdehnung des Suchraums ist es jedoch wahrscheinlich, dass ein noch besseres Modell gefunden werden kann. Wäre beispielsweise die Finalisierung nur über die Top 3 Modelle der Rastersuche durchgeführt worden, so hätte das schließlich in der Top-10-Finalisierung beste Modell nicht gefunden werden können. Dieselbe Beobachtung gilt vermutlich auch für die Top 10 Modelle. Auch die Rastersuche selbst unterliegt diesem Problem. Relevant ist hierbei, wie sich die Konfidenz über die Pareto-Optimalität des besten Modells durch Vergrößerung des Suchraums verändert. Da die Selektion der Basisarchitektur, die Rastersuche und auch die Finalisierung auf empirischen Annahmen beruhen, welche zwar falsch sein können, aber dennoch zielgerichtete Heuristiken beinhalten, nimmt die Wahrscheinlichkeit der Entdeckung besserer Konfigurationen vermutlich mit Vergrößerung des Suchraums ab. Die sich hierdurch konstituierende Konfidenz der statistischen Aussagen beschränkt sich außerdem vor allem im zeitlichen Rahmen, welcher über dieser Arbeit steht. Die Dimensionierung des Suchraums steht dadurch stets in einem Kosten-Nutzen-Verhältnis, wobei sich die Kosten vor allem durch die aufgewendete Trainingszeit und die dabei in Anspruch genommenen Ressourcen beziehen. Während die Kosten gut beispielsweise anhand der Trainingsdauer pro Epoche vorhergesagt werden können, unterliegt der zugewonnene Nutzen einem statistischen Rauschen der Tradeoff-Parameter, welches die Abschätzung der Dimensionierung erschwert.

\subsection{Messungenauigkeiten}

Neben dem statistischen Rauschen, welches insbesondere durch die hohe Variabilität der Modellkonfigurationen induziert wird, sind auch Messungenauigkeiten mit in die Betrachtung der Ergebnisse mit einzubeziehen.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 910 262]{shl/measurement-skew.pdf}
  \caption{Beobachtete Messungenauigkeiten bei der Messung der Tradeoff-Parameter.}\label{fig:measurement-skew}
\end{figure}

Die beobachteten Messungenauigkeiten sind in \Cref{fig:measurement-skew} gezeigt. Die Messung der RAM-Auslastung streut nicht, daher ist sie nicht gezeigt. Bei der Inferenzzeit hingegen ist eine Streuung feststellbar. Diese ist jedoch anhand der Betrachtungen normalverteilt um das arithmetische Mittel der Messungen und so geringfügig, dass sie im multilateralen Vergleich zwischen den Modellen an Bedeutung verliert. Die Messung der CPU-Auslastung rauscht in größerem Maße, in \Cref{fig:measurement-skew} ist das Rauschen im Histogramm anhand der jeweiligen Hardwarebeschleuniger beispielhaft dargestellt. Dieses Rauschen ist einseitig (erhöht die Last) und kommt daher mutmaßlich durch Hintergrundprozesse der App zustande, welche aufgrund der technischen Limitationen nicht weiter aufgeschlüsselt werden können. Das arithmetische Mittel verschiebt sich hierdurch vom Zentrum der im Histogramm dargestellten Verteilung. Diese Verschiebung ist jedoch in allen Messungen in annähernd gleicher Form enthalten, sodass die Messungen über die CPU-Auslastung dennoch vergleichbar bleiben.

\subsection{Abhängigkeit von Threads}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 726 258]{shl/threads.pdf}
  \caption{Beobachtete Abhängigkeit der CPU-Last von den verwendeten Threads.}\label{fig:threads}
\end{figure}

Ein weiterer Faktor, der bei der Messung eine zentrale Rolle spielt, sind die zur Modellinferenz genutzten Threads. Prinzipiell können diese frei konfiguriert werden, in den Messungen wurden diese jedoch auf 1 Thread limitiert, um eine Vergleichbarkeit zu gewährleisten. Die Anzahl Threads wurde auf 1 limitiert, da bei den beobachteten Modellen analog zu \Cref{fig:threads} kein signifikanter Speedup festgestellt werden konnte, der die Erhöhung der CPU-Auslastung rechtfertigt. Dennoch fügt die Konfiguration der Threads eine weitere Dimension hinzu, welche insbesondere auf die Auslastung der berechnenden Ressourcen wirkt. Da nicht alle Modelle auf unterschiedlich vielen Threads getestet wurden, ist dies als weiterer limitierender Faktor zu werten, denn dies bietet theoretisch die Möglichkeit, dass eines der Modelle signifikant bessere Messwerte auf einer bestimmten Konfiguration von Threads erzeugt und somit einen besseren Tradeoff bietet. Die Wahrscheinlichkeit dieses Ereignisses ist jedoch fraglich, da die CPU-Auslastung mit der Anzahl von Threads bei den getesteten Modellen analog zu \Cref{fig:threads} positiv korreliert und somit auch vermutlich den Energieverbrauch als wichtigen Tradeoff-Parameter verschlechtert.

\subsection{Abhängigkeit von der Inferenzrate}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 940 278]{shl/inference-time.pdf}
  \caption{Beobachtete Abhängigkeit der CPU-Auslastung von der Inferenzrate (iPhone XS).}\label{fig:inference-time}
\end{figure}

Analog zur Abhängigkeit von Threads besteht auch eine Abhängigkeit der berechnungsabhängigen Ressourcenparameter wie der CPU-Auslastung von der Inferenzrate. Diese wurde im Rahmen der Tradeoff-Analyse auf 2Hz festgelegt, kann jedoch auch mit einer höheren Granularität durchgeführt werden. In \Cref{fig:inference-time} ist der beobachtete Zusammenhang zwischen Inferenzrate und CPU-Auslastung der unterschiedlichen Optimierungsvarianten des Modells \texttt{4ad2e7} für die Abtastraten 1Hz, 2Hz, 5Hz, 20Hz und 50Hz gezeigt. Für die Raten 1Hz, 2Hz, 5Hz und 20Hz bietet sich ein typisches Bild. Die CPU-Last steigt annähernd linear mit der Inferenzrate. Auf der \acrshort{ane} zeigen sich größere Abweichungen zwischen den Modellen, analog zur Tradeoff-Analyse. Oberhalb von 20Hz flacht die CPU-Auslastung ab, da die von der Pipeline isolierte Messung des Modells synchron durchgeführt wurde. Die Inferenzzeit des Modells (5ms bis 50ms) wird anteilig an der Wartezeit von 20ms (für 50Hz) beim Profiling relevant, die gemessenen Ergebnisse sind nicht mehr aussagekräftig. Für höhere Inferenzraten müsste, wie es bei der Gesamtpipeline implementiert wurde, die Inferenz zur Messung selbst asynchron vom Wartezyklus aufgerufen werden, um eine sichere Inferenzrate von ca. 50Hz zu erreichen. Die Asynchronität fügt jedoch eine zusätzliche Unschärfe in der Messung der CPU-Auslastung hinzu, da diese in ihrer Implementation als Unit-Test stichprobenartig (nach und vor Inferenz) geschieht. Für Inferenzraten von 2Hz ($\frac{1}{500ms}$), wie sie bei der Tradeoff-Analyse eingesetzt wurden, ist dieser Nebeneffekt analog zu \Cref{fig:inference-time} jedoch untergeordnet und die synchronen Messungen genügen von diesem Standpunkt in ihrer Aussagekraft zur multilateralen Abwägung verschiedener Konfigurationen.

\subsection{Abhängigkeit von \acrshort{aeu} als Einheit}

Während die CPU-Auslastung des Prototyps hinreichend messbar ist und maßgeblich neben der Display-Helligkeit und \acrshort{gnss} (falls verwendet) für den Energieverbrauch verantwortlich ist, verbleibt bei der Messung des Energieverbrauchs die Limitation in der Aussagekräftigkeit der \acrshort{aeu}. Wie bereits erläutert bleibt unbekannt, in welcher Form die \acrshort{aeu} auf dem tatsächlichen Energieverbrauch des Akkus abgebildet sind. Eine Möglichkeit in der Analyse besteht darin, die Depletion des Akkus über die Zeit hinweg zu messen. Da bei der Tradeoff-Analyse zahlreiche Modelle verglichen wurden, hätte eine solche Herangehensweise wesentlich mehr Zeit in Anspruch genommen. Hierzu gehört, dass das Gerät vor der experimentellen Durchführung der Messung stets auf den gleichen Akkustand aufzuladen gewesen wäre, da die Energieabgabe des Akkus nicht linear abläuft. Eine solche Analyse könnte weitere Erkenntnisse bieten, die CPU-Auslastung ist jedoch vermutlich bei Fixierung der Display-Helligkeit und Deaktivierung des \acrshort{gnss} ein hinreichender Informationsgeber auch für den Energie-Tradeoff. Eine weitere Annahme ist hierbei, dass \acrshort{aeu} als abstrakte Einheit der Messung robust gegenüber systemeigenen Schwankungen sind und überhaupt einen Vergleich zwischen den Modellen ermöglicht.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 681 262]{shl/energy-apps.pdf}
  \caption{Energieverbrauch der prototypischen App im Vergleich zu anderen Apps (iPhone SE).}\label{fig:energy-apps}
\end{figure}

Um die gemessenen Werte dennoch besser einzuordnen, sind in \Cref{fig:energy-apps} aktuelle Versionen alltäglicher Apps der entwickelten prototypischen App mit dem Modell \texttt{4ad2e7} und 2Hz-Inferenzrate gegenübergestellt. Obwohl der gemessene Energieverbrauch bei 12 bis 13 \acrshort{aeu} und somit über der Hälfte der Skala von 20 \acrshort{aeu} liegt, kann die prototypische App einen der geringsten Energieverbräuche im Vergleich zu anderen Applikationen erreichen. Insbesondere bei der interaktiven Nutzung der Applikationen kann der Energieverbrauch auf bis zu 19 \acrshort{aeu} steigen. Der geringste Energieverbrauch wurde bei Inaktivität innerhalb des Safari-Browsers gemessen. Ein geringerer Energieverbrauch als 10 \acrshort{aeu} konnte in keinem der Fälle gemessen werden. Interessant ist hierbei außerdem, dass sich der Energieverbrauch bei Einsatz des GNSS von 12 bis 13 \acrshort{aeu} auf 14 bis 15 \acrshort{aeu} anhebt, obwohl Xcode während des Profilings eine wesentliche Erhöhung des relativen Komponentenverbrauchs zeigt. Dies weist auf einen komplexeren Zusammenhang zwischen dem Komponentenverbrauch und dem Energieverbrauch hin. Auch hier besteht also offenbar das Problem, dass die Komponentenverbräuche nur begrenzt interpretierbar sind.

\subsection{Abhängigkeit vom Datensatz und Real-World-Performance}

In der Tradeoff-Analyse sind vor allem die Unterschiede zwischen den betrachteten Modellen von Relevanz. Die Ergebnisse sind hierbei vom Datensatz abhängig. Hieran schließt sich die Frage an, ob das als Pareto-optimal erachtete Modell auch bei der Evaluation auf anderen Datensätzen, wie dem Movebis-Datensatz, als bestmögliches Modell hervorgeht. Limitierende Faktoren sind hierbei die Aufzeichnung des SHL-Datensatzes in einem begrenzten geographischen Rahmen mit einer begrenzten Anzahl von Subjekten und Tragepositionen. \cite{grubitzsch_ai-based_2021} beschreiben vor diesem Hintergrund, dass Ergebnisse auf dem SHL-Datensatz nur eine begrenzte Aussagekraft über die tatsächliche Generalisierungsfähigkeit auf unbekannte Daten (vor allem aus anderen Regionen) haben. Anhand der prototypischen App konnten bereits einzelne Verkehrsmittel stichprobenartig getestet werden, beispielsweise wird die Klasse \enquote{Car} im betrachteten Testszenario hinreichend gut erkannt, mit kurzzeitigen Schwankungen zur Klasse \enquote{Bus}. Auch die Klassen \enquote{Still}, \enquote{Run} und \enquote{Walking} konnten hinreichend und ohne starke Fluktuationen zwischen den Klassen klassifiziert werden. Da es sich hierbei jedoch um eine subjektive Wahrnehmung der Ergebnisqualität handelt, bieten sich weitere Analysen zur Real-World-Performance an. Beispielsweise kann das erhaltene Modell im Anschluss an diese Arbeit auf einem weiteren unbekannten Datensatz wie dem Movebis-Datensatz getestet werden, um die Real-World-Performance weiter bewerten und das ermittelte Modell weiter zu verbessern. Dies ist jedoch weitestgehend unabhängig von der zentralen Analyse des Tradeoffs, welche dieser Arbeit zugrunde liegt.

\section{Vergleich mit Movebis-Ansätzen}

\section{Zusammenfassung}
