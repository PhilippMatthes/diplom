\chapter{Evaluation}\label{ch:evaluation}

\section{Ergebnisse der Hyperband-Rastersuche}

Als Teil des Konzeptes wurden bereits verschiedene traditionelle Modelle und Deep-Learning-Modelle untersucht, um zu einem Basismodell zu gelangen, welches durch Variation der Hyperparameter einer Rastersuche unterzogen werden kann, und um schließlich die Tradeoff-Parameter hierauf zu evaluieren. Als Basisarchitektur wurde das ResNet als Erweiterung des in \cite{stojanov_continuous_2020} vorgestellten \acrshort{cnn}s gewählt. Die in \Cref{sec:metamodell-optimierung} definierten Hyperparameterdimensionen wurden implementiert und anschließend einer Rastersuche unterzogen.

\subsection{Trainingsverläufe}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1143 351]{shl/gridsearch-results.pdf}
  \caption{Trainingsverläufe der Hyperband-Rastersuche.}\label{fig:gridsearch-results}
\end{figure}

Die Trainingsverläufe der Hyperband-Rastersuche sind in \Cref{fig:gridsearch-results} gezeigt. Hierbei wird noch einmal deutlich, dass die Modelle im Rahmen des Hyperband-Algorithmus nicht über die gleiche Anzahl von Epochen trainiert wurden. Die Anzahl von Epochen wurde vom Hyperband-Orakel schrittweise selektiv erhöht. Bereits beim Training der verschiedenen Modellvariationen auf dem \acrshort{shl}-Datensatz konnten einige der Modelle die zuvor erreichte Validation-Accuracy von $78,94\%$ um einige Prozentpunkte übertreffen. Durch die Hyperband-Rastersuche konnten somit mehrere Modellvariationen mit besseren Ergebnisqualitäten als die der Basisarchitektur gefunden werden. Die Train-Accuracy liegt häufig höher, das diesbezüglich beste Modell erreichte hierbei über 95\%.

Außerdem lässt sich beobachten, dass die Kreuzentropie als Verlust auf den Validierungsdaten teils stark ansteigt, obwohl die Validation-Accuracy ebenfalls weiterhin ansteigt und nur selten zwischenzeitlich stärker abfällt. Diese Beobachtung lässt sich erklären durch eine stärkere Unsicherheit zwischen den Output-Neuronen in der letzten Dense Schicht mit Softmax-Aktivierungsfunktion. Während die relative Unsicherheit der Output-Neuronen in die Berechnung der kategorischen Kreuzentropie mit einfließt, zählt für die Berechnung der Accuracy nur das am stärksten aktivierte Neuron. Die Validation-Accuracy wird als besser vergleichbares Sortierungskriterium für die trainierten Ansätze selektiert. Zu vernachlässigen ist die teils hohe Unsicherheit der Output-Neuronen jedoch nicht, denn dies kann zu einer stärkeren Fluktuation der Klassen bei einer Echtzeitanwendung führen. Dieses Problem kann mitigiert werden, indem eine der Postprocessing-Techniken aus \cite{werner_kontinuierliche_2020} oder \cite{stojanov_continuous_2020} eingesetzt wird. Trotzdem sollte die Unsicherheit der Modelle mit in die weiteren Auswertungen mit einfließen, daher wurden die größten vier Kreuzentropien in \Cref{fig:gridsearch-results} mit deren Identifikationsnummer hervorgehoben.

\subsection{Analyse der Trainingsdauer zur Dimensionierung der Finalisierung}

\begin{figure}[h]
  \includegraphics[width=0.5\linewidth, bb=0 0 362 290]{shl/gridsearch-finalization-prediction.pdf}
  \caption{Trainings- und Finalisierungszeiten der Modelle.}\label{fig:finalization-prediction}
\end{figure}

Die Hyperband-Rastersuche wurde über einen Zeitraum von 2 Wochen durchgeführt. Als Ergebnis resultiert eine Rangfolge von insgesamt 28 der möglichen ResNet-Konfigurationen. Um zu entscheiden, wie viele der getesteten Hyperparameter-Konfigurationen insgesamt und über wie viele Epochen diese finalisiert werden sollen, wurden das Trainingsverhalten der Modelle statistisch analysiert. Da analog zu \Cref{fig:gridsearch-results} der Trainingsfortschritt (nach Validation-Accuracy) der über 10 oder mehr Epochen\footnote{Eine Epoche beinhaltet ca. 800.000 Samples aus dem \acrshort{shl}-Datensatz.} trainierten Modelle zumeist nach 5 bis 10 Epochen stagnierte, wurde eine maximale Trainingsdauer zur Finalisierung von 15 Epochen festgelegt. Bei dieser Entscheidung musste zusätzlich berücksichtigt werden, dass die Finalisierung in einem für diese Arbeit realisierbaren Zeitrahmen verbleibt. Sowohl die maximale Anzahl von Epochen, sofern kein Early Stopping einsetzt, als auch die Gesamtanzahl der für die Finalisierung selektierten Modelle musste auf Grundlage einer Schätzung eines akzeptablen Zeitaufwandes zum Training erfolgen.

Daher wurden die Trainingsverläufe zusätzlich statistisch evaluiert, um eine Schätzung über die Trainingsdauer pro Epoche zu erhalten und diese auf 15 Epochen zu extrapolieren. Das Ergebnis dieser Analyse ist in \Cref{fig:finalization-prediction} dargestellt. Das Histogramm zeigt, dass sich die meisten Modelle im Bereich von ca. 10 Minuten pro Epoche befinden, teils jedoch auch deutlich länger trainieren können, bis zu einer Trainingszeit von ca. 60 Minuten pro Epoche. Die Finalisierung eines solchen Modells würde somit geschätzt 15 Stunden betragen. Die meisten Modelle können in unter 3 Stunden finalisiert werden, sofern davon ausgegangen wird, dass die selektierte Finalisierungsdauer von 15 Epochen nach den vorigen Beobachtungen genügt. Eine weitere Betrachtung, welche bei der Dimensionierung der zu finalisierten Modelle einfloss, besteht darin, dass aus verschiedenen Gründen kein durchgängiges Training der Modelle mithilfe der gewählten Google-Cloud-Instanz möglich ist. Die Instanz wird beispielsweise, um serverseitig Ressourcen einzusammeln, ungefähr halbtäglich recycelt. Auch das Training musste für die zwischenzeitliche Unterbrechung so konfiguriert werden, dass die Modelle in Google Drive zwischengespeichert werden, um den verlorenen Fortschritt bei der Terminierung der Instanz zu minimieren. Da das Training nach zwischenzeitlicher Terminierung der Instanz, welche sich auch teils nächtlich und ohne Aufsicht ereignete, stets manuell wieder initialisiert werden musste, konnte keine effektive Trainingsdauer von 24 Stunden pro Tag erreicht werden. Technisch wäre es außerdem möglich gewesen, die SHL-Datensätze über Google Drive einzubinden, dazu hätte jedoch ein weiteres kostenpflichtiges Abonnement abgeschlossen werden müssen, um den Speicherplatz in Google Drive zu erhöhen. Nach Evaluation des Kosten-Nutzen-Verhältnisses wurde stattdessen der SHL-Datensatz stets vor Beginn des Trainings auf den Festspeicher der Cloud-Instanz heruntergeladen. Dies resultierte in einer weiteren täglichen Wartezeit von ca. einer Stunde, welche die effektive Trainingsdauer weiter reduziert. Mit Berücksichtigung dieser Faktoren wurden schließlich die 10 besten Modelle der Rastersuche selektiert und Finalisiert.

\subsection{Resultierende Modelle}

Die aus der Rastersuche resultierenden Modellkonfigurationen sind in \Cref{tab:gridsearch-results} gezeigt. Von 10 Modellen bestehen drei Modelle aus 2D-Convolution-Schichten und einer vorgeschalteten \acrshort{stft}. Die überwiegende Mehrheit der Top-10-Modelle klassifiziert direkte Zeitliniendaten mithilfe von 1D-Convolution-Schichten analog zur initial getesteten Basisarchitektur. Außerdem überrepräsentiert ist der Adam-Optimizer, der bereits in der prototypischen Analyse der Basisarchitekturen die besten Ergebnisse erzielte, sowie die ResNetV1-Bauweise.

\input{sections/evaluation/gridsearch-results}

Die Tiefe der Modelle scheint analog zu den Betrachtungen aus \Cref{ch:verwandte-arbeiten} auch von Bedeutung zu sein. Keines der Top-10-Modelle besitzt nur einen Block von Convolution-Schichten. Die meisten der Top-10-Modelle besitzt 2 Convolution-Blöcke, das tiefste Modell mit der ID \texttt{6af8a1} und der insgesamt größtmöglichen Konfiguration erzielt die zweitbeste Validation-Accuracy. Zu beobachten ist außerdem, dass die in \Cref{fig:gridsearch-results} mit der größten Kreuzentropie auf den Validierungsdaten markierten Top-10-Modelle (\texttt{e6c367}, \texttt{6af8a1}, \texttt{752315}) auch gleichzeitig die Modelle mit einer erhöhten Tiefe sind. Die Modelle \texttt{752315}, \texttt{e6c367} zeigen neben \texttt{35d171} gleichzeitig das verhältnismäßig größte Overfitting auf den Trainingsdaten.

\section{Ergebnisse der Top-10-Finalisierung}

\begin{table}[h]
  \input{sections/evaluation/finalization-results}
  \caption{Quantitative Ergebnisse der Top-10-Finalisierung.}\label{tab:finalization-results}
\end{table}

Die Resultate der Top-10-Finalisierung sind in \Cref{tab:finalization-results} aufgelistet. Die Ergebnisse sind mit der Rangfolge aus der Rastersuche versehen. Einige der selektierten Modelle konnten durch Finalisierung ein signifikant besseres Ergebnis erreichen, als es die initiale Rangfolge aus der Rastersuche zunächst vermuten lässt. Das Modell \texttt{4ad2e7} auf Rang 4 der Rastersuche konnte das Modell \texttt{968ca3} auf Rang 1 in der Validation-Accuracy übertreffen. Gleichzeitig besitzt das Modell \texttt{4ad2e7} auch die kleinste Speichergröße im Festspeicher von 3,96 MB. Insgesamt lässt sich beobachten, dass die größeren Modelle nicht zwangsweise eine bessere Accuracy erreichen können, als die getesteten kleineren Modellvarianten. Die Rastersuche konnte somit mehrere Modellkonfigurationen identifizieren, welche vergleichsweise klein sind, gleichzeitig jedoch keinen signifikanten Tradeoff in der Validation-Accuracy aufweisen und somit bereits von diesem Standpunkt aus als besonders attraktiv für das Deployment auf Smartphones zu betrachten sind.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1072 567]{shl/finalization-results-accuracy.pdf}
  \caption{Entwicklung der Accuracy bei der Top-10-Finalisierung.}\label{fig:finalization-results-accuracy}
\end{figure}

Ebenfalls in \Cref{tab:finalization-results} hervorgehoben und anhand der Finalisierungsverläufe in \Cref{fig:finalization-results-accuracy} sichtbar ist das Overfitting der Modelle. Während des Trainings erreichten alle Modelle ein Plateau in der Validation-Accuracy. Ein auf Overfitting hindeutender Abfall der Verläufe konnte jedoch nicht beobachtet werden. Der Finalisierungsprozess wurde bei 3 von 10 Modellen durch Early Stopping terminiert. Dass die Kurven der Train-Accuracy im Vergleich mit der Validation-Accuracy voneinander abweichen, ist ein erwartetes Verhalten, bei einigen Modellen jedoch weichen die beiden Kurven stärker ab als bei anderen. Im Vergleich zeigen die getesteten Modelle auf STFT-Eingaben hierbei eine stärkere Differenz als die Modelle auf reinen Eingabedaten. Es wurden jedoch nur wenige STFT-Modelle betrachtet.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1071 567]{shl/finalization-results-loss.pdf}
  \caption{Entwicklung der Kreuzentropie bei der Top-10-Finalisierung.}\label{fig:finalization-results-loss}
\end{figure}

Neben STFT-Modellen zeigen auch die getesteten größeren Modellkonfigurationen eine erhöhte Abweichung zwischen der Train-Accuracy und der Validation-Accuracy. Dies deutet auf Overfitting hin, welches aus der Modellgröße resultiert. Die bei der Rastersuche beobachtete Entwicklung der Kreuzentropie bildet sich auch bei der Finalisierung erneut ab. Dies ist in \Cref{fig:finalization-results-loss} gezeigt. Auch hier zeigen die vergleichsweise großen Modelle eine stärkere Differenz zwischen Trainingsdaten und Validierungsdaten und somit eine erhöhte Unsicherheit beim Test der Generalisierungsfähigkeit. Die im Rahmen der Finalisierung einbezogenen größeren Modelle zeigen also nicht nur ein verstärktes Overfitting und eine stärkere Unsicherheit anhand der Kreuzentropie, sondern konnten auch in der Accuracy keine signifikant besseren Ergebnisse erzielen als vergleichsweise kleine Modellkonfigurationen. Ein Tradeoff allein anhand der Modellgröße konnte somit nicht beobachtet werden.

\section{Ergebnisse der Tradeoff-Analyse}

Die finalisierten Modelle wurden einem Profiling unterzogen, wie es in \Cref{ch:konzept} konzeptuell vorgestellt wurde. Die Ergebnisse sind in \Cref{tab:tradeoff-analyse-ergebnisse} aufgeschlüsselt. Die zwei Tradeoff-Parameter der Validation-Accuracy und der Modellgröße im Festspeicher wurden als Ergebnisse der Finalisierung übertragen. Mit einbezogen sind außerdem Analysewerte, die eine Auskunft über die auf den jeweiligen Hardwarebeschleuniger übertragenen Operationen geben. Dieser Wert ist mitunter sehr unterschiedlich und hängt davon ab, wieviele der verwendeten Operationen innerhalb des ResNet-Modells in Abhängigkeit von der Modellkonfiguration unterstützt werden. Einige Modelle konnten somit nur zu einem geringen Teil auf der GPU oder der \acrshort{ane} ausgeführt werden. Das bezüglich des relativen Anteils der ausgelagerten Operationen am besten auslagerbare Modell ist auch das größte getestete Modell mit 168.53 MB Speicherbedarf auf dem Festwertspeicher. Außerdem konnte beobachtet werden, dass mehr Operationen auf den \acrshort{ane}-Koprozessor ausgelagert werden können als auf die interne GPU.

\subsection{Arbeitsspeicherverbrauch}

Der Arbeitsspeicherverbrauch zeigt, dass die Modelle, zumindest bei einer nicht assistierten Ausführung auf der CPU, keine signifikant unterschiedlichen Anforderungen hieran aufweisen. Die Modelle werden offenbar durch das verwendete Framework TensorFlow Lite speichereffizient geladen und erreichen somit nur marginale Arbeitsspeicherverbräuche. Ein leicht erhöhter Arbeitsspeicherverbrauch ist bei der Ausführung auf der \acrshort{ane} messbar, eine Korrelation mit der Anzahl ausgelagerter Operationen ist nicht erkennbar. Vielmehr scheint der Arbeitsspeicherverbrauch anzusteigen, wenn eine insgesamt große Anzahl von Operationen im Modell vorhanden ist, diese jedoch nur zu einem untergeordneten Teil auf die \acrshort{ane} ausgelagert werden können. Dies lässt vermuten, dass der kausale Zusammenhang hinter der geringfügigen Erhöhung des Arbeitsspeicherverbrauchs durch die Kommunikation zwischen den Operationen auf unterschiedlichen Hardwarekomponenten und den daraus entstehenden Overhead bedingt sein kann. Auch der Arbeitsspeicherverbrauch auf der GPU zeigt einen betrachtenswerten Zusammenhang. Da bei der Messung des Speicherverbrauchs auch der VRAM der GPU mit gemessen wurde, zeigt sich bei dem Modell \texttt{752315} mit den meisten ausgelagerten Operationen ein systemübergreifender Speicherverbrauch von 347 MB und somit der einzige Speicherverbrauch, der bezüglich der Systemlimitationen als erhöht interpretiert werden kann. Plattformübergreifend zeigt sich, dass der beobachtete Speicherverbrauch nur marginal zwischen den getesteten Plattformen abweicht, sofern dies beurteilt werden kann. Unbekannt ist beispielsweise, ob neuere Versionen der \acrshort{ane} andere Messwerte bedingen, beispielsweise durch eine verbesserte Verfügbarkeit von auszulagernden Operationen. Selbst, wenn dies der Fall ist, können ältere Geräte jedoch nicht vernachlässigt werden, als Gegenstand der Betrachtung möglicher zu unterstützender Geräte, auf der eine App später ausgeführt wird.

\begin{landscape}
\begin{table}[]
  \resizebox{\textwidth}{!}{%
    \input{sections/evaluation/tradeoff-results}
    \caption{Ergebnisse der Tradeoff-Analyse.}\label{tab:tradeoff-analyse-ergebnisse}
  }
\end{table}
\end{landscape}

\subsection{Inferenzzeit}

Einen weiteren zentralen Analyseaspekt stellen die Verhältnisse zur Inferenzzeit dar. Durch die Berechnung des Korellationskoeffizienten $R$ zwischen den Ressourcenparametern lassen sich die Verhältnisse quantifizieren. Während die Modellgröße mit der Anzahl der Operationen positiv korreliert ($R\approx0.93$), korreliert auch die Inferenzzeit analog zur Intuition mit der Anzahl von Operationen, der Median der Korrelationen für alle Geräte und Hardwarebeschleuniger liegt bei $R\approx0.87$. Das iPhone XS inferiert im Schnitt 55\% schneller auf der CPU und 60\% schneller auf der GPU als das iPhone SE. Die \acrshort{ane} konnte nur bei zwei Modellen einen nennenswerten Speedup erreichen. Den besten Speedup erreichte die \acrshort{ane} auf dem Modell \texttt{752315}, welches fast alle Operationen auf der \acrshort{ane} ausführen kann. Auf anderen Modellen wiederum war die \acrshort{ane} bis zu 260\% langsamer als die Ausführung auf der CPU. Während die Inferenzzeiten bei der Nutzung der \acrshort{ane} stark von der CPU abweichen, ergeben sich bei der Nutzung der GPU keine signfikanten Änderungen in der Inferenzzeit. Lediglich eine Modellkonfiguration konnte von der Ausführung auf der GPU einen Speedup von 46\% erreichen, der hierbei gemessene Speedup besteht jedoch nur auf dem iPhone SE und nicht auf dem iPhone XS.

\subsection{CPU-Auslastung}

Ein eng mit der Inferenzzeit verknüpfter Ressourcenparameter bildet sich in der CPU-Auslastung. Wird das Modell ausschließlich auf der CPU ausgeführt, korreliert die CPU-Auslastung mit der Inferenzzeit sehr stark positiv mit $R_{iPhoneSE}\approx0.98$ und $R_{iPhoneXS}\approx0.995$. Bei der Auslagerung der Modelle auf Hardwarebeschleuniger verzerrt sich dieses Bild, je nachdem, in welchem Anteil die Modelle noch auf der CPU ausgeführt werden. Das bereits häufiger als Beispiel genannte Modell \texttt{752315} kann die CPU-Auslastung um bis zu 227\% reduzieren. Die Last wird in diesem Fall jedoch nur auf die GPU verlagert. Ob dies in einem Vorteil im Energieverbrauch resultiert, muss also separat geprüft werden. Werden Operationen auf die \acrshort{ane} ausgelagert, so zeigt sich ein ähnliches Bild wie bei der Inferenzzeit. Manche Modelle wie \texttt{752315} können die CPU-Auslastung reduzieren, andere wiederum erfahren einen gegenteiligen Effekt. Beim Modell \texttt{6af8a1} erhöht sich die CPU-Auslastung sogar auf mehr als das Doppelte der Auslastung bei reiner Ausführung auf der CPU. Ursache ist auch hier vermutlich der bei der Kommunikation der Layer-Ergebnisse entstehende Overhead. Analog zur Inferenzzeit bietet der \acrshort{ane}-Koprozessor also nur dann einen Vorteil, wenn das Modell zu einem großen Teil kompatibel mit den hierdurch zur Verfügung gestellten Berechnungsoperatoren ist.

\subsection{Energieverbrauch}

Als verbleibender Ressourcenparameter wurde der Energieverbrauch gemessen. Die Messung konnte allerdings nicht einwandfrei durchgeführt werden. Unter Xcode 12 konnten im Unterschied zum iPhone SE trotz verschiedener Versuche zur Problembehandlung keine Energy Logs (in \acrshort{aeu}) des iPhone XS aufgezeichnet werden. Die Schnittstelle zeigte lediglich den Hinweis \enquote{No Data}, während alle anderen Hardwareparameter problemlos aufgezeichnet werden konnten. Die Instruments-Schnittstelle zur Aufzeichnung von Energy Logs wurde darüber hinaus in der aktuellsten Beta von Xcode komplett entfernt, sodass die genauen \acrshort{aeu}-Werte nur für das iPhone SE bekannt sind. Die Betrachtung des Energieverbrauchs insbesondere der \acrshort{ane} auf dem iPhone XS entfällt somit, vermutlich liegt dieser analog zur Inferenzzeit und zur CPU-Auslastung jedoch im Schnitt höher als die Energieverbräuche auf CPU und GPU. Der Energieverbrauch bildet insgesamt keine Messwerte ab, welche als unvorhergesehen interpretiert werden können. Die Messwerte orientieren sich vielmehr an den bereits beobachteten Relationen zur Inferenzzeit und zum CPU-Verbrauch, analog zu den Beobachtungen aus \cite{brownlee_exploring_2021}. Die Auslagerung der Modelle auf die GPU ist außerdem analog zu den Messwerten weitestgehend unerheblich für den Gesamtenergieverbrauch.

Die beobachteten geringfügigen Abweichungen zwischen CPU und GPU sind nicht signifikant und können rein durch Messungenaugkeiten entstanden sein. Beim T-Test\footnote{\url{https://de.wikipedia.org/wiki/Zweistichproben-t-Test} (Abgerufen am 17.9.2021)} der statistischen Signfikanz ist $p \geq 0.05$. Die gemessenen \acrshort{aeu}-Werte sind jedoch mit Vorsicht zu interpretieren, denn es ist nicht bekannt, ob diese einen linearen, logarithmischen oder einen anderweitigen Zusammenhang abbilden. Eine Messung in mW wäre an dieser Stelle besser, diese ist jedoch auf iOS-Geräten aus technischen Gründen nicht möglich, da hierfür keine Schnittstelle existiert oder diese entfernt wurde\footnote{\url{https://gist.github.com/marcoarment/92d58159943240d6ba47} (Abgerufen am 17.9.2021)}. Um die gemessenen \acrshort{aeu}-Werte besser interpretieren zu können, wurden unter anderem auch die qualitativen Indikatoren von Xcode genutzt, genauer die Energy-Gauge und die abgebildeten prozentualen Verhältnisse des CPU- und GPU-Verbrauchs in Relation zum Energieverbrauchs des Displays. Die Modelle zeigen hierbei im Schnitt einen wesentlich geringeren Energieverbrauch als das Display des Smartphones, in der Größenordnung 1 zu 4. Wird GNSS aktiviert, so kann eine Verdopplung des gesamten Energieverbrauchs festgestellt werden. Diese Werte sind jedoch nur qualitative und nicht genau quantifizierbare Angaben, da die Energy-Gauge von Xcode keine Zahlenwerte anzeigt.

\subsection{Tradeoff und Selektion des Pareto-optimalen Modells}

Die betrachteten Modelle zeigen, dass die Modellgröße in den meisten Fällen über Arbeitsspeicherverbrauch, Inferenzzeit, CPU-Verbrauch und Energieverbrauch bestimmt. Sind die Modelle gut auf \acrshort{ane} oder GPU auszulagern, so werden teils weniger Ressourcen verbraucht, eine signifikante Reduktion des Energieverbrauchs konnte jedoch nicht betrachtet werden. Die Größe des Modells scheint bei den betrachteten Konfigurationen jedoch kein Garant für eine gute Ergebnisqualität zu sein. Stattdessen konnte das Modell \texttt{4ad2e7} gefunden werden, welches im Vergleich die beste Accuracy bietet und gleichzeitig den kleinsten Speicherabdruck besitzt. Hierdurch verzeichnet es im Vergleich zu anderen Modellen eine besonders kurze Inferenzzeit sowie einen geringen CPU- und Energieverbrauch. Der Arbeitsspeicherverbrauch des Modells ist nur geringfügig unterschiedlich zu den anderen betrachteten Modellen. Das Modell \texttt{4ad2e7} geht somit als klarer Sieger im Tradeoff-Vergleich der Top-10-Modelle hervor. Im Vergleich zu den anderen Modellkonfigurationen kann das Modell \texttt{4ad2e7} als Pareto-optimal betrachtet werden. Eine weitere Priorisierung oder Metamodellierung anhand der Ressourcenparameter ist somit nicht notwendig, da das Modell \texttt{4ad2e7} in fast allen Ressourcendimensionen die beste Variante darstellt.

\section{Ergebnisse der Optimierung}

\input{sections/evaluation/optimization-tradeoff-results}

\subsection{Architekturelle Restrukturierung}

\subsection{Pruning}

% PQAT, XNNPACK + Limitationen

\subsection{Beschränkung des Anwendungsfalls}

\section{Limitationen und Risiken für die Validität}

\subsection{Abhängigkeit von Threads}

\subsection{Abhängigkeit von der Inferenzrate}

\subsection{Abhängigkeit vom Datensatz und Real-World-Performance}

\section{Vergleich mit Movebis-Ansätzen}

\section{Zusammenfassung}
