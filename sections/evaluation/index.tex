\chapter{Evaluation}\label{ch:evaluation}

\section{Ergebnisse der Hyperband-Rastersuche}

Als Teil des Konzeptes wurden bereits verschiedene traditionelle Modelle und Deep-Learning-Modelle untersucht, um zu einem Basismodell zu gelangen, welches durch Variation der Hyperparameter einer Rastersuche unterzogen werden kann, und um schließlich die Tradeoff-Parameter hierauf zu evaluieren. Als Basisarchitektur wurde das ResNet als Erweiterung des in \cite{stojanov_continuous_2020} vorgestellten \acrshort{cnn}s gewählt. Die in \Cref{sec:metamodell-optimierung} definierten Hyperparameterdimensionen wurden implementiert und anschließend einer Rastersuche unterzogen.

\subsection{Trainingsverläufe}

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1143 351]{shl/gridsearch-results.pdf}
  \caption{Trainingsverläufe der Hyperband-Rastersuche.}\label{fig:gridsearch-results}
\end{figure}

Die Trainingsverläufe der Hyperband-Rastersuche sind in \Cref{fig:gridsearch-results} gezeigt. Hierbei wird noch einmal deutlich, dass die Modelle im Rahmen des Hyperband-Algorithmus nicht über die gleiche Anzahl von Epochen trainiert wurden. Die Anzahl von Epochen wurde vom Hyperband-Orakel schrittweise selektiv erhöht. Bereits beim Training der verschiedenen Modellvariationen auf dem \acrshort{shl}-Datensatz konnten einige der Modelle die zuvor erreichte Validation-Accuracy von $78,94\%$ um einige Prozentpunkte übertreffen. Durch die Hyperband-Rastersuche konnten somit mehrere Modellvariationen mit besseren Ergebnisqualitäten als die der Basisarchitektur gefunden werden. Die Train-Accuracy liegt häufig höher, das diesbezüglich beste Modell erreichte hierbei über 95\%.

Außerdem lässt sich beobachten, dass die Kreuzentropie als Verlust auf den Validierungsdaten teils stark ansteigt, obwohl die Validation-Accuracy ebenfalls weiterhin ansteigt und nur selten zwischenzeitlich stärker abfällt. Diese Beobachtung lässt sich erklären durch eine stärkere Unsicherheit zwischen den Output-Neuronen in der letzten Dense Schicht mit Softmax-Aktivierungsfunktion. Während die relative Unsicherheit der Output-Neuronen in die Berechnung der kategorischen Kreuzentropie mit einfließt, zählt für die Berechnung der Accuracy nur das am stärksten aktivierte Neuron. Die Validation-Accuracy wird als besser vergleichbares Sortierungskriterium für die trainierten Ansätze selektiert. Zu vernachlässigen ist die teils hohe Unsicherheit der Output-Neuronen jedoch nicht, denn dies kann zu einer stärkeren Fluktuation der Klassen bei einer Echtzeitanwendung führen. Dieses Problem kann mitigiert werden, indem eine der Postprocessing-Techniken aus \cite{werner_kontinuierliche_2020} oder \cite{stojanov_continuous_2020} eingesetzt wird. Trotzdem sollte die Unsicherheit der Modelle mit in die weiteren Auswertungen mit einfließen, daher wurden die größten vier Kreuzentropien in \Cref{fig:gridsearch-results} mit deren Identifikationsnummer hervorgehoben.

\subsection{Analyse der Trainingsdauer zur Dimensionierung der Finalisierung}

\begin{figure}[h]
  \includegraphics[width=0.5\linewidth, bb=0 0 362 290]{shl/gridsearch-finalization-prediction.pdf}
  \caption{Trainings- und Finalisierungszeiten der Modelle.}\label{fig:finalization-prediction}
\end{figure}

Die Hyperband-Rastersuche wurde über einen Zeitraum von 2 Wochen durchgeführt. Als Ergebnis resultiert eine Rangfolge von insgesamt 28 der möglichen ResNet-Konfigurationen. Um zu entscheiden, wie viele der getesteten Hyperparameter-Konfigurationen insgesamt und über wie viele Epochen diese finalisiert werden sollen, wurden das Trainingsverhalten der Modelle statistisch analysiert. Da analog zu \Cref{fig:gridsearch-results} der Trainingsfortschritt (nach Validation-Accuracy) der über 10 oder mehr Epochen\footnote{Eine Epoche beinhaltet ca. 800.000 Samples aus dem \acrshort{shl}-Datensatz.} trainierten Modelle zumeist nach 5 bis 10 Epochen stagnierte, wurde eine maximale Trainingsdauer zur Finalisierung von 15 Epochen festgelegt. Bei dieser Entscheidung musste zusätzlich berücksichtigt werden, dass die Finalisierung in einem für diese Arbeit realisierbaren Zeitrahmen verbleibt. Sowohl die maximale Anzahl von Epochen, sofern kein Early Stopping einsetzt, als auch die Gesamtanzahl der für die Finalisierung selektierten Modelle musste auf Grundlage einer Schätzung eines akzeptablen Zeitaufwandes zum Training erfolgen.

Daher wurden die Trainingsverläufe zusätzlich statistisch evaluiert, um eine Schätzung über die Trainingsdauer pro Epoche zu erhalten und diese auf 15 Epochen zu extrapolieren. Das Ergebnis dieser Analyse ist in \Cref{fig:finalization-prediction} dargestellt. Das Histogramm zeigt, dass sich die meisten Modelle im Bereich von ca. 10 Minuten pro Epoche befinden, teils jedoch auch deutlich länger trainieren können, bis zu einer Trainingszeit von ca. 60 Minuten pro Epoche. Die Finalisierung eines solchen Modells würde somit geschätzt 15 Stunden betragen. Die meisten Modelle können in unter 3 Stunden finalisiert werden, sofern davon ausgegangen wird, dass die selektierte Finalisierungsdauer von 15 Epochen nach den vorigen Beobachtungen genügt. Eine weitere Betrachtung, welche bei der Dimensionierung der zu finalisierten Modelle einfloss, besteht darin, dass aus verschiedenen Gründen kein durchgängiges Training der Modelle mithilfe der gewählten Google-Cloud-Instanz möglich ist. Die Instanz wird beispielsweise, um serverseitig Ressourcen einzusammeln, ungefähr halbtäglich recycelt. Auch das Training musste für die zwischenzeitliche Unterbrechung so konfiguriert werden, dass die Modelle in Google Drive zwischengespeichert werden, um den verlorenen Fortschritt bei der Terminierung der Instanz zu minimieren. Da das Training nach zwischenzeitlicher Terminierung der Instanz, welche sich auch teils nächtlich und ohne Aufsicht ereignete, stets manuell wieder initialisiert werden musste, konnte keine effektive Trainingsdauer von 24 Stunden pro Tag erreicht werden. Technisch wäre es außerdem möglich gewesen, die SHL-Datensätze über Google Drive einzubinden, dazu hätte jedoch ein weiteres kostenpflichtiges Abonnement abgeschlossen werden müssen, um den Speicherplatz in Google Drive zu erhöhen. Nach Evaluation des Kosten-Nutzen-Verhältnisses wurde stattdessen der SHL-Datensatz stets vor Beginn des Trainings auf den Festspeicher der Cloud-Instanz heruntergeladen. Dies resultierte in einer weiteren täglichen Wartezeit von ca. einer Stunde, welche die effektive Trainingsdauer weiter reduziert. Mit Berücksichtigung dieser Faktoren wurden schließlich die 10 besten Modelle der Rastersuche selektiert und Finalisiert.

\subsection{Resultierende Modelle}

Die aus der Rastersuche resultierenden Modellkonfigurationen sind in \Cref{tab:gridsearch-results} gezeigt. Von 10 Modellen bestehen drei Modelle aus 2D-Convolution-Schichten und einer vorgeschalteten \acrshort{stft}. Die überwiegende Mehrheit der Top-10-Modelle klassifiziert direkte Zeitliniendaten mithilfe von 1D-Convolution-Schichten analog zur initial getesteten Basisarchitektur. Außerdem überrepräsentiert ist der Adam-Optimizer, der bereits in der prototypischen Analyse der Basisarchitekturen die besten Ergebnisse erzielte, sowie die ResNetV1-Bauweise.

\input{sections/evaluation/gridsearch-results}

Die Tiefe der Modelle scheint analog zu den Betrachtungen aus \Cref{ch:verwandte-arbeiten} auch von Bedeutung zu sein. Keines der Top-10-Modelle besitzt nur einen Block von Convolution-Schichten. Die meisten der Top-10-Modelle besitzt 2 Convolution-Blöcke, das tiefste Modell mit der ID \texttt{6af8a1} und der insgesamt größtmöglichen Konfiguration erzielt die zweitbeste Validation-Accuracy. Zu beobachten ist außerdem, dass die in \Cref{fig:gridsearch-results} mit der größten Kreuzentropie auf den Validierungsdaten markierten Top-10-Modelle (\texttt{e6c367}, \texttt{6af8a1}, \texttt{752315}) auch gleichzeitig die Modelle mit einer erhöhten Tiefe sind. Die Modelle \texttt{752315}, \texttt{e6c367} zeigen neben \texttt{35d171} gleichzeitig das verhältnismäßig größte Overfitting auf den Trainingsdaten.

\section{Ergebnisse der Top-10-Finalisierung}

\begin{table}[h]
  \input{sections/evaluation/finalization-results}
  \caption{Quantitative Ergebnisse der Top-10-Finalisierung.}\label{tab:finalization-results}
\end{table}

Die Resultate der Top-10-Finalisierung sind in \Cref{tab:finalization-results} aufgelistet. Die Ergebnisse sind mit der Rangfolge aus der Rastersuche versehen. Einige der selektierten Modelle konnten durch Finalisierung ein signifikant besseres Ergebnis erreichen, als es die initiale Rangfolge aus der Rastersuche zunächst vermuten lässt. Das Modell \texttt{4ad2e7} auf Rang 4 der Rastersuche konnte das Modell \texttt{968ca3} auf Rang 1 in der Validation-Accuracy übertreffen. Gleichzeitig besitzt das Modell \texttt{4ad2e7} auch die kleinste Speichergröße im Festspeicher von 3,96 MB. Insgesamt lässt sich beobachten, dass die größeren Modelle nicht zwangsweise eine bessere Accuracy erreichen können, als die getesteten kleineren Modellvarianten. Die Rastersuche konnte somit mehrere Modellkonfigurationen identifizieren, welche vergleichsweise klein sind, gleichzeitig jedoch keinen signifikanten Tradeoff in der Validation-Accuracy aufweisen und somit bereits von diesem Standpunkt aus als besonders attraktiv für das Deployment auf Smartphones zu betrachten sind.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1072 567]{shl/finalization-results-accuracy.pdf}
  \caption{Entwicklung der Accuracy bei der Top-10-Finalisierung.}\label{fig:finalization-results-accuracy}
\end{figure}

Ebenfalls in \Cref{tab:finalization-results} hervorgehoben und anhand der Finalisierungsverläufe in \Cref{fig:finalization-results-accuracy} sichtbar ist das Overfitting der Modelle. Während des Trainings erreichten alle Modelle ein Plateau in der Validation-Accuracy, ein starker, auf Overfitting hindeutender, Abfall der Verläufe konnte jedoch nicht beobachtet werden. Der Finalisierungsprozess wurde bei 3 von 10 Modellen durch Early Stopping terminiert. Dass die Kurven der Train-Accuracy im Vergleich mit der Validation-Accuracy voneinander abweichen ist ein erwartetes Verhalten, bei einigen Modellen jedoch weichen die beiden Kurven stärker ab als bei anderen. Im Vergleich zeigen die getesteten Modelle auf STFT-Eingaben hierbei eine stärkere Differenz als die Modelle auf reinen Eingabedaten. Es wurden jedoch nur wenige STFT-Modelle betrachtet.

\begin{figure}[h]
  \includegraphics[width=\linewidth, bb=0 0 1071 567]{shl/finalization-results-loss.pdf}
  \caption{Entwicklung der Kreuzentropie bei der Top-10-Finalisierung.}\label{fig:finalization-results-loss}
\end{figure}

Neben STFT-Modellen zeigen auch die getesteten größeren Modellkonfigurationen eine erhöhte Abweichung zwischen der Train-Accuracy und der Validation-Accuracy. Dies deutet auf Overfitting hin, welches aus der Modellgröße resultiert. Die bei der Rastersuche beobachtete Entwicklung der Kreuzentropie bildet sich auch bei der Finalisierung erneut ab. Dies ist in \Cref{fig:finalization-results-loss} gezeigt. Auch hier zeigen die vergleichsweise großen Modelle eine stärkere Differenz zwischen Trainingsdaten und Validierungsdaten und somit eine erhöhte Unsicherheit beim Test der Generalisierungsfähigkeit. Die im Rahmen der Finalisierung einbezogenen größeren Modelle zeigen also nicht nur ein verstärktes Overfitting und eine stärkere Unsicherheit anhand der Kreuzentropie, sondern konnten auch in der Accuracy keine signifikant besseren Ergebnisse erzielen als vergleichsweise kleine Modellkonfigurationen. Ein Tradeoff allein anhand der Modellgröße konnte somit nicht beobachtet werden.

\section{Ergebnisse der Tradeoff-Analyse}

Die finalisierten Modelle wurden einem Profiling unterzogen, wie es in \Cref{ch:konzept} konzeptuell vorgestellt wurde. Die Ergebnisse sind in \Cref{tab:tradeoff-analyse-ergebnisse} aufgeschlüsselt. Die zwei Tradeoff-Parameter der Validation-Accuracy und der Modellgröße im Festspeicher wurden als Ergebnisse der Finalisierung übertragen. Mit einbezogen sind außerdem Analysewerte, die eine Auskunft über die auf den jeweiligen Hardwarebeschleuniger übertragenen Operationen geben. Dieser Wert ist mitunter sehr unterschiedlich und hängt davon ab, wieviele der verwendeten Operationen innerhalb des ResNet-Modells in Abhängigkeit von der Modellkonfiguration unterstützt werden. Einige Modelle konnten somit nur zu einem geringen Teil auf der GPU oder der \acrshort{ane} ausgeführt werden. Das bezüglich des relativen Anteils der ausgelagerten Operationen am besten auslagerbare Modell ist auch das größte getestete Modell mit 168.53 MB Speicherbedarf auf dem Festwertspeicher. Außerdem konnte beobachtet werden, dass mehr Operationen auf den \acrshort{ane}-Koprozessor ausgelagert werden können als auf die interne GPU.

Der RAM-Speicherverbrauch zeigt, dass die Modelle, zumindest bei einer nicht assistierten Ausführung auf der CPU, keine signifikant unterschiedlichen Anforderungen hieran aufweisen. Die Modelle werden offenbar durch das verwendete Framework TensorFlow Lite speichereffizient geladen und erreichen somit nur marginale RAM-Verbräuche. Ein leicht erhöhter RAM-Verbrauch ist bei der Ausführung auf der \acrshort{ane} messbar, eine Korrelation mit der Anzahl ausgelagerter Operationen ist nicht erkennbar. Vielmehr scheint der RAM-Verbrauch anzusteigen, wenn eine insgesamt große Anzahl von Operationen im Modell vorhanden ist, diese jedoch nur zu einem untergeordneten Teil auf die \acrshort{ane} ausgelagert werden können. Dies lässt vermuten, dass der kausale Zusammenhang hinter der geringfügigen Erhöhung des RAM-Verbrauchs durch die Kommunikation zwischen den Operationen auf unterschiedlichen Hardwarekomponenten bedingt sein kann. Auch der RAM-Verbrauch auf der GPU zeigt einen betrachtenswerten Zusammenhang. Da bei der Messung des Speicherverbrauchs auch der VRAM der GPU mit gemessen wurde, zeigt sich bei dem Modell \texttt{752315} mit den meisten ausgelagerten Operationen ein systemübergreifender Speicherverbrauch von 347 MB und somit der einzige Speicherverbrauch, der bezüglich der Systemlimitationen als erhöht interpretiert werden kann. Plattformübergreifend zeigt sich, dass der beobachtete Speicherverbrauch nur marginal zwischen den getesteten Plattformen abweicht, sofern dies beurteilt werden kann. Unbekannt ist beispielsweise, ob neuere Versionen der \acrshort{ane} andere Messwerte bedingen, beispielsweise durch eine verbesserte Verfügbarkeit von auszulagernden Operationen. Selbst, wenn dies der Fall ist, können ältere Geräte jedoch nicht vernachlässigt werden, als Gegenstand der Betrachtung möglicher zu unterstützender Geräte, auf der eine App später ausgeführt wird.

\begin{landscape}
\begin{table}[]
  \resizebox{\textwidth}{!}{%
    \input{sections/evaluation/tradeoff-results}
    \caption{Ergebnisse der Tradeoff-Analyse.}\label{tab:tradeoff-analyse-ergebnisse}
  }
\end{table}
\end{landscape}

% Unter Xcode 12 konnten im Unterschied zum iPhone SE trotz verschiedener Versuche zur Problembehandlung keine Energy Logs (in \acrshort{aeu}) des iPhone XS aufgezeichnet werden. Die Schnittstelle zeigte lediglich den Hinweis \enquote{No Data}, während alle anderen Hardwareparameter problemlos aufgezeichnet werden konnten. Die Instruments-Schnittstelle zur Aufzeichnung von Energy Logs wurde darüber hinaus in der aktuellsten Beta von Xcode komplett entfernt, sodass die genauen \acrshort{aeu}-Werte nur für das iPhone SE bekannt sind.

\section{Ergebnisse der Optimierung}

\subsection{Limitierungen beim Pruning durch \acrshort{pqat} und XNNPACK}

\subsection{Beschränkung des Anwendungsfalls}

\section{Limitationen und Risiken für die Validität}

\subsection{Abhängigkeit von Threads}

\subsection{Abhängigkeit von der Inferenzrate}

\subsection{Real-World-Performance}

\section{Vergleich mit Movebis-Ansätzen}

\section{Zusammenfassung}
