{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-deep-learning-timeseries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCjUOLzIZ0b"
      },
      "source": [
        "# Using a deep CNN to directly classify SHL timeseries data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpWbBKRvep4"
      },
      "source": [
        "# Free up some disk space on colab\n",
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/torch\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/pystan\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/spacy\n",
        "!rm -rf /tensorflow-1.15.2/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6b-V4Ewjxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c904f9-69d7-490b-b715-569b33a92ebe"
      },
      "source": [
        "# Get needed auxiliary files for colab\n",
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 1755, done.\u001b[K\n",
            "remote: Counting objects: 100% (1092/1092), done.\u001b[K\n",
            "remote: Compressing objects: 100% (734/734), done.\u001b[K\n",
            "remote: Total 1755 (delta 542), reused 817 (delta 301), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (1755/1755), 34.53 MiB | 20.57 MiB/s, done.\n",
            "Resolving deltas: 100% (918/918), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL4FprvwmKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af887905-7f9c-4e34-dbce-825071328e16"
      },
      "source": [
        "# Change into src dir and load our datasets\n",
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWAbEF-wnYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53edff6-a1fe-4c56-a0c5-da38f5fa7bff"
      },
      "source": [
        "# Download training datasets\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-train_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
        "# Download validation dataset\n",
        "!wget -nc -O shl-dataset/challenge-2020-validation.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-19 09:37:33--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852446972 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_torso.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.45G  7.41MB/s    in 12m 6s  \n",
            "\n",
            "2021-08-19 09:49:39 (7.69 MB/s) - ‘shl-dataset/challenge-2019-train_torso.zip’ saved [5852446972/5852446972]\n",
            "\n",
            "--2021-08-19 09:49:39--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5628524721 (5.2G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_bag.zip’\n",
            "\n",
            "enge-2019-train_bag   8%[>                   ] 434.75M  7.35MB/s    eta 12m 15s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJK91R8EAb0b"
      },
      "source": [
        "# Unzip training datasets\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_torso shl-dataset/challenge-2019-train_torso.zip\n",
        "!rm shl-dataset/challenge-2019-train_torso.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_bag shl-dataset/challenge-2019-train_bag.zip\n",
        "!rm shl-dataset/challenge-2019-train_bag.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_hips shl-dataset/challenge-2019-train_hips.zip\n",
        "!rm shl-dataset/challenge-2019-train_hips.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-train_hand shl-dataset/challenge-2020-train_hand.zip\n",
        "!rm shl-dataset/challenge-2020-train_hand.zip\n",
        "# Unzip validation dataset\n",
        "!unzip -n -d shl-dataset/challenge-2020-validation shl-dataset/challenge-2020-validation.zip\n",
        "!rm shl-dataset/challenge-2020-validation.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZAJlKdu06f1"
      },
      "source": [
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMIdkUVR9-m",
        "outputId": "f440756d-fca8-4cb7-d032-3d17cbf9989c"
      },
      "source": [
        "# Check configuration and hardware resources\n",
        "\n",
        "import distutils\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcVSnPAOENt"
      },
      "source": [
        "# Define all datasets to train our model on\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-train_torso/train/Torso'),\n",
        "    Path('shl-dataset/challenge-2019-train_bag/train/Bag'),\n",
        "    Path('shl-dataset/challenge-2019-train_hips/train/Hips'),\n",
        "    Path('shl-dataset/challenge-2020-train_hand/train/Hand'),\n",
        "]\n",
        "\n",
        "VALIDATION_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Hand'),                  \n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tYXzJj9OHGo"
      },
      "source": [
        "# Define more useful constants about our dataset\n",
        "\n",
        "LABEL_ORDER = [\n",
        "    'Null',\n",
        "    'Still',\n",
        "    'Walking',\n",
        "    'Run',\n",
        "    'Bike',\n",
        "    'Car',\n",
        "    'Bus',\n",
        "    'Train',\n",
        "    'Subway',\n",
        "]\n",
        "\n",
        "SAMPLE_LENGTH = 500"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHfmi8ZOR1n"
      },
      "source": [
        "# Results from data analysis\n",
        "\n",
        "CLASS_WEIGHTS = {\n",
        "    0: 0.0, # NULL label\n",
        "    1: 1.0021671573438011, \n",
        "    2: 0.9985739895697523, \n",
        "    3: 2.8994439843842423, \n",
        "    4: 1.044135815617944, \n",
        "    5: 0.7723505499007343, \n",
        "    6: 0.8652474758172704, \n",
        "    7: 0.7842127155793044, \n",
        "    8: 1.0283208861290594\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAPsgtXOW3_"
      },
      "source": [
        "# Define features for our dataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Attributes to load from our dataset\n",
        "X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "]\n",
        "\n",
        "# Files within the dataset that contain our attributes\n",
        "X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "]\n",
        "\n",
        "# Features to generate from our loaded attributes\n",
        "# Note that `a` is going to be a dict of attribute tracks\n",
        "X_features = OrderedDict({\n",
        "    'acc_mag': lambda a: np.sqrt(a['acc_x']**2 + a['acc_y']**2 + a['acc_z']**2),\n",
        "    'mag_mag': lambda a: np.sqrt(a['mag_x']**2 + a['mag_y']**2 + a['mag_z']**2),\n",
        "    'gyr_mag': lambda a: np.sqrt(a['gyr_x']**2 + a['gyr_y']**2 + a['gyr_z']**2),\n",
        "})\n",
        "\n",
        "# Define where to find our labels for supervised learning\n",
        "y_file = 'Label.txt'\n",
        "y_attribute = 'labels'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzV0cHfZRmUh",
        "outputId": "900c3d35-eefc-41cb-ff58-d8fc7ab3bff5"
      },
      "source": [
        "# Load pretrained power transformers for feature scaling\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_feature_scalers = OrderedDict({})\n",
        "for feature_name, _ in X_features.items():\n",
        "    scaler_dir = f'models/shl-scalers/{feature_name}.scaler.joblib'\n",
        "    scaler = joblib.load(scaler_dir)\n",
        "    scaler.copy = False # Save memory\n",
        "    X_feature_scalers[feature_name] = scaler\n",
        "    print(f'Loaded scaler from {scaler_dir}.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded scaler from models/shl-scalers/acc_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/mag_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/gyr_mag.scaler.joblib.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PowerTransformer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_r8kxvWrtE",
        "outputId": "e2fefba6-61ed-4703-d111-0878f2a5f348"
      },
      "source": [
        "# Load the training and validation data into a high performance datatype\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def read_chunks(n_chunks, X_attr_readers, y_attr_reader):\n",
        "    for _ in range(n_chunks):\n",
        "        # Load raw attribute tracks\n",
        "        X_raw_attrs = OrderedDict({})\n",
        "        for X_attribute, X_attr_reader in zip(X_attributes, X_attr_readers):\n",
        "            X_attr_track = next(X_attr_reader)\n",
        "            X_attr_track = np.nan_to_num(X_attr_track.to_numpy())\n",
        "            X_raw_attrs[X_attribute] = X_attr_track\n",
        "\n",
        "        # Calculate features\n",
        "        X_feature_tracks = None\n",
        "        for X_feature_name, X_feature_func in X_features.items():\n",
        "            X_feature_track = X_feature_func(X_raw_attrs)\n",
        "            X_feature_track = X_feature_scalers[X_feature_name] \\\n",
        "                .transform(X_feature_track)\n",
        "            if X_feature_tracks is None:\n",
        "                X_feature_tracks = X_feature_track\n",
        "            else:\n",
        "                X_feature_tracks = np.dstack((X_feature_tracks, X_feature_track))\n",
        "\n",
        "        # Load labels\n",
        "        y_attr_track = next(y_attr_reader) # dim (None, sample_length)\n",
        "        y_attr_track = np.nan_to_num(y_attr_track.to_numpy()) # dim (None, sample_length)\n",
        "        y_attr_track = y_attr_track[:, 0] # dim (None, 1)\n",
        "\n",
        "        yield X_feature_tracks, y_attr_track\n",
        "\n",
        "def count_samples(dataset_dir):\n",
        "    # Every file in the dataset has the same length, use the labels file\n",
        "    n_samples = 0\n",
        "    with open(dataset_dir / y_file) as f:\n",
        "        for _ in tqdm(f, desc=f'Counting samples in {dataset_dir}'):\n",
        "            n_samples += 1\n",
        "    return n_samples\n",
        "\n",
        "def create_chunked_readers(\n",
        "    dataset_dir,\n",
        "    chunksize, \n",
        "    xdtype=np.float32, # Use np.float16 with caution, can lead to overflows\n",
        "    ydtype=np.int\n",
        "):\n",
        "    # Initialize chunked csv readers\n",
        "    read_csv_kwargs = { 'sep': ' ', 'header': None, 'chunksize': chunksize }\n",
        "\n",
        "    X_attr_readers = [] # (dim datasets x readers)\n",
        "    for filename in X_files:\n",
        "        X_reader = pd.read_csv(dataset_dir / filename, dtype=xdtype, **read_csv_kwargs)\n",
        "        X_attr_readers.append(X_reader)\n",
        "    y_attr_reader = pd.read_csv(dataset_dir / y_file, dtype=ydtype, **read_csv_kwargs)\n",
        "\n",
        "    return X_attr_readers, y_attr_reader\n",
        "\n",
        "def export_tfrecords(\n",
        "    dataset_dir,\n",
        "    n_chunks=16, # Load dataset in parts to not overload memory\n",
        "):\n",
        "    target_dir = f'{dataset_dir}.tfrecord'\n",
        "    if os.path.isfile(target_dir):\n",
        "        print(f'{target_dir} already exists.')\n",
        "        return\n",
        "\n",
        "    print(f'Exporting to {target_dir}.')\n",
        "\n",
        "    n_samples = count_samples(dataset_dir)\n",
        "    chunksize = int(np.floor(n_samples / n_chunks))\n",
        "    X_attr_readers, y_attr_reader = create_chunked_readers(dataset_dir, chunksize)    \n",
        "\n",
        "    with tf.io.TFRecordWriter(str(target_dir)) as file_writer:\n",
        "        with tqdm(total=n_samples, desc=f'Reading samples to {target_dir}') as pbar:\n",
        "            for X_feature_tracks, y_attr_track in read_chunks(\n",
        "                n_chunks, X_attr_readers, y_attr_reader\n",
        "            ):\n",
        "                for X, y in zip(X_feature_tracks, y_attr_track):\n",
        "                    X_flat = X.flatten() # TFRecords don't support multidimensional arrays\n",
        "                    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
        "                        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])) \n",
        "                    })).SerializeToString()\n",
        "                    file_writer.write(record_bytes)\n",
        "                pbar.update(chunksize)\n",
        "\n",
        "for dataset_dir in TRAIN_DATASET_DIRS + VALIDATION_DATASET_DIRS:\n",
        "    export_tfrecords(dataset_dir)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shl-dataset/challenge-2019-train_torso/train/Torso.tfrecord already exists.\n",
            "Exporting to shl-dataset/challenge-2019-train_bag/train/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-train_bag/train/Bag: 196072it [00:02, 74285.74it/s]\n",
            "Reading samples from shl-dataset/challenge-2019-train_bag/train/Bag.tfrecord: 100%|█████████▉| 196064/196072 [04:21<00:00, 750.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-train_hips/train/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-train_hips/train/Hips: 196072it [00:02, 72422.27it/s]\n",
            "Reading samples from shl-dataset/challenge-2019-train_hips/train/Hips.tfrecord: 100%|█████████▉| 196064/196072 [04:26<00:00, 734.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-train_hand/train/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-train_hand/train/Hand: 196072it [00:02, 72511.64it/s]\n",
            "Reading samples from shl-dataset/challenge-2020-train_hand/train/Hand.tfrecord: 100%|█████████▉| 196064/196072 [04:26<00:00, 736.54it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Torso: 28789it [00:00, 90973.95it/s]\n",
            "Reading samples from shl-dataset/challenge-2020-validation/validation/Torso.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 673.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Bag: 28789it [00:00, 91195.40it/s]\n",
            "Reading samples from shl-dataset/challenge-2020-validation/validation/Bag.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 675.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Hips: 28789it [00:00, 91566.55it/s]\n",
            "Reading samples from shl-dataset/challenge-2020-validation/validation/Hips.tfrecord: 100%|█████████▉| 28784/28789 [00:43<00:00, 669.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Hand: 28789it [00:00, 91931.22it/s]\n",
            "Reading samples from shl-dataset/challenge-2020-validation/validation/Hand.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 679.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weYDZUofD4TD"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "SHUFFLE_SIZE = 16384 # Must be larger than batch size\n",
        "\n",
        "def decode_tfrecord(record_bytes):\n",
        "    example = tf.io.parse_single_example(record_bytes, {\n",
        "        'X': tf.io.FixedLenFeature([SAMPLE_LENGTH, len(X_features)], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([1], tf.int64)\n",
        "    })\n",
        "    return example['X'], example['y']\n",
        "\n",
        "def create_dataset_tensors(dataset_dirs):\n",
        "    tfrecord_dirs = [f'{d}.tfrecord' for d in dataset_dirs]\n",
        "    print(f'Creating dataset over {tfrecord_dirs}.')     \n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_dirs) \\\n",
        "        .map(decode_tfrecord, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .shuffle(SHUFFLE_SIZE) \\\n",
        "        .batch(BATCH_SIZE)\n",
        "    count = sum(1 for _ in dataset)\n",
        "    print(f'Counted {count * BATCH_SIZE} samples in dataset.')\n",
        "    return dataset\n",
        "\n",
        "def create_train_tensors():\n",
        "    return create_dataset_tensors(TRAIN_DATASET_DIRS)\n",
        "\n",
        "def create_validation_tensors():\n",
        "    return create_dataset_tensors(VALIDATION_DATASET_DIRS)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLoGsGB76LHM"
      },
      "source": [
        "!pip install keras-tuner -q"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Xd0mWdi5ri"
      },
      "source": [
        "# Define helper functions for model creation\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def make_resnet_block(input_layer, block_height):\n",
        "    conv_kwargs = { \n",
        "        'filters': block_height, \n",
        "        'padding': 'same', \n",
        "        'kernel_regularizer': 'l2',\n",
        "    }\n",
        "\n",
        "    conv_x = layers.Conv1D(kernel_size=8, **conv_kwargs)(input_layer)\n",
        "    conv_x = layers.BatchNormalization()(conv_x)\n",
        "    conv_x = layers.LeakyReLU(alpha=0.2)(conv_x)\n",
        "\n",
        "    conv_y = layers.Conv1D(kernel_size=5, **conv_kwargs)(conv_x)\n",
        "    conv_y = layers.BatchNormalization()(conv_y)\n",
        "    conv_y = layers.LeakyReLU(alpha=0.2)(conv_y)\n",
        "\n",
        "    conv_z = layers.Conv1D(kernel_size=3, **conv_kwargs)(conv_y)\n",
        "    conv_z = layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    shortcut = layers.Conv1D(kernel_size=1, **conv_kwargs)(input_layer)\n",
        "    shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    output_block = layers.add([shortcut, conv_z])\n",
        "    output_block = layers.LeakyReLU(alpha=0.2)(output_block)\n",
        "\n",
        "    return output_block\n",
        "\n",
        "\n",
        "def make_resnet(hp):\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "    input_layer = layers.Input(input_shape)\n",
        "\n",
        "    endpoint_layer = input_layer # Will be built now\n",
        "    for i in range(hp.Int('n_layers', 2, 10)):\n",
        "        endpoint_layer = make_resnet_block(\n",
        "            endpoint_layer, \n",
        "            hp.Int(f'block_{i}_maps', 64, 512, step=64),\n",
        "        )\n",
        "    \n",
        "    gap_layer = layers.GlobalAveragePooling1D()(endpoint_layer)\n",
        "    output_layer = layers.Dense(len(LABEL_ORDER), activation='softmax')(gap_layer)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOH7F65U7fYx",
        "outputId": "46e4fde8-a274-49ec-a110-a6b5d0c95005"
      },
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=make_resnet, \n",
        "    objective='val_acc', \n",
        "    max_epochs=15, \n",
        "    overwrite=True,\n",
        "    directory='models',\n",
        "    project_name='shl-resnet-gridsearch',\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 4\n",
            "n_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 5, 'step': 1, 'sampling': None}\n",
            "block_0_maps (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': None}\n",
            "block_0_regularizer (Boolean)\n",
            "{'default': True, 'conditions': []}\n",
            "block_0_activation (Choice)\n",
            "{'default': 'lrelu', 'conditions': [], 'values': ['lrelu', 'relu'], 'ordered': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmEnc_5l8Wln"
      },
      "source": [
        "# Define callbacks for our training\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "decay_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_acc',\n",
        "    factor=0.5, \n",
        "    patience=5, # Epochs\n",
        "    min_lr=0.0001, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "stop_early = callbacks.EarlyStopping(\n",
        "    monitor='val_acc', \n",
        "    patience=10, # Epochs\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX8Z0ZyW-nJ9",
        "outputId": "355009d0-6efa-4ba5-def2-59db4c402a88"
      },
      "source": [
        "# Keras tuner grid search training\n",
        "\n",
        "tuner.search(\n",
        "    create_train_tensors(),\n",
        "    epochs=15,\n",
        "    callbacks=[decay_lr, stop_early],\n",
        "    validation_data=create_validation_tensors(),\n",
        "    verbose=1,\n",
        "    shuffle=False, # Shuffling doesn't work with our prefetching\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 21 Complete [01h 17m 57s]\n",
            "val_acc: 0.5507139563560486\n",
            "\n",
            "Best val_acc So Far: 0.5605545043945312\n",
            "Total elapsed time: 15h 38m 46s\n",
            "\n",
            "Search: Running Trial #22\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "n_layers          |4                 |2                 \n",
            "block_0_maps      |256               |128               \n",
            "block_0_regular...|False             |False             \n",
            "block_0_activation|lrelu             |relu              \n",
            "block_1_maps      |192               |128               \n",
            "block_1_regular...|False             |False             \n",
            "block_1_activation|lrelu             |lrelu             \n",
            "block_2_maps      |192               |256               \n",
            "block_2_regular...|False             |True              \n",
            "block_2_activation|lrelu             |relu              \n",
            "block_3_maps      |192               |None              \n",
            "block_3_regular...|True              |None              \n",
            "block_3_activation|lrelu             |None              \n",
            "block_4_maps      |192               |None              \n",
            "block_4_regular...|True              |None              \n",
            "block_4_activation|lrelu             |None              \n",
            "tuner/epochs      |5                 |2                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |1                 |2                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/5\n",
            "      6/Unknown - 5s 220ms/step - loss: 9.4774 - acc: 0.3333WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0723s vs `on_train_batch_end` time: 0.1292s). Check your callbacks.\n",
            "6127/6127 [==============================] - 1399s 228ms/step - loss: 0.7507 - acc: 0.7416 - val_loss: 1.5073 - val_acc: 0.5427\n",
            "Epoch 2/5\n",
            "6127/6127 [==============================] - 1396s 228ms/step - loss: 0.5209 - acc: 0.8013 - val_loss: 1.4618 - val_acc: 0.5563\n",
            "Epoch 3/5\n",
            "6127/6127 [==============================] - 1396s 228ms/step - loss: 0.4601 - acc: 0.8232 - val_loss: 1.5912 - val_acc: 0.5687\n",
            "Epoch 4/5\n",
            "5816/6127 [===========================>..] - ETA: 1:08 - loss: 0.4187 - acc: 0.8392"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1T427A2_lUp"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('models/shl-resnet-gridsearch', 'zip', 'models/shl-resnet-gridsearch')\n",
        "files.download(zip_filename) # Download to control machine"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}