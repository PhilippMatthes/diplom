{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-deep-learning-timeseries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCjUOLzIZ0b"
      },
      "source": [
        "# Using a deep CNN to directly classify SHL timeseries data\n",
        "\n",
        "The following notebook contains code to classify SHL timeseries data with deep convolutional neural networks. This is devided into the following steps:\n",
        "\n",
        "1. Download the SHL dataset.\n",
        "2. Preprocess the SHL dataset into features and make it readable efficiently by our training engine.\n",
        "3. Define one or multiple ml models.\n",
        "4. Train the model(s) and utilize grid search to find the best configuration.\n",
        "5. Export the models and their training parameters for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aqsHfSGiSqN"
      },
      "source": [
        "## Step 1: Download the SHL Dataset\n",
        "\n",
        "The SHL dataset is very big, so we will need to free up some disk space on colab, first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpWbBKRvep4"
      },
      "source": [
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/torch\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/pystan\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/spacy\n",
        "!rm -rf /tensorflow-1.15.2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqN3gogwim4q"
      },
      "source": [
        "Next, get our base repo so that we can use predefined architectures and pretrained scalers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6b-V4Ewjxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a098699a-647f-41b8-d550-8af15be2082b"
      },
      "source": [
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 1833, done.\u001b[K\n",
            "remote: Counting objects: 100% (1170/1170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (799/799), done.\u001b[K\n",
            "remote: Total 1833 (delta 591), reused 842 (delta 311), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (1833/1833), 39.82 MiB | 26.44 MiB/s, done.\n",
            "Resolving deltas: 100% (967/967), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXfbxnbwixmp"
      },
      "source": [
        "Switch to our src dir for further processing. This command is specific to Google Colab, so it might not work on your local Jupyter Notebook instance.\n",
        "\n",
        "Additionally, we create the dataset dir in which our dataset will be downloaded next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL4FprvwmKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883b5f5c-4bcb-417d-b3b0-71b2bc954d2b"
      },
      "source": [
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZZQRX1jEnA"
      },
      "source": [
        "Download the SHL dataset from the shl server. This might take some time, on Google Colab its approx. 45 minutes. You can also mount your Google Drive if you have enough space available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWAbEF-wnYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52adf6fe-7d3c-4862-ec6e-20419e4c035a"
      },
      "source": [
        "!wget -nc -O shl-dataset/challenge-2019-user1_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-user1_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-user1_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-user1_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-23 09:18:07--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852446972 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_torso.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.45G  11.2MB/s    in 8m 30s  \n",
            "\n",
            "2021-08-23 09:26:38 (10.9 MB/s) - ‘shl-dataset/challenge-2019-user1_torso.zip’ saved [5852446972/5852446972]\n",
            "\n",
            "--2021-08-23 09:26:39--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5628524721 (5.2G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_bag.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.24G  11.2MB/s    in 8m 11s  \n",
            "\n",
            "2021-08-23 09:34:50 (10.9 MB/s) - ‘shl-dataset/challenge-2019-user1_bag.zip’ saved [5628524721/5628524721]\n",
            "\n",
            "--2021-08-23 09:34:50--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5871677913 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_hips.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.47G  11.2MB/s    in 8m 32s  \n",
            "\n",
            "2021-08-23 09:43:23 (10.9 MB/s) - ‘shl-dataset/challenge-2019-user1_hips.zip’ saved [5871677913/5871677913]\n",
            "\n",
            "--2021-08-23 09:43:24--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6040097130 (5.6G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-user1_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.62G  11.2MB/s    in 8m 58s  \n",
            "\n",
            "2021-08-23 09:52:23 (10.7 MB/s) - ‘shl-dataset/challenge-2020-user1_hand.zip’ saved [6040097130/6040097130]\n",
            "\n",
            "--2021-08-23 09:52:23--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3263492796 (3.0G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   3.04G  11.2MB/s    in 4m 51s  \n",
            "\n",
            "2021-08-23 09:57:15 (10.7 MB/s) - ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’ saved [3263492796/3263492796]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8C9qCcPjRwc"
      },
      "source": [
        "Next we unzip our dataset into the running instance's filestorage. *Note that this will probably not work for free subscriptions of Google Colab, since the data is approximately 90-100 GB when extracted.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJK91R8EAb0b",
        "outputId": "f8e64023-81a6-4cd3-cea5-71ae73e7ddb2"
      },
      "source": [
        "# Unzip training datasets\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_torso shl-dataset/challenge-2019-user1_torso.zip\n",
        "!rm shl-dataset/challenge-2019-user1_torso.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_bag shl-dataset/challenge-2019-user1_bag.zip\n",
        "!rm shl-dataset/challenge-2019-user1_bag.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_hips shl-dataset/challenge-2019-user1_hips.zip\n",
        "!rm shl-dataset/challenge-2019-user1_hips.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-user1_hand shl-dataset/challenge-2020-user1_hand.zip\n",
        "!rm shl-dataset/challenge-2020-user1_hand.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-users23_torso_bag_hips_hand shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
        "!rm shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  shl-dataset/challenge-2019-user1_torso.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_torso/train/Torso/\n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-user1_bag.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_bag/train/Bag/\n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-user1_hips.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_hips/train/Hips/\n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-user1_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-user1_hand/train/\n",
            "   creating: shl-dataset/challenge-2020-user1_hand/train/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/\n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Pressure.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz4yHwGyjq-U"
      },
      "source": [
        "## Step 2: Preprocess the data\n",
        "\n",
        "Explanations will from now on be inside the code, so that you can copy it without losing the contextual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF9LtE8PSAQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7979981c-45c2-4a67-b2aa-c90e47c548b7"
      },
      "source": [
        "# Check the CUDA version\n",
        "\n",
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZAJlKdu06f1",
        "outputId": "e7ffd3ea-0a82-42d8-f56d-bf8a3547d29b"
      },
      "source": [
        "# Change into our project src directory\n",
        "# Note: use this as an entrypoint when you already downloaded the dataset\n",
        "\n",
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMIdkUVR9-m",
        "outputId": "c86970c8-f0f1-4569-f5e1-5283256c078b"
      },
      "source": [
        "# Check configuration and hardware resources\n",
        "\n",
        "import distutils\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(f'Using TensorFlow: {tf.__version__}')\n",
        "\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "print('GPU Devices:')\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow: 2.6.0\n",
            "GPU Devices:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcVSnPAOENt"
      },
      "source": [
        "# Define all datasets to train our model on\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-user1_torso/train/Torso'),\n",
        "    Path('shl-dataset/challenge-2019-user1_bag/train/Bag'),\n",
        "    Path('shl-dataset/challenge-2019-user1_hips/train/Hips'),\n",
        "    Path('shl-dataset/challenge-2020-user1_hand/train/Hand'),\n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand'),   \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tYXzJj9OHGo"
      },
      "source": [
        "# Define more useful constants about our dataset\n",
        "\n",
        "LABEL_ORDER = [\n",
        "    'Null',\n",
        "    'Still',\n",
        "    'Walking',\n",
        "    'Run',\n",
        "    'Bike',\n",
        "    'Car',\n",
        "    'Bus',\n",
        "    'Train',\n",
        "    'Subway',\n",
        "]\n",
        "\n",
        "SAMPLE_LENGTH = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHfmi8ZOR1n"
      },
      "source": [
        "# Results from data analysis\n",
        "\n",
        "CLASS_WEIGHTS = {\n",
        "    0: 0.0, # NULL label\n",
        "    1: 1.0021671573438011, \n",
        "    2: 0.9985739895697523, \n",
        "    3: 2.8994439843842423, \n",
        "    4: 1.044135815617944, \n",
        "    5: 0.7723505499007343, \n",
        "    6: 0.8652474758172704, \n",
        "    7: 0.7842127155793044, \n",
        "    8: 1.0283208861290594\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAPsgtXOW3_"
      },
      "source": [
        "# Define features for our dataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Attributes to load from our dataset\n",
        "X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "    # Parts that are not needed:\n",
        "    # 'gra_x', 'gra_y', 'gra_z',\n",
        "    # 'lacc_x', 'lacc_y', 'lacc_z',\n",
        "    # 'ori_x', 'ori_y', 'ori_z', 'ori_w',\n",
        "]\n",
        "\n",
        "# Files within the dataset that contain our attributes\n",
        "X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    # Parts that are not needed:\n",
        "    # 'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    # 'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    # 'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "]\n",
        "\n",
        "# Features to generate from our loaded attributes\n",
        "# Note that `a` is going to be a dict of attribute tracks\n",
        "X_features = OrderedDict({\n",
        "    'acc_mag': lambda a: np.sqrt(a['acc_x']**2 + a['acc_y']**2 + a['acc_z']**2),\n",
        "    'mag_mag': lambda a: np.sqrt(a['mag_x']**2 + a['mag_y']**2 + a['mag_z']**2),\n",
        "    'gyr_mag': lambda a: np.sqrt(a['gyr_x']**2 + a['gyr_y']**2 + a['gyr_z']**2),\n",
        "})\n",
        "\n",
        "# Define where to find our labels for supervised learning\n",
        "y_file = 'Label.txt'\n",
        "y_attribute = 'labels'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzV0cHfZRmUh",
        "outputId": "e2ed8499-3935-4a90-b7c0-5fbdafc3b5b8"
      },
      "source": [
        "# Load pretrained power transformers for feature scaling\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_feature_scalers = OrderedDict({})\n",
        "for feature_name, _ in X_features.items():\n",
        "    scaler_dir = f'models/shl-scalers/{feature_name}.scaler.joblib'\n",
        "    scaler = joblib.load(scaler_dir)\n",
        "    scaler.copy = False # Save memory\n",
        "    X_feature_scalers[feature_name] = scaler\n",
        "    print(f'Loaded scaler from {scaler_dir}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded scaler from models/shl-scalers/acc_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/mag_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/gyr_mag.scaler.joblib.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PowerTransformer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_r8kxvWrtE",
        "outputId": "350add57-d48b-41d3-c487-fa91156c29ca"
      },
      "source": [
        "# Load the training and validation data into a high performance datatype\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from typing import Generator, List, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def read_chunks(\n",
        "    n_chunks: int, \n",
        "    X_attr_readers: List[pd.io.parsers.TextFileReader], \n",
        "    y_attr_reader: pd.io.parsers.TextFileReader\n",
        ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
        "    \"\"\"\n",
        "    Read chunks of attribute data and yield it to the caller as tuples of X, y.\n",
        "    \n",
        "    This function returns a generator which can be iterated.\n",
        "    \"\"\"\n",
        "    for _ in range(n_chunks):\n",
        "        # Load raw attribute tracks\n",
        "        X_raw_attrs = OrderedDict({})\n",
        "        for X_attribute, X_attr_reader in zip(X_attributes, X_attr_readers):\n",
        "            X_attr_track = next(X_attr_reader)\n",
        "            X_attr_track = np.nan_to_num(X_attr_track.to_numpy())\n",
        "            X_raw_attrs[X_attribute] = X_attr_track\n",
        "\n",
        "        # Calculate features\n",
        "        X_feature_tracks = None\n",
        "        for X_feature_name, X_feature_func in X_features.items():\n",
        "            X_feature_track = X_feature_func(X_raw_attrs)\n",
        "            X_feature_track = X_feature_scalers[X_feature_name] \\\n",
        "                .transform(X_feature_track)\n",
        "            if X_feature_tracks is None:\n",
        "                X_feature_tracks = X_feature_track\n",
        "            else:\n",
        "                X_feature_tracks = np.dstack((X_feature_tracks, X_feature_track))\n",
        "\n",
        "        # Load labels\n",
        "        y_attr_track = next(y_attr_reader) # dim (None, sample_length)\n",
        "        y_attr_track = np.nan_to_num(y_attr_track.to_numpy()) # dim (None, sample_length)\n",
        "        y_attr_track = y_attr_track[:, 0] # dim (None, 1)\n",
        "\n",
        "        yield X_feature_tracks, y_attr_track\n",
        "\n",
        "def count_samples(dataset_dir: Path) -> int:\n",
        "    \"\"\"Count the total amount of samples in a shl dataset.\"\"\"\n",
        "    n_samples = 0\n",
        "    # Every file in the dataset has the same length, use the labels file\n",
        "    with open(dataset_dir / y_file) as f:\n",
        "        for _ in tqdm(f, desc=f'Counting samples in {dataset_dir}'):\n",
        "            n_samples += 1\n",
        "    return n_samples\n",
        "\n",
        "def create_chunked_readers(\n",
        "    dataset_dir: Path,\n",
        "    chunksize: int, \n",
        "    xdtype=np.float32, # Use np.float16 with caution, can lead to overflows\n",
        "    ydtype=np.int\n",
        ") -> Tuple[List[pd.io.parsers.TextFileReader], pd.io.parsers.TextFileReader]:\n",
        "    \"\"\"Initialize chunked csv readers and return them to the caller as a tuple.\"\"\"\n",
        "    read_csv_kwargs = { 'sep': ' ', 'header': None, 'chunksize': chunksize }\n",
        "\n",
        "    X_attr_readers = [] # (dim datasets x readers)\n",
        "    for filename in X_files:\n",
        "        X_reader = pd.read_csv(dataset_dir / filename, dtype=xdtype, **read_csv_kwargs)\n",
        "        X_attr_readers.append(X_reader)\n",
        "    y_attr_reader = pd.read_csv(dataset_dir / y_file, dtype=ydtype, **read_csv_kwargs)\n",
        "\n",
        "    return X_attr_readers, y_attr_reader\n",
        "\n",
        "def export_tfrecords(\n",
        "    dataset_dir: Path,\n",
        "    n_chunks=16, # Load dataset in parts to not overload memory\n",
        "):\n",
        "    \"\"\"Transform the given shl dataset into a memory efficient TFRecord.\"\"\"\n",
        "    target_dir = f'{dataset_dir}.tfrecord'\n",
        "    if os.path.isfile(target_dir):\n",
        "        print(f'{target_dir} already exists.')\n",
        "        return\n",
        "\n",
        "    print(f'Exporting to {target_dir}.')\n",
        "\n",
        "    n_samples = count_samples(dataset_dir)\n",
        "    chunksize = int(np.floor(n_samples / n_chunks))\n",
        "    X_attr_readers, y_attr_reader = create_chunked_readers(dataset_dir, chunksize)    \n",
        "\n",
        "    with tf.io.TFRecordWriter(str(target_dir)) as file_writer:\n",
        "        with tqdm(total=n_samples, desc=f'Reading samples to {target_dir}') as pbar:\n",
        "            for X_feature_tracks, y_attr_track in read_chunks(\n",
        "                n_chunks, X_attr_readers, y_attr_reader\n",
        "            ):\n",
        "                for X, y in zip(X_feature_tracks, y_attr_track):\n",
        "                    X_flat = X.flatten() # TFRecords don't support multidimensional arrays\n",
        "                    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
        "                        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])) \n",
        "                    })).SerializeToString()\n",
        "                    file_writer.write(record_bytes)\n",
        "                pbar.update(chunksize)\n",
        "\n",
        "for dataset_dir in TRAIN_DATASET_DIRS:\n",
        "    export_tfrecords(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-user1_torso/train/Torso: 196072it [00:02, 73033.40it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord: 100%|█████████▉| 196064/196072 [04:17<00:00, 762.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-user1_bag/train/Bag: 196072it [00:02, 72501.94it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord: 100%|█████████▉| 196064/196072 [04:19<00:00, 756.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-user1_hips/train/Hips: 196072it [00:02, 72529.54it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord: 100%|█████████▉| 196064/196072 [04:21<00:00, 749.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-user1_hand/train/Hand: 196072it [00:02, 72518.70it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord: 100%|█████████▉| 196064/196072 [04:23<00:00, 743.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso: 28789it [00:00, 90447.34it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 675.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag: 28789it [00:00, 91870.51it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 677.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips: 28789it [00:00, 89658.63it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 675.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand: 28789it [00:00, 91186.93it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 674.52it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weYDZUofD4TD",
        "outputId": "a5b437fa-210b-4781-f4ac-a38e66639110"
      },
      "source": [
        "def decode_tfrecord(record_bytes) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Decode a TFRecord example to X, y from its serialized representation.\"\"\"\n",
        "    example = tf.io.parse_single_example(record_bytes, {\n",
        "        'X': tf.io.FixedLenFeature([SAMPLE_LENGTH, len(X_features)], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([1], tf.int64)\n",
        "    })\n",
        "    return example['X'], example['y']\n",
        "\n",
        "def create_train_validation_datasets(\n",
        "    dataset_dirs: List[Path], \n",
        "    batch_size=64,\n",
        "    shuffle_size=20_000, # Must be larger than batch_size\n",
        "    test_size=256 # In batches\n",
        ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "    \"\"\"\n",
        "    Create interleaved, shuffled and batched train and \n",
        "    validation datasets from the dataset dirs.\n",
        "    \n",
        "    Note that this function reads previously generated TFRecords under \n",
        "    `dataset_dir.tfrecord` -> use `export_tfrecords` for that.\n",
        "    \"\"\"\n",
        "    tfrecord_dirs = [f'{d}.tfrecord' for d in dataset_dirs]\n",
        "    print(f'Creating train and validation dataset over {tfrecord_dirs}.')\n",
        "\n",
        "    # Create a strategy to interleave the datasets\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_dirs) \\\n",
        "        .interleave(\n",
        "            lambda x: tf.data.TFRecordDataset(x), \n",
        "            cycle_length=batch_size, # Number of input elements that are processed concurrently\n",
        "            block_length=1 # Return only one element at a time, batching is done later\n",
        "        ) \\\n",
        "        .shuffle(shuffle_size) \\\n",
        "        .map(decode_tfrecord, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(batch_size)\n",
        "    count = sum(1 for _ in dataset)\n",
        "    print(f'Counted {count * batch_size} samples in combined dataset.')\n",
        "    training_dataset = dataset.skip(test_size)\n",
        "    count = sum(1 for _ in training_dataset)\n",
        "    print(f'Counted {count * batch_size} samples in training dataset.')\n",
        "    validation_dataset = dataset.take(test_size)\n",
        "    count = sum(1 for _ in validation_dataset)\n",
        "    print(f'Counted {count * batch_size} samples in validation dataset.')\n",
        "    return training_dataset, validation_dataset\n",
        "\n",
        "train_dataset, validation_dataset = create_train_validation_datasets(TRAIN_DATASET_DIRS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating train and validation dataset over ['shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord', 'shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord', 'shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord', 'shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord'].\n",
            "Counted 899392 samples in combined dataset.\n",
            "Counted 883008 samples in training dataset.\n",
            "Counted 16384 samples in validation dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haZF5N7ikLbC"
      },
      "source": [
        "## Steps 3-5: Defining, training and evaluating models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXmk4SKK_LCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "185a6e50-c0f4-4823-f045-3bae85860b88"
      },
      "source": [
        "# We will use the keras tuner contribution package for a hyperparameter gridsearch\n",
        "\n",
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install keras-tuner -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 4.5 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80SUHOW_ZrLV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ec149a-6e4b-416d-bec4-dd019512b143"
      },
      "source": [
        "# Mount Google Drive for progress logging\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3dP3vAlxe9r"
      },
      "source": [
        "import tempfile\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from keras_tuner import Hyperband\n",
        "from keras_tuner.engine import hypermodel as hm_module\n",
        "from keras_tuner.engine.logger import Logger\n",
        "\n",
        "class Tuner(Hyperband):\n",
        "    \"\"\"\n",
        "    A custom hyperband tuner.\n",
        "    \"\"\"\n",
        "    def __init__(self, gridsearch_dir: Path, *init_args, **init_kwargs):\n",
        "        self.gridsearch_dir = gridsearch_dir\n",
        "        super().__init__(*init_args, **init_kwargs)\n",
        "\n",
        "    def run_trial(self, trial, *fit_args, **fit_kwargs):\n",
        "        \"\"\"\n",
        "        Zip our progress and save to Google Drive every time a trial is run.\n",
        "        \"\"\"\n",
        "        with tempfile.TemporaryDirectory() as tempdir:\n",
        "            # Copy all files (except the checkpoints which become very large)\n",
        "            # to a temporary directory and zip them, then download\n",
        "            files_to_ignore = shutil.ignore_patterns('checkpoints*')\n",
        "            target = f'{tempdir}/gridsearch'\n",
        "            shutil.copytree(self.gridsearch_dir, target, ignore=files_to_ignore)\n",
        "            shutil.make_archive('models/gridsearch', 'zip', target)\n",
        "            datestr = datetime.today().strftime('%Y-%m-%d')\n",
        "            shutil.copyfile('models/gridsearch.zip', f'/content/drive/MyDrive/gridsearch-{datestr}.zip')\n",
        "        super().run_trial(trial, *fit_args, **fit_kwargs)\n",
        "\n",
        "    def _on_train_begin(self, model, hp, *fit_args, **fit_kwargs):\n",
        "        \"\"\"\n",
        "        Circumvent an issue in the implementation of the Hyperband keras tuner - \n",
        "        Models seem to  start from cold every new epoch, which is clearly unwanted. \n",
        "\n",
        "        See: https://github.com/keras-team/keras-tuner/issues/372\n",
        "        And: https://arxiv.org/pdf/1603.06560.pdf\n",
        "        \"\"\"\n",
        "        prev_trial_id = hp.values['tuner/trial_id'] if 'tuner/trial_id' in hp else None\n",
        "        if prev_trial_id:\n",
        "            prev_trial = self.oracle.trials[prev_trial_id]\n",
        "            best_epoch = prev_trial.best_step\n",
        "            # the code below is from load_model method of Tuner class\n",
        "            with hm_module.maybe_distribute(self.distribution_strategy):\n",
        "                model.load_weights(self._get_checkpoint_fname(\n",
        "                    prev_trial.trial_id, best_epoch\n",
        "                ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U302m77jRPO6"
      },
      "source": [
        "# We will use the kapre contribution package to include STFT layers\n",
        "\n",
        "!{sys.executable} -m pip install kapre -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra8ELaQbD_vj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from keras_tuner import HyperParameters\n",
        "from keras_tuner.applications import HyperResNet\n",
        "\n",
        "class HyperResNet2D(HyperResNet):\n",
        "    \"\"\"\n",
        "    A ResNet hypermodel based on 2D convolutions.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "class HyperResNet1D(HyperResNet):\n",
        "    \"\"\"\n",
        "    A ResNet hypermodel based on 1D convolutions.\n",
        "    \n",
        "    The code of this class is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "    \"\"\"\n",
        "\n",
        "    def build(self, hp: HyperParameters):\n",
        "        version = hp.Choice(\"version\", [\"v1\", \"v2\", \"next\"], default=\"v2\")\n",
        "        conv3_depth = hp.Choice(\"conv3_depth\", [4, 8])\n",
        "        conv4_depth = hp.Choice(\"conv4_depth\", [6, 23, 36])\n",
        "\n",
        "        # Version-conditional fixed parameters\n",
        "        preact = True if version == \"v2\" else False\n",
        "        use_bias = False if version == \"next\" else True\n",
        "\n",
        "        # Model definition.\n",
        "        bn_axis = 2 # normalize feature axis, i.e. (n_batches, n_timesteps, n_features)\n",
        "\n",
        "        if self.input_tensor is not None:\n",
        "            inputs = tf.keras.utils.get_source_inputs(self.input_tensor)\n",
        "            x = self.input_tensor\n",
        "        else:\n",
        "            inputs = layers.Input(shape=self.input_shape)\n",
        "            x = inputs\n",
        "\n",
        "        # Initial conv1d block.\n",
        "        x = layers.ZeroPadding1D(padding=3, name=\"conv1_pad\")(x)\n",
        "        x = layers.Conv1D(64, 7, strides=2, use_bias=use_bias, name=\"conv1_conv\")(x)\n",
        "        if preact is False:\n",
        "            x = layers.BatchNormalization(\n",
        "                axis=bn_axis, epsilon=1.001e-5, name=\"conv1_bn\"\n",
        "            )(x)\n",
        "            x = layers.Activation(\"relu\", name=\"conv1_relu\")(x)\n",
        "        x = layers.ZeroPadding1D(padding=1, name=\"pool1_pad\")(x)\n",
        "        x = layers.MaxPooling1D(3, strides=2, name=\"pool1_pool\")(x)\n",
        "\n",
        "        # Middle hypertunable stack.\n",
        "        if version == \"v1\":\n",
        "            x = stack1(x, 64, 3, stride1=1, name=\"conv2\")\n",
        "            x = stack1(x, 128, conv3_depth, name=\"conv3\")\n",
        "            x = stack1(x, 256, conv4_depth, name=\"conv4\")\n",
        "            x = stack1(x, 512, 3, name=\"conv5\")\n",
        "        elif version == \"v2\":\n",
        "            x = stack2(x, 64, 3, name=\"conv2\")\n",
        "            x = stack2(x, 128, conv3_depth, name=\"conv3\")\n",
        "            x = stack2(x, 256, conv4_depth, name=\"conv4\")\n",
        "            x = stack2(x, 512, 3, stride1=1, name=\"conv5\")\n",
        "        elif version == \"next\":\n",
        "            x = stack3(x, 64, 3, name=\"conv2\")\n",
        "            x = stack3(x, 256, conv3_depth, name=\"conv3\")\n",
        "            x = stack3(x, 512, conv4_depth, name=\"conv4\")\n",
        "            x = stack3(x, 1024, 3, stride1=1, name=\"conv5\")\n",
        "\n",
        "        # Top of the model.\n",
        "        if preact is True:\n",
        "            x = layers.BatchNormalization(\n",
        "                axis=bn_axis, epsilon=1.001e-5, name=\"post_bn\"\n",
        "            )(x)\n",
        "            x = layers.Activation(\"relu\", name=\"post_relu\")(x)\n",
        "\n",
        "        pooling = hp.Choice(\"pooling\", [\"avg\", \"max\"], default=\"avg\")\n",
        "        if pooling == \"avg\":\n",
        "            x = layers.GlobalAveragePooling1D(name=\"avg_pool\")(x)\n",
        "        elif pooling == \"max\":\n",
        "            x = layers.GlobalMaxPooling1D(name=\"max_pool\")(x)\n",
        "\n",
        "        if self.include_top:\n",
        "            x = layers.Dense(self.classes, activation=\"softmax\", name=\"probs\")(x)\n",
        "            model = keras.Model(inputs, x, name=\"ResNet\")\n",
        "            optimizer_name = hp.Choice(\n",
        "                \"optimizer\", [\"adam\", \"rmsprop\", \"sgd\"], default=\"adam\"\n",
        "            )\n",
        "            optimizer = keras.optimizers.get(optimizer_name)\n",
        "            optimizer.learning_rate = hp.Choice(\n",
        "                \"learning_rate\", [0.1, 0.01, 0.001], default=0.01\n",
        "            )\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=[\"accuracy\"],\n",
        "            )\n",
        "            return model\n",
        "        else:\n",
        "            return keras.Model(inputs, x, name=\"ResNet\")\n",
        "\n",
        "\n",
        "def block1(x, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\n",
        "    \"\"\"\n",
        "    A residual block.\n",
        "\n",
        "    The code of this function is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Args:\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer.\n",
        "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
        "        stride: default 1, stride of the first layer.\n",
        "        conv_shortcut: default True, use convolution shortcut if True,\n",
        "            otherwise identity shortcut.\n",
        "        name: string, block label.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor for the residual block.\n",
        "    \"\"\"\n",
        "    bn_axis = 2 # normalize feature axis, i.e. (n_batches, n_timesteps, n_features)\n",
        "\n",
        "    if conv_shortcut is True:\n",
        "        shortcut = layers.Conv1D(\n",
        "            4 * filters, 1, strides=stride, name=name + \"_0_conv\"\n",
        "        )(x)\n",
        "        shortcut = layers.BatchNormalization(\n",
        "            axis=bn_axis, epsilon=1.001e-5, name=name + \"_0_bn\"\n",
        "        )(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = layers.Conv1D(filters, 1, strides=stride, name=name + \"_1_conv\")(x)\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_1_bn\"\n",
        "    )(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_1_relu\")(x)\n",
        "\n",
        "    x = layers.Conv1D(filters, kernel_size, padding=\"same\", name=name + \"_2_conv\")(x)\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_2_bn\"\n",
        "    )(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_2_relu\")(x)\n",
        "\n",
        "    x = layers.Conv1D(4 * filters, 1, name=name + \"_3_conv\")(x)\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_3_bn\"\n",
        "    )(x)\n",
        "\n",
        "    x = layers.Add(name=name + \"_add\")([shortcut, x])\n",
        "    x = layers.Activation(\"relu\", name=name + \"_out\")(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def stack1(x, filters, blocks, stride1=2, name=None):\n",
        "    \"\"\"\n",
        "    A set of stacked residual blocks.\n",
        "\n",
        "    The code of this function is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Args:\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer in a block.\n",
        "        blocks: integer, blocks in the stacked blocks.\n",
        "        stride1: default 2, stride of the first layer in the first block.\n",
        "        name: string, stack label.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor for the stacked blocks.\n",
        "    \"\"\"\n",
        "    x = block1(x, filters, stride=stride1, name=name + \"_block1\")\n",
        "    for i in range(2, blocks + 1):\n",
        "        x = block1(x, filters, conv_shortcut=False, name=name + \"_block\" + str(i))\n",
        "    return x\n",
        "\n",
        "\n",
        "def block2(x, filters, kernel_size=3, stride=1, conv_shortcut=False, name=None):\n",
        "    \"\"\"\n",
        "    A residual block.\n",
        "\n",
        "    The code of this function is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Args:\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer.\n",
        "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
        "        stride: default 1, stride of the first layer.\n",
        "        conv_shortcut: default False, use convolution shortcut if True,\n",
        "            otherwise identity shortcut.\n",
        "        name: string, block label.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor for the residual block.\n",
        "    \"\"\"\n",
        "    bn_axis = 2 # normalize feature axis, i.e. (n_batches, n_timesteps, n_features)\n",
        "\n",
        "    preact = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_preact_bn\"\n",
        "    )(x)\n",
        "    preact = layers.Activation(\"relu\", name=name + \"_preact_relu\")(preact)\n",
        "\n",
        "    if conv_shortcut is True:\n",
        "        shortcut = layers.Conv1D(\n",
        "            4 * filters, 1, strides=stride, name=name + \"_0_conv\"\n",
        "        )(preact)\n",
        "    else:\n",
        "        shortcut = layers.MaxPooling1D(1, strides=stride)(x) if stride > 1 else x\n",
        "\n",
        "    x = layers.Conv1D(filters, 1, strides=1, use_bias=False, name=name + \"_1_conv\")(\n",
        "        preact\n",
        "    )\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_1_bn\"\n",
        "    )(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_1_relu\")(x)\n",
        "\n",
        "    x = layers.ZeroPadding1D(padding=1, name=name + \"_2_pad\")(x)\n",
        "    x = layers.Conv1D(\n",
        "        filters, kernel_size, strides=stride, use_bias=False, name=name + \"_2_conv\"\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_2_bn\"\n",
        "    )(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_2_relu\")(x)\n",
        "\n",
        "    x = layers.Conv1D(4 * filters, 1, name=name + \"_3_conv\")(x)\n",
        "    x = layers.Add(name=name + \"_out\")([shortcut, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def stack2(x, filters, blocks, stride1=2, name=None):\n",
        "    \"\"\"\n",
        "    A set of stacked residual blocks.\n",
        "\n",
        "    The code of this function is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Args:\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer in a block.\n",
        "        blocks: integer, blocks in the stacked blocks.\n",
        "        stride1: default 2, stride of the first layer in the first block.\n",
        "        name: string, stack label.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor for the stacked blocks.\n",
        "    \"\"\"\n",
        "    x = block2(x, filters, conv_shortcut=True, name=name + \"_block1\")\n",
        "    for i in range(2, blocks):\n",
        "        x = block2(x, filters, name=name + \"_block\" + str(i))\n",
        "    x = block2(x, filters, stride=stride1, name=name + \"_block\" + str(blocks))\n",
        "    return x\n",
        "\n",
        "\n",
        "def block3(\n",
        "    x, filters, kernel_size=3, stride=1, groups=32, conv_shortcut=True, name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    A residual block.\n",
        "\n",
        "    The code of this function is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Args:\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer.\n",
        "        kernel_size: default 3, kernel size of the bottleneck layer.\n",
        "        stride: default 1, stride of the first layer.\n",
        "        groups: default 32, group size for grouped convolution.\n",
        "        conv_shortcut: default True, use convolution shortcut if True,\n",
        "            otherwise identity shortcut.\n",
        "        name: string, block label.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor for the residual block.\n",
        "    \"\"\"\n",
        "    bn_axis = 2 # normalize feature axis, i.e. (n_batches, n_timesteps, n_features)\n",
        "\n",
        "    if conv_shortcut is True:\n",
        "        shortcut = layers.Conv1D(\n",
        "            (64 // groups) * filters,\n",
        "            1,\n",
        "            strides=stride,\n",
        "            use_bias=False,\n",
        "            name=name + \"_0_conv\",\n",
        "        )(x)\n",
        "        shortcut = layers.BatchNormalization(\n",
        "            axis=bn_axis, epsilon=1.001e-5, name=name + \"_0_bn\"\n",
        "        )(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    x = layers.Conv1D(filters, 1, use_bias=False, name=name + \"_1_conv\")(x)\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_1_bn\"\n",
        "    )(x)\n",
        "    x = layers.Activation(\"relu\", name=name + \"_1_relu\")(x)\n",
        "\n",
        "    c = filters // groups\n",
        "    x = layers.ZeroPadding1D(padding=1, name=name + \"_2_pad\")(x)\n",
        "    # We use SeparableConv1D instead of DepthwiseConv1D because that is\n",
        "    # only available in the nightly build of tensorflow (as of August 23, 2021)\n",
        "    x = layers.SeparableConv1D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=stride,\n",
        "        depth_multiplier=c,\n",
        "        use_bias=False,\n",
        "        name=name + \"_2_conv\",\n",
        "    )(x)\n",
        "    x_shape = backend.int_shape(x)[1:-1]\n",
        "    x = layers.Reshape(x_shape + (groups, c, c))(x)\n",
        "    output_shape = x_shape + (groups, c) if backend.backend() == \"theano\" else None\n",
        "\n",
        "    x = layers.Lambda(\n",
        "        lambda x: sum([x[:, :, :, :, i] for i in range(c)]),\n",
        "        output_shape=output_shape,\n",
        "        name=name + \"_2_reduce\",\n",
        "    )(x)\n",
        "\n",
        "    x = layers.Reshape(x_shape + (filters,))(x)\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_2_bn\"\n",
        "    )(x)\n",
        "\n",
        "    x = layers.Activation(\"relu\", name=name + \"_2_relu\")(x)\n",
        "\n",
        "    x = layers.Conv1D(\n",
        "        (64 // groups) * filters, 1, use_bias=False, name=name + \"_3_conv\"\n",
        "    )(x)\n",
        "\n",
        "    x = layers.BatchNormalization(\n",
        "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_3_bn\"\n",
        "    )(x)\n",
        "\n",
        "    x = layers.Add(name=name + \"_add\")([shortcut, x])\n",
        "    x = layers.Activation(\"relu\", name=name + \"_out\")(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def stack3(x, filters, blocks, stride1=2, groups=32, name=None):\n",
        "    \"\"\"\n",
        "    A set of stacked residual blocks.\n",
        "\n",
        "    The code of this function is based on https://github.com/keras-team/keras-tuner\n",
        "    which is licensed under Apache License 2.0, see https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "    Args:\n",
        "        x: input tensor.\n",
        "        filters: integer, filters of the bottleneck layer in a block.\n",
        "        blocks: integer, blocks in the stacked blocks.\n",
        "        stride1: default 2, stride of the first layer in the first block.\n",
        "        groups: default 32, group size for grouped convolution.\n",
        "        name: string, stack label.\n",
        "\n",
        "    Returns:\n",
        "        Output tensor for the stacked blocks.\n",
        "    \"\"\"\n",
        "    x = block3(x, filters, stride=stride1, groups=groups, name=name + \"_block1\")\n",
        "\n",
        "    for i in range(2, blocks + 1):\n",
        "        x = block3(\n",
        "            x,\n",
        "            filters,\n",
        "            groups=groups,\n",
        "            conv_shortcut=False,\n",
        "            name=name + \"_block\" + str(i),\n",
        "        )\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Xd0mWdi5ri"
      },
      "source": [
        "# Create a combined hypermodel\n",
        "\n",
        "import kapre\n",
        "\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "def combined_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"\n",
        "    Make a combined resnet hypermodel.\n",
        "    \n",
        "    Use either a 1D model to directly classify the timeseries data,\n",
        "    or use a 2D model on a preceding STFT transform layer.\n",
        "    \"\"\"\n",
        "\n",
        "    model_type = hp.Choice('model_type', ['1d', '2d'])\n",
        "\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "    n_outputs = len(LABEL_ORDER)\n",
        "\n",
        "    model = models.Sequential()\n",
        "\n",
        "    if model_type == '1d':\n",
        "        with hp.conditional_scope('model_type', ['1d']):\n",
        "            # Direct timeseries classification\n",
        "            model.add(HyperResNet1D(\n",
        "                include_top=True,\n",
        "                input_shape=input_shape,\n",
        "                input_tensor=None,\n",
        "                classes=n_outputs\n",
        "            ).build(hp))\n",
        "    elif model_type == '2d':\n",
        "        with hp.conditional_scope('model_type', ['2d']):\n",
        "            # Short-time fourier transform\n",
        "            model.add(kapre.STFT( \n",
        "                n_fft=100,\n",
        "                hop_length=5,\n",
        "                pad_end=False,\n",
        "                input_data_format='channels_last', \n",
        "                output_data_format='channels_last',\n",
        "                input_shape=input_shape,\n",
        "                name='stft-layer'\n",
        "            ))\n",
        "            # Convert resulting tensor into magnitudes (decibel)\n",
        "            model.add(kapre.Magnitude())\n",
        "            model.add(kapre.MagnitudeToDecibel())\n",
        "            # Normalize magnitudes\n",
        "            model.add(layers.LayerNormalization())\n",
        "            model.add(layers.UpSampling2D(2))\n",
        "            # Add our ResNet classifier hypermodel\n",
        "            model.add(HyperResNet2D(\n",
        "                include_top=True, \n",
        "                input_shape=(162, 102, 3), # Output shape of our upsampled STFT layer\n",
        "                input_tensor=None, \n",
        "                classes=n_outputs\n",
        "            ).build(hp))\n",
        "    else:\n",
        "        raise ValueError('Unknown meta architecture!')\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy', # No OHE necessary\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOH7F65U7fYx",
        "outputId": "745c33e2-ed54-4f60-d157-232eba537ba3"
      },
      "source": [
        "tuner = Tuner(\n",
        "    gridsearch_dir=Path('models/shl-resnet-gridsearch'),\n",
        "    hypermodel=combined_hypermodel, \n",
        "    objective='val_acc', \n",
        "    max_epochs=15,\n",
        "    overwrite=False,\n",
        "    directory='models',\n",
        "    project_name='shl-resnet-gridsearch',\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 7\n",
            "model_type (Choice)\n",
            "{'default': '1d', 'conditions': [], 'values': ['1d', '2d'], 'ordered': False}\n",
            "version (Choice)\n",
            "{'default': 'v2', 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['1d']}}], 'values': ['v1', 'v2', 'next'], 'ordered': False}\n",
            "conv3_depth (Choice)\n",
            "{'default': 4, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['1d']}}], 'values': [4, 8], 'ordered': True}\n",
            "conv4_depth (Choice)\n",
            "{'default': 6, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['1d']}}], 'values': [6, 23, 36], 'ordered': True}\n",
            "pooling (Choice)\n",
            "{'default': 'avg', 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['1d']}}], 'values': ['avg', 'max'], 'ordered': False}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['1d']}}], 'values': ['adam', 'rmsprop', 'sgd'], 'ordered': False}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['1d']}}], 'values': [0.1, 0.01, 0.001], 'ordered': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmEnc_5l8Wln"
      },
      "source": [
        "# Define callbacks for our training\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "decay_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_acc',\n",
        "    factor=0.5, \n",
        "    patience=5, # Epochs\n",
        "    min_lr=0.00001, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "stop_early = callbacks.EarlyStopping(\n",
        "    monitor='val_acc', \n",
        "    patience=10, # Epochs\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "update_tensorboard = callbacks.TensorBoard('logs/gridsearch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haG_mrDCVuCQ"
      },
      "source": [
        "# Activate TensorBoard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/gridsearch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX8Z0ZyW-nJ9",
        "outputId": "8a0e03e9-7c9a-4853-fc9e-c88a5f385be9"
      },
      "source": [
        "# Keras tuner grid search training\n",
        "\n",
        "tuner.search(\n",
        "    train_dataset,\n",
        "    epochs=15,\n",
        "    callbacks=[decay_lr, stop_early],\n",
        "    validation_data=validation_dataset,\n",
        "    verbose=1,\n",
        "    shuffle=False, # Shuffling doesn't work with our prefetching\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #7\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "model_type        |2d                |2d                \n",
            "version           |next              |v2                \n",
            "conv3_depth       |4                 |4                 \n",
            "conv4_depth       |6                 |6                 \n",
            "pooling           |avg               |avg               \n",
            "optimizer         |sgd               |adam              \n",
            "learning_rate     |0.001             |0.01              \n",
            "tuner/epochs      |2                 |2                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |2                 |2                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/2\n",
            "      6/Unknown - 15s 404ms/step - loss: 4.3018 - acc: 0.2370WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1743s vs `on_train_batch_end` time: 0.1914s). Check your callbacks.\n",
            "   4017/Unknown - 1633s 403ms/step - loss: 0.5477 - acc: 0.7942"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}