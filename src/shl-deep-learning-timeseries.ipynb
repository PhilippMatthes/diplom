{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-deep-learning-timeseries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCjUOLzIZ0b"
      },
      "source": [
        "# Using a deep CNN to directly classify SHL timeseries data\n",
        "\n",
        "The following notebook contains code to classify SHL timeseries data with deep convolutional neural networks. This is devided into the following steps:\n",
        "\n",
        "1. Download the SHL dataset.\n",
        "2. Preprocess the SHL dataset into features and make it readable efficiently by our training engine.\n",
        "3. Define one or multiple ml models.\n",
        "4. Train the model(s) and utilize grid search to find the best configuration.\n",
        "5. Export the models and their training parameters for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aqsHfSGiSqN"
      },
      "source": [
        "## Step 1: Download the SHL Dataset\n",
        "\n",
        "The SHL dataset is very big, so we will need to free up some disk space on colab, first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpWbBKRvep4"
      },
      "source": [
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/torch\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/pystan\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/spacy\n",
        "!rm -rf /tensorflow-1.15.2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqN3gogwim4q"
      },
      "source": [
        "Next, get our base repo so that we can use predefined architectures and pretrained scalers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6b-V4Ewjxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512c86de-07a6-410b-cf30-eef0a8beb803"
      },
      "source": [
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 1767, done.\u001b[K\n",
            "remote: Counting objects: 100% (1104/1104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (746/746), done.\u001b[K\n",
            "remote: Total 1767 (delta 551), reused 817 (delta 301), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (1767/1767), 34.63 MiB | 25.06 MiB/s, done.\n",
            "Resolving deltas: 100% (927/927), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXfbxnbwixmp"
      },
      "source": [
        "Switch to our src dir for further processing. This command is specific to Google Colab, so it might not work on your local Jupyter Notebook instance.\n",
        "\n",
        "Additionally, we create the dataset dir in which our dataset will be downloaded next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL4FprvwmKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ff180b-6c86-4d22-a225-b03ba5c28319"
      },
      "source": [
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZZQRX1jEnA"
      },
      "source": [
        "Download the SHL dataset from the shl server. This might take some time, on Google Colab its approx. 45 minutes. You can also mount your Google Drive if you have enough space available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWAbEF-wnYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539169e7-32bd-4a44-9114-9cc6a9c06d7f"
      },
      "source": [
        "!wget -nc -O shl-dataset/challenge-2019-user1_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-user1_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-user1_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-user1_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-20 06:52:11--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852446972 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_torso.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.45G  9.37MB/s    in 10m 5s  \n",
            "\n",
            "2021-08-20 07:02:17 (9.22 MB/s) - ‘shl-dataset/challenge-2019-user1_torso.zip’ saved [5852446972/5852446972]\n",
            "\n",
            "--2021-08-20 07:02:17--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5628524721 (5.2G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_bag.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.24G  9.44MB/s    in 9m 33s  \n",
            "\n",
            "2021-08-20 07:11:51 (9.37 MB/s) - ‘shl-dataset/challenge-2019-user1_bag.zip’ saved [5628524721/5628524721]\n",
            "\n",
            "--2021-08-20 07:11:51--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5871677913 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_hips.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.47G  9.60MB/s    in 9m 58s  \n",
            "\n",
            "2021-08-20 07:21:51 (9.36 MB/s) - ‘shl-dataset/challenge-2019-user1_hips.zip’ saved [5871677913/5871677913]\n",
            "\n",
            "--2021-08-20 07:21:51--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6040097130 (5.6G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-user1_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.62G  8.86MB/s    in 10m 37s \n",
            "\n",
            "2021-08-20 07:32:30 (9.04 MB/s) - ‘shl-dataset/challenge-2020-user1_hand.zip’ saved [6040097130/6040097130]\n",
            "\n",
            "--2021-08-20 07:32:30--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3263492796 (3.0G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   3.04G  9.62MB/s    in 5m 30s  \n",
            "\n",
            "2021-08-20 07:38:02 (9.42 MB/s) - ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’ saved [3263492796/3263492796]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8C9qCcPjRwc"
      },
      "source": [
        "Next we unzip our dataset into the running instance's filestorage. *Note that this will probably not work for free subscriptions of Google Colab, since the data is approximately 90-100 GB when extracted.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJK91R8EAb0b",
        "outputId": "3333fd4a-47b5-475f-f8fe-a57640fbeccf"
      },
      "source": [
        "# Unzip training datasets\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_torso shl-dataset/challenge-2019-user1_torso.zip\n",
        "!rm shl-dataset/challenge-2019-user1_torso.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_bag shl-dataset/challenge-2019-user1_bag.zip\n",
        "!rm shl-dataset/challenge-2019-user1_bag.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_hips shl-dataset/challenge-2019-user1_hips.zip\n",
        "!rm shl-dataset/challenge-2019-user1_hips.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-user1_hand shl-dataset/challenge-2020-user1_hand.zip\n",
        "!rm shl-dataset/challenge-2020-user1_hand.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-users23_torso_bag_hips_hand shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
        "!rm shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  shl-dataset/challenge-2019-user1_torso.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_torso/train/Torso/\n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-user1_bag.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_bag/train/Bag/\n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-user1_hips.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_hips/train/Hips/\n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-user1_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-user1_hand/train/\n",
            "   creating: shl-dataset/challenge-2020-user1_hand/train/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
            "   creating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/\n",
            "   creating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/\n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/\n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/\n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/\n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso/Pressure.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz4yHwGyjq-U"
      },
      "source": [
        "## Step 2: Preprocess the data\n",
        "\n",
        "Explanations will from now on be inside the code, so that you can copy it without losing the contextual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZAJlKdu06f1",
        "outputId": "35e6d9e4-d107-4fe1-d6f7-7a55cef9118e"
      },
      "source": [
        "# Change into our project src directory and select the TensorFlow version\n",
        "# Note: use this as an entrypoint when you already downloaded the dataset\n",
        "\n",
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMIdkUVR9-m",
        "outputId": "578b6034-4e5f-4d2b-f1a3-c35abff9911c"
      },
      "source": [
        "# Check configuration and hardware resources\n",
        "\n",
        "import distutils\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcVSnPAOENt"
      },
      "source": [
        "# Define all datasets to train our model on\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-user1_torso/train/Torso'),\n",
        "    Path('shl-dataset/challenge-2019-user1_bag/train/Bag'),\n",
        "    Path('shl-dataset/challenge-2019-user1_hips/train/Hips'),\n",
        "    Path('shl-dataset/challenge-2020-user1_hand/train/Hand'),\n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand'),   \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tYXzJj9OHGo"
      },
      "source": [
        "# Define more useful constants about our dataset\n",
        "\n",
        "LABEL_ORDER = [\n",
        "    'Null',\n",
        "    'Still',\n",
        "    'Walking',\n",
        "    'Run',\n",
        "    'Bike',\n",
        "    'Car',\n",
        "    'Bus',\n",
        "    'Train',\n",
        "    'Subway',\n",
        "]\n",
        "\n",
        "SAMPLE_LENGTH = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHfmi8ZOR1n"
      },
      "source": [
        "# Results from data analysis\n",
        "\n",
        "CLASS_WEIGHTS = {\n",
        "    0: 0.0, # NULL label\n",
        "    1: 1.0021671573438011, \n",
        "    2: 0.9985739895697523, \n",
        "    3: 2.8994439843842423, \n",
        "    4: 1.044135815617944, \n",
        "    5: 0.7723505499007343, \n",
        "    6: 0.8652474758172704, \n",
        "    7: 0.7842127155793044, \n",
        "    8: 1.0283208861290594\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAPsgtXOW3_"
      },
      "source": [
        "# Define features for our dataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Attributes to load from our dataset\n",
        "X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "    # Parts that are not needed:\n",
        "    # 'gra_x', 'gra_y', 'gra_z',\n",
        "    # 'lacc_x', 'lacc_y', 'lacc_z',\n",
        "    # 'ori_x', 'ori_y', 'ori_z', 'ori_w',\n",
        "]\n",
        "\n",
        "# Files within the dataset that contain our attributes\n",
        "X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    # Parts that are not needed:\n",
        "    # 'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    # 'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    # 'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "]\n",
        "\n",
        "# Features to generate from our loaded attributes\n",
        "# Note that `a` is going to be a dict of attribute tracks\n",
        "X_features = OrderedDict({\n",
        "    'acc_mag': lambda a: np.sqrt(a['acc_x']**2 + a['acc_y']**2 + a['acc_z']**2),\n",
        "    'mag_mag': lambda a: np.sqrt(a['mag_x']**2 + a['mag_y']**2 + a['mag_z']**2),\n",
        "    'gyr_mag': lambda a: np.sqrt(a['gyr_x']**2 + a['gyr_y']**2 + a['gyr_z']**2),\n",
        "})\n",
        "\n",
        "# Define where to find our labels for supervised learning\n",
        "y_file = 'Label.txt'\n",
        "y_attribute = 'labels'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzV0cHfZRmUh",
        "outputId": "c80cb7ce-c896-40af-beac-a47e750b6a08"
      },
      "source": [
        "# Load pretrained power transformers for feature scaling\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_feature_scalers = OrderedDict({})\n",
        "for feature_name, _ in X_features.items():\n",
        "    scaler_dir = f'models/shl-scalers/{feature_name}.scaler.joblib'\n",
        "    scaler = joblib.load(scaler_dir)\n",
        "    scaler.copy = False # Save memory\n",
        "    X_feature_scalers[feature_name] = scaler\n",
        "    print(f'Loaded scaler from {scaler_dir}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded scaler from models/shl-scalers/acc_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/mag_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/gyr_mag.scaler.joblib.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PowerTransformer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_r8kxvWrtE",
        "outputId": "4a5a798e-a404-4750-91ce-5fab3e458220"
      },
      "source": [
        "# Load the training and validation data into a high performance datatype\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from typing import Generator, List, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def read_chunks(\n",
        "    n_chunks: int, \n",
        "    X_attr_readers: List[pd.io.parsers.TextFileReader], \n",
        "    y_attr_reader: pd.io.parsers.TextFileReader\n",
        ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
        "    \"\"\"\n",
        "    Read chunks of attribute data and yield it to the caller as tuples of X, y.\n",
        "    \n",
        "    This function returns a generator which can be iterated.\n",
        "    \"\"\"\n",
        "    for _ in range(n_chunks):\n",
        "        # Load raw attribute tracks\n",
        "        X_raw_attrs = OrderedDict({})\n",
        "        for X_attribute, X_attr_reader in zip(X_attributes, X_attr_readers):\n",
        "            X_attr_track = next(X_attr_reader)\n",
        "            X_attr_track = np.nan_to_num(X_attr_track.to_numpy())\n",
        "            X_raw_attrs[X_attribute] = X_attr_track\n",
        "\n",
        "        # Calculate features\n",
        "        X_feature_tracks = None\n",
        "        for X_feature_name, X_feature_func in X_features.items():\n",
        "            X_feature_track = X_feature_func(X_raw_attrs)\n",
        "            X_feature_track = X_feature_scalers[X_feature_name] \\\n",
        "                .transform(X_feature_track)\n",
        "            if X_feature_tracks is None:\n",
        "                X_feature_tracks = X_feature_track\n",
        "            else:\n",
        "                X_feature_tracks = np.dstack((X_feature_tracks, X_feature_track))\n",
        "\n",
        "        # Load labels\n",
        "        y_attr_track = next(y_attr_reader) # dim (None, sample_length)\n",
        "        y_attr_track = np.nan_to_num(y_attr_track.to_numpy()) # dim (None, sample_length)\n",
        "        y_attr_track = y_attr_track[:, 0] # dim (None, 1)\n",
        "\n",
        "        yield X_feature_tracks, y_attr_track\n",
        "\n",
        "def count_samples(dataset_dir: Path) -> int:\n",
        "    \"\"\"Count the total amount of samples in a shl dataset.\"\"\"\n",
        "    n_samples = 0\n",
        "    # Every file in the dataset has the same length, use the labels file\n",
        "    with open(dataset_dir / y_file) as f:\n",
        "        for _ in tqdm(f, desc=f'Counting samples in {dataset_dir}'):\n",
        "            n_samples += 1\n",
        "    return n_samples\n",
        "\n",
        "def create_chunked_readers(\n",
        "    dataset_dir: Path,\n",
        "    chunksize: int, \n",
        "    xdtype=np.float32, # Use np.float16 with caution, can lead to overflows\n",
        "    ydtype=np.int\n",
        ") -> Tuple[List[pd.io.parsers.TextFileReader], pd.io.parsers.TextFileReader]:\n",
        "    \"\"\"Initialize chunked csv readers and return them to the caller as a tuple.\"\"\"\n",
        "    read_csv_kwargs = { 'sep': ' ', 'header': None, 'chunksize': chunksize }\n",
        "\n",
        "    X_attr_readers = [] # (dim datasets x readers)\n",
        "    for filename in X_files:\n",
        "        X_reader = pd.read_csv(dataset_dir / filename, dtype=xdtype, **read_csv_kwargs)\n",
        "        X_attr_readers.append(X_reader)\n",
        "    y_attr_reader = pd.read_csv(dataset_dir / y_file, dtype=ydtype, **read_csv_kwargs)\n",
        "\n",
        "    return X_attr_readers, y_attr_reader\n",
        "\n",
        "def export_tfrecords(\n",
        "    dataset_dir: Path,\n",
        "    n_chunks=16, # Load dataset in parts to not overload memory\n",
        "):\n",
        "    \"\"\"Transform the given shl dataset into a memory efficient TFRecord.\"\"\"\n",
        "    target_dir = f'{dataset_dir}.tfrecord'\n",
        "    if os.path.isfile(target_dir):\n",
        "        print(f'{target_dir} already exists.')\n",
        "        return\n",
        "\n",
        "    print(f'Exporting to {target_dir}.')\n",
        "\n",
        "    n_samples = count_samples(dataset_dir)\n",
        "    chunksize = int(np.floor(n_samples / n_chunks))\n",
        "    X_attr_readers, y_attr_reader = create_chunked_readers(dataset_dir, chunksize)    \n",
        "\n",
        "    with tf.io.TFRecordWriter(str(target_dir)) as file_writer:\n",
        "        with tqdm(total=n_samples, desc=f'Reading samples to {target_dir}') as pbar:\n",
        "            for X_feature_tracks, y_attr_track in read_chunks(\n",
        "                n_chunks, X_attr_readers, y_attr_reader\n",
        "            ):\n",
        "                for X, y in zip(X_feature_tracks, y_attr_track):\n",
        "                    X_flat = X.flatten() # TFRecords don't support multidimensional arrays\n",
        "                    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
        "                        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])) \n",
        "                    })).SerializeToString()\n",
        "                    file_writer.write(record_bytes)\n",
        "                pbar.update(chunksize)\n",
        "\n",
        "for dataset_dir in TRAIN_DATASET_DIRS:\n",
        "    export_tfrecords(dataset_dir)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord already exists.\n",
            "shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord already exists.\n",
            "shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord already exists.\n",
            "shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord already exists.\n",
            "shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso.tfrecord already exists.\n",
            "shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag.tfrecord already exists.\n",
            "shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips.tfrecord already exists.\n",
            "shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand.tfrecord already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weYDZUofD4TD",
        "outputId": "677b2ee4-2175-44b8-876d-2665428cc17f"
      },
      "source": [
        "def decode_tfrecord(record_bytes) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Decode a TFRecord example to X, y from its serialized representation.\"\"\"\n",
        "    example = tf.io.parse_single_example(record_bytes, {\n",
        "        'X': tf.io.FixedLenFeature([SAMPLE_LENGTH, len(X_features)], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([1], tf.int64)\n",
        "    })\n",
        "    return example['X'], example['y']\n",
        "\n",
        "def create_train_validation_datasets(\n",
        "    dataset_dirs: List[Path], \n",
        "    batch_size=64,\n",
        "    shuffle_size=20_000, # Must be larger than batch_size\n",
        "    test_size=256 # In batches\n",
        ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "    \"\"\"\n",
        "    Create interleaved, shuffled and batched train and \n",
        "    validation datasets from the dataset dirs.\n",
        "    \n",
        "    Note that this function reads previously generated TFRecords under \n",
        "    `dataset_dir.tfrecord` -> use `export_tfrecords` for that.\n",
        "    \"\"\"\n",
        "    tfrecord_dirs = [f'{d}.tfrecord' for d in dataset_dirs]\n",
        "    print(f'Creating train and validation dataset over {tfrecord_dirs}.')\n",
        "\n",
        "    # Create a strategy to interleave the datasets\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_dirs) \\\n",
        "        .interleave(\n",
        "            lambda x: tf.data.TFRecordDataset(x), \n",
        "            cycle_length=batch_size, # Number of input elements that are processed concurrently\n",
        "            block_length=1 # Return only one element at a time, batching is done later\n",
        "        ) \\\n",
        "        .shuffle(shuffle_size) \\\n",
        "        .map(decode_tfrecord, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(batch_size)\n",
        "    count = sum(1 for _ in dataset)\n",
        "    print(f'Counted {count * batch_size} samples in combined dataset.')\n",
        "    training_dataset = dataset.skip(test_size)\n",
        "    count = sum(1 for _ in training_dataset)\n",
        "    print(f'Counted {count * batch_size} samples in training dataset.')\n",
        "    validation_dataset = dataset.take(test_size)\n",
        "    count = sum(1 for _ in validation_dataset)\n",
        "    print(f'Counted {count * batch_size} samples in validation dataset.')\n",
        "    return training_dataset, validation_dataset\n",
        "\n",
        "train_dataset, validation_dataset = create_train_validation_datasets(TRAIN_DATASET_DIRS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating train and validation dataset over ['shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord', 'shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord', 'shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord', 'shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord', 'shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Torso.tfrecord', 'shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Bag.tfrecord', 'shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hips.tfrecord', 'shl-dataset/challenge-users23_torso_bag_hips_hand/validation/Hand.tfrecord'].\n",
            "Counted 899392 samples in combined dataset.\n",
            "Counted 883008 samples in training dataset.\n",
            "Counted 16384 samples in validation dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haZF5N7ikLbC"
      },
      "source": [
        "## Steps 3-5: Defining, training and evaluating models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXmk4SKK_LCF"
      },
      "source": [
        "# We will use the keras tuner contribution package for a hyperparameter gridsearch\n",
        "\n",
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install keras-tuner -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pdmzyPk6Jn"
      },
      "source": [
        "# Create a logger that will save our progress, even when\n",
        "# colab decides to kill our training instance later on\n",
        "\n",
        "from keras_tuner.engine.logger import Logger\n",
        "\n",
        "class ZIPProducer(Logger):\n",
        "    \"\"\"\n",
        "    A helper class to be passed with the `logger` argument of `tuner.search`.\n",
        "    \n",
        "    On trial completion, this class will automatically \n",
        "    create a zip archive for later analysis. Note that the\n",
        "    download has to occur manually, because `google.colab.files.download`\n",
        "    does not work for some reason.\n",
        "    \"\"\"\n",
        "    def __init__(self, gridsearch_dir: Path):\n",
        "        self.gridsearch_dir = gridsearch_dir\n",
        "    \n",
        "    def register_tuner(self, tuner_state):\n",
        "        \"\"\"Informs the logger that a new search is starting.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def register_trial(self, trial_id, trial_state):\n",
        "        \"\"\"Informs the logger that a new Trial is starting.\"\"\"\n",
        "        shutil.make_archive(self.gridsearch_dir, 'zip', self.gridsearch_dir)\n",
        "        print(f'Saved gridsearch progress under {self.gridsearch_dir}.zip -> Make sure to download!')\n",
        "\n",
        "    def report_trial_state(self, trial_id, trial_state):\n",
        "        \"\"\"Gives the logger information about trial status.\"\"\"\n",
        "        pass            \n",
        "\n",
        "    def exit(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3dP3vAlxe9r"
      },
      "source": [
        "from keras_tuner import Hyperband\n",
        "from keras_tuner.engine import hypermodel as hm_module\n",
        "\n",
        "class Tuner(Hyperband):\n",
        "    \"\"\"\n",
        "    A custom hyperband tuner, which circumvents an issue \n",
        "    in the implementation of keras tuner - Models seem to \n",
        "    start from cold every new epoch, which is clearly unwanted. \n",
        "\n",
        "    See: https://github.com/keras-team/keras-tuner/issues/372\n",
        "    And: https://arxiv.org/pdf/1603.06560.pdf\n",
        "    \"\"\"\n",
        "    def _on_train_begin(self, model, hp, *fit_args, **fit_kwargs):\n",
        "        prev_trial_id = hp.values['tuner/trial_id'] if 'tuner/trial_id' in hp else None\n",
        "        if prev_trial_id:\n",
        "            prev_trial = self.oracle.trials[prev_trial_id]\n",
        "            best_epoch = prev_trial.best_step\n",
        "            # the code below is from load_model method of Tuner class\n",
        "            with hm_module.maybe_distribute(self.distribution_strategy):\n",
        "                model.load_weights(self._get_checkpoint_fname(\n",
        "                    prev_trial.trial_id, best_epoch\n",
        "                ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U302m77jRPO6"
      },
      "source": [
        "# We will use the kapre contribution package to include STFT layers\n",
        "\n",
        "!{sys.executable} -m pip install kapre -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Xd0mWdi5ri"
      },
      "source": [
        "# Define helper functions for resnet model creation\n",
        "\n",
        "import kapre\n",
        "\n",
        "from keras_tuner import HyperParameters\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, applications\n",
        "\n",
        "def twod_resnet_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Create a 2d stft hypermodel with the given hyperparameters.\"\"\"\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "\n",
        "    architectures = OrderedDict({\n",
        "        'ResNet50': applications.ResNet50, # 98 MB\n",
        "        'ResNet101': applications.ResNet101, # 172 MB\n",
        "        'ResNet152': applications.ResNet152, # 232 MB\n",
        "        'ResNet50V2': applications.ResNet50V2, # 98 MB\n",
        "        'ResNet101V2': applications.ResNet101V2, # 171 MB\n",
        "        'ResNet152V2': applications.ResNet152V2, # 232 MB\n",
        "    })\n",
        "\n",
        "    # Hyperparameters\n",
        "    chosen_architecture_name = hp.Choice(\n",
        "        '2d_model_architecture', \n",
        "        values=list(architectures.keys())\n",
        "    )\n",
        "    \n",
        "    chosen_architecture = architectures[chosen_architecture_name]\n",
        "\n",
        "    model = models.Sequential([\n",
        "        # Short-time fourier transform\n",
        "        kapre.STFT(\n",
        "            n_fft=100,\n",
        "            hop_length=5,\n",
        "            pad_end=False,\n",
        "            input_data_format='channels_last', \n",
        "            output_data_format='channels_last',\n",
        "            input_shape=input_shape,\n",
        "            name='stft-layer'\n",
        "        ),\n",
        "        kapre.Magnitude(),\n",
        "        kapre.MagnitudeToDecibel(),\n",
        "\n",
        "        layers.UpSampling2D(2),\n",
        "\n",
        "        chosen_architecture(\n",
        "            include_top=True, input_tensor=None, \n",
        "            input_shape=(162, 102, 3), weights=None,\n",
        "            pooling='avg', classes=len(LABEL_ORDER)\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def oned_resnet_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Create a 1d resnet hypermodel with the given hyperparameters.\"\"\"\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "    input_layer = layers.Input(input_shape)\n",
        "\n",
        "    # Hyperparameters\n",
        "    base_block_height = hp.Int('1d_base_block_height', 32, 128, step=32)\n",
        "    blocks_until_size_duplication = hp.Int('1d_blocks_until_size_duplication', 2, 4)\n",
        "    n_blocks = hp.Int('1d_n_blocks', 2, 6)\n",
        "\n",
        "    def oned_resnet_block(input_layer: layers.Layer, block_height: int) -> layers.Layer:\n",
        "        \"\"\"Create a 1d resnet block with the given block height.\"\"\"\n",
        "        conv_kwargs = { \n",
        "            'filters': block_height, \n",
        "            'padding': 'same', \n",
        "            'kernel_regularizer': 'l2',\n",
        "        }\n",
        "\n",
        "        conv_x = layers.Conv1D(kernel_size=8, **conv_kwargs)(input_layer)\n",
        "        conv_x = layers.BatchNormalization()(conv_x)\n",
        "        conv_x = layers.LeakyReLU(alpha=0.2)(conv_x)\n",
        "\n",
        "        conv_y = layers.Conv1D(kernel_size=5, **conv_kwargs)(conv_x)\n",
        "        conv_y = layers.BatchNormalization()(conv_y)\n",
        "        conv_y = layers.LeakyReLU(alpha=0.2)(conv_y)\n",
        "\n",
        "        conv_z = layers.Conv1D(kernel_size=3, **conv_kwargs)(conv_y)\n",
        "        conv_z = layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        shortcut = layers.Conv1D(kernel_size=1, **conv_kwargs)(input_layer)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "        output_block = layers.add([shortcut, conv_z])\n",
        "        output_block = layers.LeakyReLU(alpha=0.2)(output_block)\n",
        "\n",
        "        return output_block\n",
        "\n",
        "    endpoint_layer = input_layer # Will be built now\n",
        "    for i in range(n_blocks):\n",
        "        n_filters = (int(np.floor(i / blocks_until_size_duplication)) + 1) * base_block_height\n",
        "        endpoint_layer = make_resnet_block(endpoint_layer, n_filters)\n",
        "    \n",
        "    gap_layer = layers.GlobalAveragePooling1D()(endpoint_layer)\n",
        "    output_layer = layers.Dense(len(LABEL_ORDER), activation='softmax')(gap_layer)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def resnet_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Make a resnet hypermodel\"\"\"\n",
        "    model_type = hp.Choice('model_type', ['2d', '1d'])\n",
        "\n",
        "    hp_kwargs = { 'parent_name': 'model_type', 'parent_values': [model_type] }\n",
        "    if model_type == '2d':\n",
        "        with hp.conditional_scope('model_type', ['2d']):\n",
        "            model = twod_resnet_hypermodel(hp)\n",
        "    elif model_type == '1d':\n",
        "        with hp.conditional_scope('model_type', ['1d']):\n",
        "            model = oned_resnet_hypermodel(hp)\n",
        "    else:\n",
        "        raise ValueError('Unknown meta architecture!')\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy', # No OHE necessary\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOH7F65U7fYx",
        "outputId": "eab8d3d6-418a-4600-dd96-5dcc43c7d37a"
      },
      "source": [
        "tuner = Tuner(\n",
        "    hypermodel=resnet_hypermodel, \n",
        "    objective='val_acc', \n",
        "    max_epochs=15, \n",
        "    overwrite=True,\n",
        "    directory='models',\n",
        "    project_name='shl-resnet-gridsearch',\n",
        "    logger=ZIPProducer('models/shl-resnet-gridsearch'),\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "model_type (Choice)\n",
            "{'default': '2d', 'conditions': [], 'values': ['2d', '1d'], 'ordered': False}\n",
            "2d_model_architecture (Choice)\n",
            "{'default': 'ResNet50', 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['2d']}}], 'values': ['ResNet50', 'ResNet101', 'ResNet152', 'ResNet50V2', 'ResNet101V2', 'ResNet152V2'], 'ordered': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmEnc_5l8Wln"
      },
      "source": [
        "# Define callbacks for our training\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "decay_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_acc',\n",
        "    factor=0.5, \n",
        "    patience=5, # Epochs\n",
        "    min_lr=0.00001, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "stop_early = callbacks.EarlyStopping(\n",
        "    monitor='val_acc', \n",
        "    patience=10, # Epochs\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "id": "cX8Z0ZyW-nJ9",
        "outputId": "fcc4f439-46a0-4e3a-9006-294d64bd0306"
      },
      "source": [
        "# Keras tuner grid search training\n",
        "\n",
        "tuner.search(\n",
        "    train_dataset,\n",
        "    epochs=15,\n",
        "    callbacks=[decay_lr, stop_early],\n",
        "    validation_data=validation_dataset,\n",
        "    verbose=1,\n",
        "    shuffle=False, # Shuffling doesn't work with our prefetching\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 7 Complete [01h 48m 07s]\n",
            "val_acc: 0.68048095703125\n",
            "\n",
            "Best val_acc So Far: 0.68048095703125\n",
            "Total elapsed time: 06h 22m 48s\n",
            "Saved gridsearch progress under models/shl-resnet-gridsearch.zip -> Make sure to download!\n",
            "\n",
            "Search: Running Trial #8\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "model_type        |1d                |2d                \n",
            "1d_base_block_h...|64                |None              \n",
            "1d_blocks_until...|3                 |None              \n",
            "1d_n_blocks       |2                 |None              \n",
            "tuner/epochs      |2                 |2                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |2                 |2                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/2\n",
            "13797/13797 [==============================] - 250s 18ms/step - loss: 0.8415 - acc: 0.7043 - val_loss: 2.1362 - val_acc: 0.4088\n",
            "Epoch 2/2\n",
            " 9760/13797 [====================>.........] - ETA: 1:12 - loss: 0.6842 - acc: 0.7414"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-4f3ac5f6a4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Shuffling doesn't work with our prefetching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLASS_WEIGHTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m    146\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_batch_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs, is_batch_hook)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_batch_hook\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_hooks_support_tf_logs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r_LyEYcC7_s"
      },
      "source": [
        "best_hyperparams = tuner.get_best_hyperparameters(8)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQxnHB9VD7uk",
        "outputId": "ddf5afbc-4cde-4da5-bcf3-0b11f8cdbf2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(best_hyperparams, 'hyperparams.joblib')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hyperparams.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfU5raONEFsK",
        "outputId": "89b8935f-01a7-4fdb-ec81-09fc462b5028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "joblib.load('hyperparams.joblib')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb092554c10>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09254d550>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09227b110>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09227b150>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09227b390>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09227bd50>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09227be10>,\n",
              " <keras_tuner.engine.hyperparameters.HyperParameters at 0x7fb09253c090>]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}