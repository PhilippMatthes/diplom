{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-deep-learning-timeseries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCjUOLzIZ0b"
      },
      "source": [
        "# Using a deep CNN to directly classify SHL timeseries data\n",
        "\n",
        "The following notebook contains code to classify SHL timeseries data with deep convolutional neural networks. This is devided into the following steps:\n",
        "\n",
        "1. Download the SHL dataset.\n",
        "2. Preprocess the SHL dataset into features and make it readable efficiently by our training engine.\n",
        "3. Define one or multiple ml models.\n",
        "4. Train the model(s) and utilize grid search to find the best configuration.\n",
        "5. Export the models and their training parameters for later analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aqsHfSGiSqN"
      },
      "source": [
        "## Step 1: Download the SHL Dataset\n",
        "\n",
        "The SHL dataset is very big, so we will need to free up some disk space on colab, first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpWbBKRvep4"
      },
      "source": [
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/torch\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/pystan\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/spacy\n",
        "!rm -rf /tensorflow-1.15.2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqN3gogwim4q"
      },
      "source": [
        "Next, get our base repo so that we can use predefined architectures and pretrained scalers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6b-V4Ewjxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7ffc0a-161d-48a3-f110-cb9d8cdb5e1a"
      },
      "source": [
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 1797, done.\u001b[K\n",
            "remote: Counting objects: 100% (1134/1134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (769/769), done.\u001b[K\n",
            "remote: Total 1797 (delta 570), reused 832 (delta 307), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (1797/1797), 34.69 MiB | 23.92 MiB/s, done.\n",
            "Resolving deltas: 100% (946/946), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXfbxnbwixmp"
      },
      "source": [
        "Switch to our src dir for further processing. This command is specific to Google Colab, so it might not work on your local Jupyter Notebook instance.\n",
        "\n",
        "Additionally, we create the dataset dir in which our dataset will be downloaded next."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL4FprvwmKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d6a0e85-1e1f-4c2e-bdf5-47475d55dbf1"
      },
      "source": [
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDZZQRX1jEnA"
      },
      "source": [
        "Download the SHL dataset from the shl server. This might take some time, on Google Colab its approx. 45 minutes. You can also mount your Google Drive if you have enough space available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWAbEF-wnYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72f2fc9-fc5d-4f87-aae5-77547709b34d"
      },
      "source": [
        "!wget -nc -O shl-dataset/challenge-2019-user1_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-user1_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-user1_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-user1_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-22 09:18:00--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852446972 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_torso.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.45G  9.48MB/s    in 9m 58s  \n",
            "\n",
            "2021-08-22 09:27:59 (9.33 MB/s) - ‘shl-dataset/challenge-2019-user1_torso.zip’ saved [5852446972/5852446972]\n",
            "\n",
            "--2021-08-22 09:27:59--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5628524721 (5.2G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_bag.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.24G  9.65MB/s    in 9m 30s  \n",
            "\n",
            "2021-08-22 09:37:31 (9.41 MB/s) - ‘shl-dataset/challenge-2019-user1_bag.zip’ saved [5628524721/5628524721]\n",
            "\n",
            "--2021-08-22 09:37:31--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5871677913 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-user1_hips.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.47G  9.14MB/s    in 10m 50s \n",
            "\n",
            "2021-08-22 09:48:23 (8.61 MB/s) - ‘shl-dataset/challenge-2019-user1_hips.zip’ saved [5871677913/5871677913]\n",
            "\n",
            "--2021-08-22 09:48:23--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6040097130 (5.6G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-user1_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.62G  9.41MB/s    in 11m 5s  \n",
            "\n",
            "2021-08-22 09:59:29 (8.66 MB/s) - ‘shl-dataset/challenge-2020-user1_hand.zip’ saved [6040097130/6040097130]\n",
            "\n",
            "--2021-08-22 09:59:30--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3263492796 (3.0G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   3.04G  9.25MB/s    in 6m 11s  \n",
            "\n",
            "2021-08-22 10:05:42 (8.39 MB/s) - ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’ saved [3263492796/3263492796]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8C9qCcPjRwc"
      },
      "source": [
        "Next we unzip our dataset into the running instance's filestorage. *Note that this will probably not work for free subscriptions of Google Colab, since the data is approximately 90-100 GB when extracted.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJK91R8EAb0b",
        "outputId": "45ebfbac-3caa-4b92-98a9-a2b689fc7573"
      },
      "source": [
        "# Unzip training datasets\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_torso shl-dataset/challenge-2019-user1_torso.zip\n",
        "!rm shl-dataset/challenge-2019-user1_torso.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_bag shl-dataset/challenge-2019-user1_bag.zip\n",
        "!rm shl-dataset/challenge-2019-user1_bag.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-user1_hips shl-dataset/challenge-2019-user1_hips.zip\n",
        "!rm shl-dataset/challenge-2019-user1_hips.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-user1_hand shl-dataset/challenge-2020-user1_hand.zip\n",
        "!rm shl-dataset/challenge-2020-user1_hand.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-users23_torso_bag_hips_hand shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
        "!rm shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  shl-dataset/challenge-2019-user1_torso.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_torso/train/Torso/\n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_torso/train/Torso/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-user1_bag.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_bag/train/Bag/\n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_bag/train/Bag/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-user1_hips.zip\n",
            "   creating: shl-dataset/challenge-2019-user1_hips/train/Hips/\n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-user1_hips/train/Hips/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-user1_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-user1_hand/train/\n",
            "   creating: shl-dataset/challenge-2020-user1_hand/train/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-user1_hand/train/Hand/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/\n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Pressure.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz4yHwGyjq-U"
      },
      "source": [
        "## Step 2: Preprocess the data\n",
        "\n",
        "Explanations will from now on be inside the code, so that you can copy it without losing the contextual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZAJlKdu06f1",
        "outputId": "1dca7b8b-ee3d-4274-b8fb-d7c3af3d4c62"
      },
      "source": [
        "# Change into our project src directory and select the TensorFlow version\n",
        "# Note: use this as an entrypoint when you already downloaded the dataset\n",
        "\n",
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMIdkUVR9-m",
        "outputId": "6f3cbd99-1c13-484b-c2e6-e83cdc55459e"
      },
      "source": [
        "# Check configuration and hardware resources\n",
        "\n",
        "import distutils\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcVSnPAOENt"
      },
      "source": [
        "# Define all datasets to train our model on\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-user1_torso/train/Torso'),\n",
        "    Path('shl-dataset/challenge-2019-user1_bag/train/Bag'),\n",
        "    Path('shl-dataset/challenge-2019-user1_hips/train/Hips'),\n",
        "    Path('shl-dataset/challenge-2020-user1_hand/train/Hand'),\n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand'),   \n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tYXzJj9OHGo"
      },
      "source": [
        "# Define more useful constants about our dataset\n",
        "\n",
        "LABEL_ORDER = [\n",
        "    'Null',\n",
        "    'Still',\n",
        "    'Walking',\n",
        "    'Run',\n",
        "    'Bike',\n",
        "    'Car',\n",
        "    'Bus',\n",
        "    'Train',\n",
        "    'Subway',\n",
        "]\n",
        "\n",
        "SAMPLE_LENGTH = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHfmi8ZOR1n"
      },
      "source": [
        "# Results from data analysis\n",
        "\n",
        "CLASS_WEIGHTS = {\n",
        "    0: 0.0, # NULL label\n",
        "    1: 1.0021671573438011, \n",
        "    2: 0.9985739895697523, \n",
        "    3: 2.8994439843842423, \n",
        "    4: 1.044135815617944, \n",
        "    5: 0.7723505499007343, \n",
        "    6: 0.8652474758172704, \n",
        "    7: 0.7842127155793044, \n",
        "    8: 1.0283208861290594\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAPsgtXOW3_"
      },
      "source": [
        "# Define features for our dataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Attributes to load from our dataset\n",
        "X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "    # Parts that are not needed:\n",
        "    # 'gra_x', 'gra_y', 'gra_z',\n",
        "    # 'lacc_x', 'lacc_y', 'lacc_z',\n",
        "    # 'ori_x', 'ori_y', 'ori_z', 'ori_w',\n",
        "]\n",
        "\n",
        "# Files within the dataset that contain our attributes\n",
        "X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    # Parts that are not needed:\n",
        "    # 'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    # 'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    # 'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "]\n",
        "\n",
        "# Features to generate from our loaded attributes\n",
        "# Note that `a` is going to be a dict of attribute tracks\n",
        "X_features = OrderedDict({\n",
        "    'acc_mag': lambda a: np.sqrt(a['acc_x']**2 + a['acc_y']**2 + a['acc_z']**2),\n",
        "    'mag_mag': lambda a: np.sqrt(a['mag_x']**2 + a['mag_y']**2 + a['mag_z']**2),\n",
        "    'gyr_mag': lambda a: np.sqrt(a['gyr_x']**2 + a['gyr_y']**2 + a['gyr_z']**2),\n",
        "})\n",
        "\n",
        "# Define where to find our labels for supervised learning\n",
        "y_file = 'Label.txt'\n",
        "y_attribute = 'labels'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzV0cHfZRmUh",
        "outputId": "5f9ed6df-cc1f-48b7-81ef-45f9300dfbc6"
      },
      "source": [
        "# Load pretrained power transformers for feature scaling\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_feature_scalers = OrderedDict({})\n",
        "for feature_name, _ in X_features.items():\n",
        "    scaler_dir = f'models/shl-scalers/{feature_name}.scaler.joblib'\n",
        "    scaler = joblib.load(scaler_dir)\n",
        "    scaler.copy = False # Save memory\n",
        "    X_feature_scalers[feature_name] = scaler\n",
        "    print(f'Loaded scaler from {scaler_dir}.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded scaler from models/shl-scalers/acc_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/mag_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/gyr_mag.scaler.joblib.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PowerTransformer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_r8kxvWrtE",
        "outputId": "73d3b88b-3218-4df0-e8c7-24773776748b"
      },
      "source": [
        "# Load the training and validation data into a high performance datatype\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from typing import Generator, List, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def read_chunks(\n",
        "    n_chunks: int, \n",
        "    X_attr_readers: List[pd.io.parsers.TextFileReader], \n",
        "    y_attr_reader: pd.io.parsers.TextFileReader\n",
        ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
        "    \"\"\"\n",
        "    Read chunks of attribute data and yield it to the caller as tuples of X, y.\n",
        "    \n",
        "    This function returns a generator which can be iterated.\n",
        "    \"\"\"\n",
        "    for _ in range(n_chunks):\n",
        "        # Load raw attribute tracks\n",
        "        X_raw_attrs = OrderedDict({})\n",
        "        for X_attribute, X_attr_reader in zip(X_attributes, X_attr_readers):\n",
        "            X_attr_track = next(X_attr_reader)\n",
        "            X_attr_track = np.nan_to_num(X_attr_track.to_numpy())\n",
        "            X_raw_attrs[X_attribute] = X_attr_track\n",
        "\n",
        "        # Calculate features\n",
        "        X_feature_tracks = None\n",
        "        for X_feature_name, X_feature_func in X_features.items():\n",
        "            X_feature_track = X_feature_func(X_raw_attrs)\n",
        "            X_feature_track = X_feature_scalers[X_feature_name] \\\n",
        "                .transform(X_feature_track)\n",
        "            if X_feature_tracks is None:\n",
        "                X_feature_tracks = X_feature_track\n",
        "            else:\n",
        "                X_feature_tracks = np.dstack((X_feature_tracks, X_feature_track))\n",
        "\n",
        "        # Load labels\n",
        "        y_attr_track = next(y_attr_reader) # dim (None, sample_length)\n",
        "        y_attr_track = np.nan_to_num(y_attr_track.to_numpy()) # dim (None, sample_length)\n",
        "        y_attr_track = y_attr_track[:, 0] # dim (None, 1)\n",
        "\n",
        "        yield X_feature_tracks, y_attr_track\n",
        "\n",
        "def count_samples(dataset_dir: Path) -> int:\n",
        "    \"\"\"Count the total amount of samples in a shl dataset.\"\"\"\n",
        "    n_samples = 0\n",
        "    # Every file in the dataset has the same length, use the labels file\n",
        "    with open(dataset_dir / y_file) as f:\n",
        "        for _ in tqdm(f, desc=f'Counting samples in {dataset_dir}'):\n",
        "            n_samples += 1\n",
        "    return n_samples\n",
        "\n",
        "def create_chunked_readers(\n",
        "    dataset_dir: Path,\n",
        "    chunksize: int, \n",
        "    xdtype=np.float32, # Use np.float16 with caution, can lead to overflows\n",
        "    ydtype=np.int\n",
        ") -> Tuple[List[pd.io.parsers.TextFileReader], pd.io.parsers.TextFileReader]:\n",
        "    \"\"\"Initialize chunked csv readers and return them to the caller as a tuple.\"\"\"\n",
        "    read_csv_kwargs = { 'sep': ' ', 'header': None, 'chunksize': chunksize }\n",
        "\n",
        "    X_attr_readers = [] # (dim datasets x readers)\n",
        "    for filename in X_files:\n",
        "        X_reader = pd.read_csv(dataset_dir / filename, dtype=xdtype, **read_csv_kwargs)\n",
        "        X_attr_readers.append(X_reader)\n",
        "    y_attr_reader = pd.read_csv(dataset_dir / y_file, dtype=ydtype, **read_csv_kwargs)\n",
        "\n",
        "    return X_attr_readers, y_attr_reader\n",
        "\n",
        "def export_tfrecords(\n",
        "    dataset_dir: Path,\n",
        "    n_chunks=16, # Load dataset in parts to not overload memory\n",
        "):\n",
        "    \"\"\"Transform the given shl dataset into a memory efficient TFRecord.\"\"\"\n",
        "    target_dir = f'{dataset_dir}.tfrecord'\n",
        "    if os.path.isfile(target_dir):\n",
        "        print(f'{target_dir} already exists.')\n",
        "        return\n",
        "\n",
        "    print(f'Exporting to {target_dir}.')\n",
        "\n",
        "    n_samples = count_samples(dataset_dir)\n",
        "    chunksize = int(np.floor(n_samples / n_chunks))\n",
        "    X_attr_readers, y_attr_reader = create_chunked_readers(dataset_dir, chunksize)    \n",
        "\n",
        "    with tf.io.TFRecordWriter(str(target_dir)) as file_writer:\n",
        "        with tqdm(total=n_samples, desc=f'Reading samples to {target_dir}') as pbar:\n",
        "            for X_feature_tracks, y_attr_track in read_chunks(\n",
        "                n_chunks, X_attr_readers, y_attr_reader\n",
        "            ):\n",
        "                for X, y in zip(X_feature_tracks, y_attr_track):\n",
        "                    X_flat = X.flatten() # TFRecords don't support multidimensional arrays\n",
        "                    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
        "                        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])) \n",
        "                    })).SerializeToString()\n",
        "                    file_writer.write(record_bytes)\n",
        "                pbar.update(chunksize)\n",
        "\n",
        "for dataset_dir in TRAIN_DATASET_DIRS:\n",
        "    export_tfrecords(dataset_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-user1_torso/train/Torso: 196072it [00:02, 72192.42it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord: 100%|█████████▉| 196064/196072 [04:10<00:00, 783.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-user1_bag/train/Bag: 196072it [00:02, 69826.30it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord: 100%|█████████▉| 196064/196072 [04:18<00:00, 758.70it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-user1_hips/train/Hips: 196072it [00:02, 72411.10it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord: 100%|█████████▉| 196064/196072 [04:23<00:00, 742.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-user1_hand/train/Hand: 196072it [00:02, 72558.47it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord: 100%|█████████▉| 196064/196072 [04:23<00:00, 743.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso: 28789it [00:00, 89751.26it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 674.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag: 28789it [00:00, 91037.33it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 678.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips: 28789it [00:00, 89821.23it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 675.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand: 28789it [00:00, 91006.93it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord: 100%|█████████▉| 28784/28789 [00:42<00:00, 678.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weYDZUofD4TD",
        "outputId": "3397a498-0eff-4b84-c5b3-b715d779c46d"
      },
      "source": [
        "def decode_tfrecord(record_bytes) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Decode a TFRecord example to X, y from its serialized representation.\"\"\"\n",
        "    example = tf.io.parse_single_example(record_bytes, {\n",
        "        'X': tf.io.FixedLenFeature([SAMPLE_LENGTH, len(X_features)], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([1], tf.int64)\n",
        "    })\n",
        "    return example['X'], example['y']\n",
        "\n",
        "def create_train_validation_datasets(\n",
        "    dataset_dirs: List[Path], \n",
        "    batch_size=64,\n",
        "    shuffle_size=20_000, # Must be larger than batch_size\n",
        "    test_size=256 # In batches\n",
        ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
        "    \"\"\"\n",
        "    Create interleaved, shuffled and batched train and \n",
        "    validation datasets from the dataset dirs.\n",
        "    \n",
        "    Note that this function reads previously generated TFRecords under \n",
        "    `dataset_dir.tfrecord` -> use `export_tfrecords` for that.\n",
        "    \"\"\"\n",
        "    tfrecord_dirs = [f'{d}.tfrecord' for d in dataset_dirs]\n",
        "    print(f'Creating train and validation dataset over {tfrecord_dirs}.')\n",
        "\n",
        "    # Create a strategy to interleave the datasets\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_dirs) \\\n",
        "        .interleave(\n",
        "            lambda x: tf.data.TFRecordDataset(x), \n",
        "            cycle_length=batch_size, # Number of input elements that are processed concurrently\n",
        "            block_length=1 # Return only one element at a time, batching is done later\n",
        "        ) \\\n",
        "        .shuffle(shuffle_size) \\\n",
        "        .map(decode_tfrecord, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(batch_size)\n",
        "    count = sum(1 for _ in dataset)\n",
        "    print(f'Counted {count * batch_size} samples in combined dataset.')\n",
        "    training_dataset = dataset.skip(test_size)\n",
        "    count = sum(1 for _ in training_dataset)\n",
        "    print(f'Counted {count * batch_size} samples in training dataset.')\n",
        "    validation_dataset = dataset.take(test_size)\n",
        "    count = sum(1 for _ in validation_dataset)\n",
        "    print(f'Counted {count * batch_size} samples in validation dataset.')\n",
        "    return training_dataset, validation_dataset\n",
        "\n",
        "train_dataset, validation_dataset = create_train_validation_datasets(TRAIN_DATASET_DIRS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating train and validation dataset over ['shl-dataset/challenge-2019-user1_torso/train/Torso.tfrecord', 'shl-dataset/challenge-2019-user1_bag/train/Bag.tfrecord', 'shl-dataset/challenge-2019-user1_hips/train/Hips.tfrecord', 'shl-dataset/challenge-2020-user1_hand/train/Hand.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord', 'shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord'].\n",
            "Counted 899392 samples in combined dataset.\n",
            "Counted 883008 samples in training dataset.\n",
            "Counted 16384 samples in validation dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haZF5N7ikLbC"
      },
      "source": [
        "## Steps 3-5: Defining, training and evaluating models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXmk4SKK_LCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be65227f-c0f5-4d21-e867-887eb51b324f"
      },
      "source": [
        "# We will use the keras tuner contribution package for a hyperparameter gridsearch\n",
        "\n",
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install keras-tuner -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 3.2 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pdmzyPk6Jn"
      },
      "source": [
        "# Create a logger that will save our progress, even when\n",
        "# colab decides to kill our training instance later on\n",
        "\n",
        "import tempfile\n",
        "\n",
        "from keras_tuner.engine.logger import Logger\n",
        "from google.colab.files import download\n",
        "\n",
        "class ZIPProducer(Logger):\n",
        "    \"\"\"\n",
        "    A helper class to be passed with the `logger` argument of `tuner.search`.\n",
        "    \n",
        "    On trial completion, this class will automatically \n",
        "    create a zip archive of the progress for later analysis.\n",
        "    \"\"\"\n",
        "    def __init__(self, gridsearch_dir: Path):\n",
        "        self.gridsearch_dir = gridsearch_dir\n",
        "    \n",
        "    def register_tuner(self, tuner_state):\n",
        "        \"\"\"Informs the logger that a new search is starting.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def register_trial(self, trial_id, trial_state):\n",
        "        \"\"\"Informs the logger that a new Trial is starting.\"\"\"\n",
        "        with tempfile.TemporaryDirectory() as tempdir:\n",
        "            # Copy all files (except the checkpoints which become very large)\n",
        "            # to a temporary directory and zip them, then download\n",
        "            files_to_ignore = shutil.ignore_patterns('checkpoints*')\n",
        "            target = f'{tempdir}/gridsearch'\n",
        "            shutil.copytree(self.gridsearch_dir, target, ignore=files_to_ignore)\n",
        "            shutil.make_archive('models/gridsearch', 'zip', target)\n",
        "            download('models/gridsearch.zip')\n",
        "        print(f'Saved gridsearch progress under gridsearch.zip -> Make sure to download!')\n",
        "\n",
        "    def report_trial_state(self, trial_id, trial_state):\n",
        "        \"\"\"Gives the logger information about trial status.\"\"\"\n",
        "        pass            \n",
        "\n",
        "    def exit(self):\n",
        "        pass"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3dP3vAlxe9r"
      },
      "source": [
        "from keras_tuner import Hyperband\n",
        "from keras_tuner.engine import hypermodel as hm_module\n",
        "\n",
        "class Tuner(Hyperband):\n",
        "    \"\"\"\n",
        "    A custom hyperband tuner, which circumvents an issue \n",
        "    in the implementation of keras tuner - Models seem to \n",
        "    start from cold every new epoch, which is clearly unwanted. \n",
        "\n",
        "    See: https://github.com/keras-team/keras-tuner/issues/372\n",
        "    And: https://arxiv.org/pdf/1603.06560.pdf\n",
        "    \"\"\"\n",
        "    def _on_train_begin(self, model, hp, *fit_args, **fit_kwargs):\n",
        "        prev_trial_id = hp.values['tuner/trial_id'] if 'tuner/trial_id' in hp else None\n",
        "        if prev_trial_id:\n",
        "            prev_trial = self.oracle.trials[prev_trial_id]\n",
        "            best_epoch = prev_trial.best_step\n",
        "            # the code below is from load_model method of Tuner class\n",
        "            with hm_module.maybe_distribute(self.distribution_strategy):\n",
        "                model.load_weights(self._get_checkpoint_fname(\n",
        "                    prev_trial.trial_id, best_epoch\n",
        "                ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U302m77jRPO6"
      },
      "source": [
        "# We will use the kapre contribution package to include STFT layers\n",
        "\n",
        "!{sys.executable} -m pip install kapre -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Xd0mWdi5ri"
      },
      "source": [
        "# Define helper functions for resnet model creation\n",
        "\n",
        "import kapre\n",
        "\n",
        "from keras_tuner import HyperParameters\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, applications\n",
        "\n",
        "def twod_resnet_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Create a 2d stft hypermodel with the given hyperparameters.\"\"\"\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "\n",
        "    architectures = OrderedDict({\n",
        "        'ResNet50': applications.ResNet50, # 98 MB\n",
        "        'ResNet101': applications.ResNet101, # 172 MB\n",
        "        'ResNet152': applications.ResNet152, # 232 MB\n",
        "        'ResNet50V2': applications.ResNet50V2, # 98 MB\n",
        "        'ResNet101V2': applications.ResNet101V2, # 171 MB\n",
        "        'ResNet152V2': applications.ResNet152V2, # 232 MB\n",
        "    })\n",
        "\n",
        "    # Hyperparameters\n",
        "    chosen_architecture_name = hp.Choice(\n",
        "        '2d_model_architecture', \n",
        "        values=list(architectures.keys())\n",
        "    )\n",
        "    \n",
        "    chosen_architecture = architectures[chosen_architecture_name]\n",
        "\n",
        "    model = models.Sequential([\n",
        "        # Short-time fourier transform\n",
        "        kapre.STFT(\n",
        "            n_fft=100,\n",
        "            hop_length=5,\n",
        "            pad_end=False,\n",
        "            input_data_format='channels_last', \n",
        "            output_data_format='channels_last',\n",
        "            input_shape=input_shape,\n",
        "            name='stft-layer'\n",
        "        ),\n",
        "        kapre.Magnitude(),\n",
        "        kapre.MagnitudeToDecibel(),\n",
        "\n",
        "        layers.UpSampling2D(2),\n",
        "\n",
        "        chosen_architecture(\n",
        "            include_top=True, input_tensor=None, \n",
        "            input_shape=(162, 102, 3), weights=None,\n",
        "            pooling='avg', classes=len(LABEL_ORDER)\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def oned_resnet_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Create a 1d resnet hypermodel with the given hyperparameters.\"\"\"\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "    input_layer = layers.Input(input_shape)\n",
        "\n",
        "    # Hyperparameters\n",
        "    base_block_height = hp.Int('1d_base_block_height', 32, 128, step=32)\n",
        "    blocks_until_size_duplication = hp.Int('1d_blocks_until_size_duplication', 2, 4)\n",
        "    n_blocks = hp.Int('1d_n_blocks', 2, 6)\n",
        "\n",
        "    def oned_resnet_block(input_layer: layers.Layer, block_height: int) -> layers.Layer:\n",
        "        \"\"\"Create a 1d resnet block with the given block height.\"\"\"\n",
        "        conv_kwargs = { \n",
        "            'filters': block_height, \n",
        "            'padding': 'same', \n",
        "            'kernel_regularizer': 'l2',\n",
        "        }\n",
        "\n",
        "        conv_x = layers.Conv1D(kernel_size=8, **conv_kwargs)(input_layer)\n",
        "        conv_x = layers.BatchNormalization()(conv_x)\n",
        "        conv_x = layers.LeakyReLU(alpha=0.2)(conv_x)\n",
        "\n",
        "        conv_y = layers.Conv1D(kernel_size=5, **conv_kwargs)(conv_x)\n",
        "        conv_y = layers.BatchNormalization()(conv_y)\n",
        "        conv_y = layers.LeakyReLU(alpha=0.2)(conv_y)\n",
        "\n",
        "        conv_z = layers.Conv1D(kernel_size=3, **conv_kwargs)(conv_y)\n",
        "        conv_z = layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        shortcut = layers.Conv1D(kernel_size=1, **conv_kwargs)(input_layer)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "        output_block = layers.add([shortcut, conv_z])\n",
        "        output_block = layers.LeakyReLU(alpha=0.2)(output_block)\n",
        "\n",
        "        return output_block\n",
        "\n",
        "    endpoint_layer = input_layer # Will be built now\n",
        "    for i in range(n_blocks):\n",
        "        n_filters = (int(np.floor(i / blocks_until_size_duplication)) + 1) * base_block_height\n",
        "        endpoint_layer = make_resnet_block(endpoint_layer, n_filters)\n",
        "    \n",
        "    gap_layer = layers.GlobalAveragePooling1D()(endpoint_layer)\n",
        "    output_layer = layers.Dense(len(LABEL_ORDER), activation='softmax')(gap_layer)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model\n",
        "\n",
        "def resnet_hypermodel(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Make a resnet hypermodel\"\"\"\n",
        "    model_type = hp.Choice('model_type', ['2d', '1d'])\n",
        "\n",
        "    hp_kwargs = { 'parent_name': 'model_type', 'parent_values': [model_type] }\n",
        "    if model_type == '2d':\n",
        "        with hp.conditional_scope('model_type', ['2d']):\n",
        "            model = twod_resnet_hypermodel(hp)\n",
        "    elif model_type == '1d':\n",
        "        with hp.conditional_scope('model_type', ['1d']):\n",
        "            model = oned_resnet_hypermodel(hp)\n",
        "    else:\n",
        "        raise ValueError('Unknown meta architecture!')\n",
        "\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy', # No OHE necessary\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOH7F65U7fYx",
        "outputId": "1cd2f3c5-456b-4a7e-ec00-d69fb58508b8"
      },
      "source": [
        "tuner = Tuner(\n",
        "    hypermodel=resnet_hypermodel, \n",
        "    objective='val_acc', \n",
        "    max_epochs=15, \n",
        "    overwrite=False,\n",
        "    directory='models',\n",
        "    project_name='shl-resnet-gridsearch',\n",
        "    logger=ZIPProducer('models/shl-resnet-gridsearch'),\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project models/shl-resnet-gridsearch/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from models/shl-resnet-gridsearch/tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 2\n",
            "model_type (Choice)\n",
            "{'default': '2d', 'conditions': [], 'values': ['2d', '1d'], 'ordered': False}\n",
            "2d_model_architecture (Choice)\n",
            "{'default': 'ResNet50', 'conditions': [{'class_name': 'Parent', 'config': {'name': 'model_type', 'values': ['2d']}}], 'values': ['ResNet50', 'ResNet101', 'ResNet152', 'ResNet50V2', 'ResNet101V2', 'ResNet152V2'], 'ordered': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmEnc_5l8Wln"
      },
      "source": [
        "# Define callbacks for our training\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "decay_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_acc',\n",
        "    factor=0.5, \n",
        "    patience=5, # Epochs\n",
        "    min_lr=0.00001, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "stop_early = callbacks.EarlyStopping(\n",
        "    monitor='val_acc', \n",
        "    patience=10, # Epochs\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "cX8Z0ZyW-nJ9",
        "outputId": "5923655e-bd97-4cc4-dba4-5f2af4f836e6"
      },
      "source": [
        "# Keras tuner grid search training\n",
        "\n",
        "tuner.search(\n",
        "    train_dataset,\n",
        "    epochs=15,\n",
        "    callbacks=[decay_lr, stop_early],\n",
        "    validation_data=validation_dataset,\n",
        "    verbose=1,\n",
        "    shuffle=False, # Shuffling doesn't work with our prefetching\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d3ad26f5-6ae9-4356-b077-5a461ffea77c\", \"gridsearch.zip\", 2092)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Saved gridsearch progress under gridsearch.zip -> Make sure to download!\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "model_type        |2d                |2d                \n",
            "2d_model_archit...|ResNet152V2       |ResNet152         \n",
            "tuner/epochs      |2                 |2                 \n",
            "tuner/initial_e...|0                 |0                 \n",
            "tuner/bracket     |2                 |2                 \n",
            "tuner/round       |0                 |0                 \n",
            "\n",
            "Epoch 1/2\n",
            "   9380/Unknown - 3068s 326ms/step - loss: 0.3644 - acc: 0.8621"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}