{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-deep-learning-timeseries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCjUOLzIZ0b"
      },
      "source": [
        "# Using a deep CNN to directly classify SHL timeseries data\n",
        "\n",
        "The following notebook contains code to classify SHL timeseries data with deep convolutional neural networks. This is devided into the following steps:\n",
        "\n",
        "1. Download the SHL dataset.\n",
        "2. Preprocess the SHL dataset into features and make it readable efficiently by our training engine.\n",
        "3. Define one or multiple ml models.\n",
        "4. Train the model(s) and utilize grid search to find the best configuration.\n",
        "5. Export the models and their training parameters for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpWbBKRvep4"
      },
      "source": [
        "# Free up some disk space on colab\n",
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/torch\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/pystan\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/spacy\n",
        "!rm -rf /tensorflow-1.15.2/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6b-V4Ewjxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c904f9-69d7-490b-b715-569b33a92ebe"
      },
      "source": [
        "# Get needed auxiliary files for colab\n",
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 1755, done.\u001b[K\n",
            "remote: Counting objects: 100% (1092/1092), done.\u001b[K\n",
            "remote: Compressing objects: 100% (734/734), done.\u001b[K\n",
            "remote: Total 1755 (delta 542), reused 817 (delta 301), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (1755/1755), 34.53 MiB | 20.57 MiB/s, done.\n",
            "Resolving deltas: 100% (918/918), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL4FprvwmKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af887905-7f9c-4e34-dbce-825071328e16"
      },
      "source": [
        "# Change into src dir and load our datasets\n",
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWAbEF-wnYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53edff6-a1fe-4c56-a0c5-da38f5fa7bff"
      },
      "source": [
        "# Download training datasets\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-train_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
        "# Download validation dataset\n",
        "!wget -nc -O shl-dataset/challenge-2020-validation.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-19 09:37:33--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852446972 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_torso.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.45G  7.41MB/s    in 12m 6s  \n",
            "\n",
            "2021-08-19 09:49:39 (7.69 MB/s) - ‘shl-dataset/challenge-2019-train_torso.zip’ saved [5852446972/5852446972]\n",
            "\n",
            "--2021-08-19 09:49:39--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5628524721 (5.2G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_bag.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.24G  8.81MB/s    in 11m 16s \n",
            "\n",
            "2021-08-19 10:00:57 (7.93 MB/s) - ‘shl-dataset/challenge-2019-train_bag.zip’ saved [5628524721/5628524721]\n",
            "\n",
            "--2021-08-19 10:00:57--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5871677913 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_hips.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.47G  11.2MB/s    in 8m 56s  \n",
            "\n",
            "2021-08-19 10:09:54 (10.4 MB/s) - ‘shl-dataset/challenge-2019-train_hips.zip’ saved [5871677913/5871677913]\n",
            "\n",
            "--2021-08-19 10:09:54--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6040097130 (5.6G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-train_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.62G  11.2MB/s    in 8m 52s  \n",
            "\n",
            "2021-08-19 10:18:47 (10.8 MB/s) - ‘shl-dataset/challenge-2020-train_hand.zip’ saved [6040097130/6040097130]\n",
            "\n",
            "--2021-08-19 10:18:47--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3263492796 (3.0G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-validation.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   3.04G  11.2MB/s    in 4m 46s  \n",
            "\n",
            "2021-08-19 10:23:34 (10.9 MB/s) - ‘shl-dataset/challenge-2020-validation.zip’ saved [3263492796/3263492796]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJK91R8EAb0b",
        "outputId": "fa590eb6-c2ff-48fa-e8af-25e6a0888ee5"
      },
      "source": [
        "# Unzip training datasets\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_torso shl-dataset/challenge-2019-train_torso.zip\n",
        "!rm shl-dataset/challenge-2019-train_torso.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_bag shl-dataset/challenge-2019-train_bag.zip\n",
        "!rm shl-dataset/challenge-2019-train_bag.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_hips shl-dataset/challenge-2019-train_hips.zip\n",
        "!rm shl-dataset/challenge-2019-train_hips.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-train_hand shl-dataset/challenge-2020-train_hand.zip\n",
        "!rm shl-dataset/challenge-2020-train_hand.zip\n",
        "# Unzip validation dataset\n",
        "!unzip -n -d shl-dataset/challenge-2020-validation shl-dataset/challenge-2020-validation.zip\n",
        "!rm shl-dataset/challenge-2020-validation.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  shl-dataset/challenge-2019-train_torso.zip\n",
            "   creating: shl-dataset/challenge-2019-train_torso/train/Torso/\n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_torso/train/Torso/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-train_bag.zip\n",
            "   creating: shl-dataset/challenge-2019-train_bag/train/Bag/\n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_bag/train/Bag/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2019-train_hips.zip\n",
            "   creating: shl-dataset/challenge-2019-train_hips/train/Hips/\n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2019-train_hips/train/Hips/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-train_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-train_hand/train/\n",
            "   creating: shl-dataset/challenge-2020-train_hand/train/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-validation.zip\n",
            "   creating: shl-dataset/challenge-2020-validation/validation/\n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Bag/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Hips/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Torso/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Pressure.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZAJlKdu06f1",
        "outputId": "74208161-94f2-475b-db92-9f1570d5ba04"
      },
      "source": [
        "# Change into our project src directory and select the TensorFlow version\n",
        "# Note: use this as an entrypoint when you already downloaded the dataset\n",
        "\n",
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMIdkUVR9-m",
        "outputId": "a91ad972-db68-4040-c8ce-be1d814005dc"
      },
      "source": [
        "# Check configuration and hardware resources\n",
        "\n",
        "import distutils\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
        "\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcVSnPAOENt"
      },
      "source": [
        "# Define all datasets to train our model on\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-train_torso/train/Torso'),\n",
        "    Path('shl-dataset/challenge-2019-train_bag/train/Bag'),\n",
        "    Path('shl-dataset/challenge-2019-train_hips/train/Hips'),\n",
        "    Path('shl-dataset/challenge-2020-train_hand/train/Hand'),\n",
        "]\n",
        "\n",
        "VALIDATION_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Hand'),                  \n",
        "]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tYXzJj9OHGo"
      },
      "source": [
        "# Define more useful constants about our dataset\n",
        "\n",
        "LABEL_ORDER = [\n",
        "    'Null',\n",
        "    'Still',\n",
        "    'Walking',\n",
        "    'Run',\n",
        "    'Bike',\n",
        "    'Car',\n",
        "    'Bus',\n",
        "    'Train',\n",
        "    'Subway',\n",
        "]\n",
        "\n",
        "SAMPLE_LENGTH = 500"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypHfmi8ZOR1n"
      },
      "source": [
        "# Results from data analysis\n",
        "\n",
        "CLASS_WEIGHTS = {\n",
        "    0: 0.0, # NULL label\n",
        "    1: 1.0021671573438011, \n",
        "    2: 0.9985739895697523, \n",
        "    3: 2.8994439843842423, \n",
        "    4: 1.044135815617944, \n",
        "    5: 0.7723505499007343, \n",
        "    6: 0.8652474758172704, \n",
        "    7: 0.7842127155793044, \n",
        "    8: 1.0283208861290594\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAPsgtXOW3_"
      },
      "source": [
        "# Define features for our dataset\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Attributes to load from our dataset\n",
        "X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "    # Parts that are not needed:\n",
        "    # 'gra_x', 'gra_y', 'gra_z',\n",
        "    # 'lacc_x', 'lacc_y', 'lacc_z',\n",
        "    # 'ori_x', 'ori_y', 'ori_z', 'ori_w',\n",
        "]\n",
        "\n",
        "# Files within the dataset that contain our attributes\n",
        "X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    # Parts that are not needed:\n",
        "    # 'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    # 'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    # 'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "]\n",
        "\n",
        "# Features to generate from our loaded attributes\n",
        "# Note that `a` is going to be a dict of attribute tracks\n",
        "X_features = OrderedDict({\n",
        "    'acc_mag': lambda a: np.sqrt(a['acc_x']**2 + a['acc_y']**2 + a['acc_z']**2),\n",
        "    'mag_mag': lambda a: np.sqrt(a['mag_x']**2 + a['mag_y']**2 + a['mag_z']**2),\n",
        "    'gyr_mag': lambda a: np.sqrt(a['gyr_x']**2 + a['gyr_y']**2 + a['gyr_z']**2),\n",
        "})\n",
        "\n",
        "# Define where to find our labels for supervised learning\n",
        "y_file = 'Label.txt'\n",
        "y_attribute = 'labels'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzV0cHfZRmUh",
        "outputId": "98242ded-3a3c-4722-94b6-0cf7a5f73e19"
      },
      "source": [
        "# Load pretrained power transformers for feature scaling\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_feature_scalers = OrderedDict({})\n",
        "for feature_name, _ in X_features.items():\n",
        "    scaler_dir = f'models/shl-scalers/{feature_name}.scaler.joblib'\n",
        "    scaler = joblib.load(scaler_dir)\n",
        "    scaler.copy = False # Save memory\n",
        "    X_feature_scalers[feature_name] = scaler\n",
        "    print(f'Loaded scaler from {scaler_dir}.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded scaler from models/shl-scalers/acc_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/mag_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/gyr_mag.scaler.joblib.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PowerTransformer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL_r8kxvWrtE",
        "outputId": "c2b8e922-adc4-4e73-ab96-d73e5fa3f1d0"
      },
      "source": [
        "# Load the training and validation data into a high performance datatype\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from typing import Generator, List, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def read_chunks(\n",
        "    n_chunks: int, \n",
        "    X_attr_readers: List[pd.io.parsers.TextFileReader], \n",
        "    y_attr_reader: pd.io.parsers.TextFileReader\n",
        ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
        "    \"\"\"\n",
        "    Read chunks of attribute data and yield it to the caller as tuples of X, y.\n",
        "    \n",
        "    This function returns a generator which can be iterated.\n",
        "    \"\"\"\n",
        "    for _ in range(n_chunks):\n",
        "        # Load raw attribute tracks\n",
        "        X_raw_attrs = OrderedDict({})\n",
        "        for X_attribute, X_attr_reader in zip(X_attributes, X_attr_readers):\n",
        "            X_attr_track = next(X_attr_reader)\n",
        "            X_attr_track = np.nan_to_num(X_attr_track.to_numpy())\n",
        "            X_raw_attrs[X_attribute] = X_attr_track\n",
        "\n",
        "        # Calculate features\n",
        "        X_feature_tracks = None\n",
        "        for X_feature_name, X_feature_func in X_features.items():\n",
        "            X_feature_track = X_feature_func(X_raw_attrs)\n",
        "            X_feature_track = X_feature_scalers[X_feature_name] \\\n",
        "                .transform(X_feature_track)\n",
        "            if X_feature_tracks is None:\n",
        "                X_feature_tracks = X_feature_track\n",
        "            else:\n",
        "                X_feature_tracks = np.dstack((X_feature_tracks, X_feature_track))\n",
        "\n",
        "        # Load labels\n",
        "        y_attr_track = next(y_attr_reader) # dim (None, sample_length)\n",
        "        y_attr_track = np.nan_to_num(y_attr_track.to_numpy()) # dim (None, sample_length)\n",
        "        y_attr_track = y_attr_track[:, 0] # dim (None, 1)\n",
        "\n",
        "        yield X_feature_tracks, y_attr_track\n",
        "\n",
        "def count_samples(dataset_dir: Path) -> int:\n",
        "    \"\"\"Count the total amount of samples in a shl dataset.\"\"\"\n",
        "    n_samples = 0\n",
        "    # Every file in the dataset has the same length, use the labels file\n",
        "    with open(dataset_dir / y_file) as f:\n",
        "        for _ in tqdm(f, desc=f'Counting samples in {dataset_dir}'):\n",
        "            n_samples += 1\n",
        "    return n_samples\n",
        "\n",
        "def create_chunked_readers(\n",
        "    dataset_dir: Path,\n",
        "    chunksize: int, \n",
        "    xdtype=np.float32, # Use np.float16 with caution, can lead to overflows\n",
        "    ydtype=np.int\n",
        ") -> Tuple[List[pd.io.parsers.TextFileReader], pd.io.parsers.TextFileReader]:\n",
        "    \"\"\"Initialize chunked csv readers and return them to the caller as a tuple.\"\"\"\n",
        "    read_csv_kwargs = { 'sep': ' ', 'header': None, 'chunksize': chunksize }\n",
        "\n",
        "    X_attr_readers = [] # (dim datasets x readers)\n",
        "    for filename in X_files:\n",
        "        X_reader = pd.read_csv(dataset_dir / filename, dtype=xdtype, **read_csv_kwargs)\n",
        "        X_attr_readers.append(X_reader)\n",
        "    y_attr_reader = pd.read_csv(dataset_dir / y_file, dtype=ydtype, **read_csv_kwargs)\n",
        "\n",
        "    return X_attr_readers, y_attr_reader\n",
        "\n",
        "def export_tfrecords(\n",
        "    dataset_dir: Path,\n",
        "    n_chunks=16, # Load dataset in parts to not overload memory\n",
        "):\n",
        "    \"\"\"Transform the given shl dataset into a memory efficient TFRecord.\"\"\"\n",
        "    target_dir = f'{dataset_dir}.tfrecord'\n",
        "    if os.path.isfile(target_dir):\n",
        "        print(f'{target_dir} already exists.')\n",
        "        return\n",
        "\n",
        "    print(f'Exporting to {target_dir}.')\n",
        "\n",
        "    n_samples = count_samples(dataset_dir)\n",
        "    chunksize = int(np.floor(n_samples / n_chunks))\n",
        "    X_attr_readers, y_attr_reader = create_chunked_readers(dataset_dir, chunksize)    \n",
        "\n",
        "    with tf.io.TFRecordWriter(str(target_dir)) as file_writer:\n",
        "        with tqdm(total=n_samples, desc=f'Reading samples to {target_dir}') as pbar:\n",
        "            for X_feature_tracks, y_attr_track in read_chunks(\n",
        "                n_chunks, X_attr_readers, y_attr_reader\n",
        "            ):\n",
        "                for X, y in zip(X_feature_tracks, y_attr_track):\n",
        "                    X_flat = X.flatten() # TFRecords don't support multidimensional arrays\n",
        "                    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n",
        "                        'X': tf.train.Feature(float_list=tf.train.FloatList(value=X_flat)),\n",
        "                        'y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y])) \n",
        "                    })).SerializeToString()\n",
        "                    file_writer.write(record_bytes)\n",
        "                pbar.update(chunksize)\n",
        "\n",
        "for dataset_dir in TRAIN_DATASET_DIRS + VALIDATION_DATASET_DIRS:\n",
        "    export_tfrecords(dataset_dir)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-train_torso/train/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-train_torso/train/Torso: 196072it [00:02, 72961.33it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-train_torso/train/Torso.tfrecord: 100%|█████████▉| 196064/196072 [05:00<00:00, 653.02it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-train_bag/train/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-train_bag/train/Bag: 196072it [00:02, 72417.29it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-train_bag/train/Bag.tfrecord: 100%|█████████▉| 196064/196072 [05:02<00:00, 647.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2019-train_hips/train/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2019-train_hips/train/Hips: 196072it [00:02, 72465.93it/s]\n",
            "Reading samples to shl-dataset/challenge-2019-train_hips/train/Hips.tfrecord: 100%|█████████▉| 196064/196072 [05:04<00:00, 644.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-train_hand/train/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-train_hand/train/Hand: 196072it [00:02, 72377.76it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-train_hand/train/Hand.tfrecord: 100%|█████████▉| 196064/196072 [05:05<00:00, 642.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Torso.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Torso: 28789it [00:00, 90105.95it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-validation/validation/Torso.tfrecord: 100%|█████████▉| 28784/28789 [00:49<00:00, 584.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Bag.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Bag: 28789it [00:00, 90224.11it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-validation/validation/Bag.tfrecord: 100%|█████████▉| 28784/28789 [00:48<00:00, 595.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Hips.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Hips: 28789it [00:00, 87460.32it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-validation/validation/Hips.tfrecord: 100%|█████████▉| 28784/28789 [00:48<00:00, 590.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exporting to shl-dataset/challenge-2020-validation/validation/Hand.tfrecord.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Counting samples in shl-dataset/challenge-2020-validation/validation/Hand: 28789it [00:00, 90188.46it/s]\n",
            "Reading samples to shl-dataset/challenge-2020-validation/validation/Hand.tfrecord: 100%|█████████▉| 28784/28789 [00:48<00:00, 592.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weYDZUofD4TD"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "N_SHUFFLED_SHARDS = 512 # Must be larger than batch size\n",
        "\n",
        "def decode_tfrecord(record_bytes) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"Decode a TFRecord example to X, y from its serialized representation.\"\"\"\n",
        "    example = tf.io.parse_single_example(record_bytes, {\n",
        "        'X': tf.io.FixedLenFeature([SAMPLE_LENGTH, len(X_features)], tf.float32),\n",
        "        'y': tf.io.FixedLenFeature([1], tf.int64)\n",
        "    })\n",
        "    return example['X'], example['y']\n",
        "\n",
        "def create_dataset_tensors(dataset_dirs: List[Path]) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Create a interleaved, shuffled and batched dataset from the dataset dirs.\n",
        "    \n",
        "    Note that this function reads previously generated TFRecords under \n",
        "    `dataset_dir.tfrecord` -> use `export_tfrecords` for that.\n",
        "    \"\"\"\n",
        "    tfrecord_dirs = [f'{d}.tfrecord' for d in dataset_dirs]\n",
        "    print(f'Creating dataset over {tfrecord_dirs}.')\n",
        "\n",
        "    # Create a strategy to interleave the datasets\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(tfrecord_dirs) \\\n",
        "        .interleave(\n",
        "            lambda x: tf.data.TFRecordDataset(x), \n",
        "            cycle_length=len(tfrecord_dirs), # Number of input elements that are processed concurrently\n",
        "            block_length=1 # Return only one element at a time, batching is done later\n",
        "        ) \\\n",
        "        .map(decode_tfrecord, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .shuffle(N_SHUFFLED_SHARDS) \\\n",
        "        .batch(BATCH_SIZE)\n",
        "    count = sum(1 for _ in dataset)\n",
        "    print(f'Counted {count * BATCH_SIZE} samples in dataset.')\n",
        "    return dataset\n",
        "\n",
        "def create_train_tensors() -> tf.data.Dataset:\n",
        "    \"\"\"Create a TFRecord based and therefore efficiently readable train dataset.\"\"\"\n",
        "    return create_dataset_tensors(TRAIN_DATASET_DIRS)\n",
        "\n",
        "def create_validation_tensors() -> tf.data.Dataset:\n",
        "    \"\"\"Create a TFRecord based and therefore efficiently readable validation dataset.\"\"\"\n",
        "    return create_dataset_tensors(VALIDATION_DATASET_DIRS)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2MnJNNGGkxq",
        "outputId": "7c777d57-bf5f-4f71-c5ef-85bd92d7b7b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_dataset = create_train_tensors()\n",
        "validation_dataset = create_validation_tensors()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating dataset over ['shl-dataset/challenge-2019-train_torso/train/Torso.tfrecord', 'shl-dataset/challenge-2019-train_bag/train/Bag.tfrecord', 'shl-dataset/challenge-2019-train_hips/train/Hips.tfrecord', 'shl-dataset/challenge-2020-train_hand/train/Hand.tfrecord'].\n",
            "Counted 784384 samples in dataset.\n",
            "Creating dataset over ['shl-dataset/challenge-2020-validation/validation/Torso.tfrecord', 'shl-dataset/challenge-2020-validation/validation/Bag.tfrecord', 'shl-dataset/challenge-2020-validation/validation/Hips.tfrecord', 'shl-dataset/challenge-2020-validation/validation/Hand.tfrecord'].\n",
            "Counted 115200 samples in dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXmk4SKK_LCF",
        "outputId": "7f6ef4a5-237d-44fc-d8d0-de0207c05437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install keras-tuner -q"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 27.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 2.8 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Xd0mWdi5ri"
      },
      "source": [
        "# Define helper functions for resnet model creation\n",
        "\n",
        "from keras_tuner import HyperParameters\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def make_resnet_block(input_layer: layers.Layer, block_height: int) -> layers.Layer:\n",
        "    \"\"\"Create a resnet block with the given block height.\"\"\"\n",
        "    conv_kwargs = { \n",
        "        'filters': block_height, \n",
        "        'padding': 'same', \n",
        "        'kernel_regularizer': 'l2',\n",
        "    }\n",
        "\n",
        "    conv_x = layers.Conv1D(kernel_size=8, **conv_kwargs)(input_layer)\n",
        "    conv_x = layers.BatchNormalization()(conv_x)\n",
        "    conv_x = layers.LeakyReLU(alpha=0.2)(conv_x)\n",
        "\n",
        "    conv_y = layers.Conv1D(kernel_size=5, **conv_kwargs)(conv_x)\n",
        "    conv_y = layers.BatchNormalization()(conv_y)\n",
        "    conv_y = layers.LeakyReLU(alpha=0.2)(conv_y)\n",
        "\n",
        "    conv_z = layers.Conv1D(kernel_size=3, **conv_kwargs)(conv_y)\n",
        "    conv_z = layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    shortcut = layers.Conv1D(kernel_size=1, **conv_kwargs)(input_layer)\n",
        "    shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    output_block = layers.add([shortcut, conv_z])\n",
        "    output_block = layers.LeakyReLU(alpha=0.2)(output_block)\n",
        "\n",
        "    return output_block\n",
        "\n",
        "\n",
        "def make_resnet(hp: HyperParameters) -> models.Model:\n",
        "    \"\"\"Create a compiled resnet hypermodel with the given hyperparameters.\"\"\"\n",
        "    input_shape = (SAMPLE_LENGTH, len(X_features))\n",
        "    input_layer = layers.Input(input_shape)\n",
        "\n",
        "    endpoint_layer = input_layer # Will be built now\n",
        "    for i in range(hp.Int('n_layers', 2, 4)):\n",
        "        endpoint_layer = make_resnet_block(\n",
        "            endpoint_layer, \n",
        "            hp.Int(f'block_{i}_maps', 64, 512, step=64),\n",
        "        )\n",
        "    \n",
        "    gap_layer = layers.GlobalAveragePooling1D()(endpoint_layer)\n",
        "    output_layer = layers.Dense(len(LABEL_ORDER), activation='softmax')(gap_layer)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy', # No OHE necessary\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        metrics=['acc']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoTZ0umCInGC"
      },
      "source": [
        "GRIDSEARCH_BASE_DIR = 'models'\n",
        "GRIDSEARCH_PROJECT_NAME = 'shl-resnet-gridsearch'\n",
        "GRIDSEARCH_DIR = f'{GRIDSEARCH_BASE_DIR}/{GRIDSEARCH_PROJECT_NAME}'"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b9Yx2cxIjk4"
      },
      "source": [
        "# Create a logger that will save our progress, even when\n",
        "# colab decides to kill our training instance\n",
        "\n",
        "from google.colab import files\n",
        "from keras_tuner.engine.logger import Logger\n",
        "from keras_tuner.engine.trial import TrialStatus\n",
        "\n",
        "class ZIPProducer(Logger):\n",
        "    \"\"\"\n",
        "    A helper class to be passed with the `logger` argument of `tuner.search`.\n",
        "    \n",
        "    On trial completion, this class will automatically create a zip archive\n",
        "    and download it to the supervisor computer, for later analysis.\n",
        "    \"\"\"\n",
        "    \n",
        "    def register_tuner(self, tuner_state):\n",
        "        \"\"\"Informs the logger that a new search is starting.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def register_trial(self, trial_id, trial_state):\n",
        "        \"\"\"Informs the logger that a new Trial is starting.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def report_trial_state(self, trial_id, trial_state):\n",
        "        \"\"\"Gives the logger information about trial status.\"\"\"\n",
        "        if trial_state == TrialStatus.COMPLETED:\n",
        "            shutil.make_archive(GRIDSEARCH_DIR, 'zip', GRIDSEARCH_DIR)\n",
        "            files.download(f'{GRIDSEARCH_DIR}.zip') # Download to control machine\n",
        "\n",
        "    def exit(self):\n",
        "        pass"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOH7F65U7fYx",
        "outputId": "4322c814-1261-418d-d8e1-2ec6195e8b96"
      },
      "source": [
        "from keras_tuner import Hyperband\n",
        "\n",
        "tuner = Hyperband(\n",
        "    hypermodel=make_resnet, \n",
        "    objective='val_acc', \n",
        "    max_epochs=15, \n",
        "    overwrite=True,\n",
        "    directory=GRIDSEARCH_BASE_DIR,\n",
        "    project_name=GRIDSEARCH_PROJECT_NAME,\n",
        "    logger=ZIPProducer(),\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 3\n",
            "n_layers (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': None}\n",
            "block_0_maps (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 512, 'step': 64, 'sampling': None}\n",
            "block_1_maps (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 512, 'step': 64, 'sampling': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmEnc_5l8Wln"
      },
      "source": [
        "# Define callbacks for our training\n",
        "\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "decay_lr = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_acc',\n",
        "    factor=0.5, \n",
        "    patience=5, # Epochs\n",
        "    min_lr=0.0001, \n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "stop_early = callbacks.EarlyStopping(\n",
        "    monitor='val_acc', \n",
        "    patience=10, # Epochs\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX8Z0ZyW-nJ9",
        "outputId": "8d11466b-222b-44ad-cd7c-49e1a18c9911"
      },
      "source": [
        "# Keras tuner grid search training\n",
        "\n",
        "tuner.search(\n",
        "    train_dataset,\n",
        "    epochs=15,\n",
        "    callbacks=[decay_lr, stop_early],\n",
        "    validation_data=validation_dataset,\n",
        "    verbose=1,\n",
        "    shuffle=False, # Shuffling doesn't work with our prefetching\n",
        "    class_weight=CLASS_WEIGHTS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "n_layers          |3                 |?                 \n",
            "block_0_maps      |128               |?                 \n",
            "block_1_maps      |448               |?                 \n",
            "tuner/epochs      |2                 |?                 \n",
            "tuner/initial_e...|0                 |?                 \n",
            "tuner/bracket     |2                 |?                 \n",
            "tuner/round       |0                 |?                 \n",
            "\n",
            "Epoch 1/2\n",
            "      6/Unknown - 6s 379ms/step - loss: 19.9346 - acc: 0.7956WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1134s vs `on_train_batch_end` time: 0.2218s). Check your callbacks.\n",
            "    631/Unknown - 243s 378ms/step - loss: 13.7611 - acc: 0.4530"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}