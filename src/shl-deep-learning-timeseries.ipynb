{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-deep-learning-timeseries.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQCjUOLzIZ0b"
      },
      "source": [
        "# Using a deep CNN to directly classify SHL timeseries data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKpWbBKRvep4"
      },
      "source": [
        "# Free up some disk space on colab\n",
        "!rm -rf /usr/local/lib/python2.7\n",
        "!rm -rf /swift\n",
        "!rm -rf /usr/local/cuda-10.0\n",
        "!rm -rf /usr/local/cuda-10.1\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/torch\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/pystan\n",
        "!rm -rf /usr/local/lib/python3.6/dist-packages/spacy\n",
        "!rm -rf /tensorflow-1.15.2/\n",
        "!rm -rf /opt/nvidia"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z6b-V4Ewjxi"
      },
      "source": [
        "# Get needed auxiliary files for colab\n",
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXL4FprvwmKe",
        "outputId": "abebf318-6d8e-42ee-846a-51640091e962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Change into src dir and load our datasets\n",
        "%cd /content/diplom/src\n",
        "!mkdir shl-dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n",
            "mkdir: cannot create directory ‘shl-dataset’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tWAbEF-wnYU"
      },
      "source": [
        "# Download training datasets\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
        "!wget -nc -O shl-dataset/challenge-2020-train_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
        "# Download validation dataset\n",
        "!wget -nc -O shl-dataset/challenge-2020-validation.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OlIByJSIr6U",
        "outputId": "f788e454-a787-4041-f812-63640f0b587f"
      },
      "source": [
        "# Unzip training datasets\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_torso shl-dataset/challenge-2019-train_torso.zip\n",
        "!rm shl-dataset/challenge-2019-train_torso.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_bag shl-dataset/challenge-2019-train_bag.zip\n",
        "!rm shl-dataset/challenge-2019-train_bag.zip\n",
        "!unzip -n -d shl-dataset/challenge-2019-train_hips shl-dataset/challenge-2019-train_hips.zip\n",
        "!rm shl-dataset/challenge-2019-train_hips.zip\n",
        "!unzip -n -d shl-dataset/challenge-2020-train_hand shl-dataset/challenge-2020-train_hand.zip\n",
        "!rm shl-dataset/challenge-2020-train_hand.zip\n",
        "# Unzip validation dataset\n",
        "!unzip -n -d shl-dataset/challenge-2020-validation shl-dataset/challenge-2020-validation.zip\n",
        "!rm shl-dataset/challenge-2020-validation.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open shl-dataset/challenge-2019-train_torso.zip, shl-dataset/challenge-2019-train_torso.zip.zip or shl-dataset/challenge-2019-train_torso.zip.ZIP.\n",
            "rm: cannot remove 'shl-dataset/challenge-2019-train_torso.zip': No such file or directory\n",
            "Archive:  shl-dataset/challenge-2019-train_bag.zip\n",
            "unzip:  cannot find or open shl-dataset/challenge-2019-train_hips.zip, shl-dataset/challenge-2019-train_hips.zip.zip or shl-dataset/challenge-2019-train_hips.zip.ZIP.\n",
            "rm: cannot remove 'shl-dataset/challenge-2019-train_hips.zip': No such file or directory\n",
            "Archive:  shl-dataset/challenge-2020-train_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-train_hand/train/\n",
            "   creating: shl-dataset/challenge-2020-train_hand/train/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-train_hand/train/Hand/Pressure.txt  \n",
            "Archive:  shl-dataset/challenge-2020-validation.zip\n",
            "   creating: shl-dataset/challenge-2020-validation/validation/\n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Bag/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Bag/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hand/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Hips/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Hips/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-validation/validation/Torso/\n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-validation/validation/Torso/Pressure.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZAJlKdu06f1",
        "outputId": "2dbc786c-222c-424a-a169-cc5c827a6208"
      },
      "source": [
        "%cd /content/diplom/src\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/diplom/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyYqkhWuy3BS"
      },
      "source": [
        "# Import garbage collector to save memory here and there\n",
        "import gc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Q5C3EOivKT"
      },
      "source": [
        "# Define all datasets to train our model on\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "TRAIN_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2019-train_torso/train/Torso'),\n",
        "    Path('shl-dataset/challenge-2019-train_bag/train/Bag'),\n",
        "    Path('shl-dataset/challenge-2019-train_hips/train/Hips'),\n",
        "    Path('shl-dataset/challenge-2020-train_hand/train/Hand'),\n",
        "]\n",
        "\n",
        "VALIDATION_DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-validation/validation/Hand'),                  \n",
        "]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPcUNZfPIZ0g",
        "outputId": "7a7a89f9-be62-4788-d6cc-661f402acf87"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Check that we can use our GPU, to not wait forever during training\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 1467716063153075211]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1sQlrfPdu3E"
      },
      "source": [
        "# Load power transformers for preprocessing\n",
        "\n",
        "import joblib\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "from tools.dataset import shl_dataset_X_attributes\n",
        "\n",
        "scalers = OrderedDict({})\n",
        "for a in shl_dataset_X_attributes:\n",
        "    scaler = joblib.load(f'models/shl-scalers/{a}.scaler.joblib')\n",
        "    scaler.copy = False # Save memory\n",
        "    scalers[a] = scaler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU7uFeizcSDR"
      },
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Compute class weights for unbiased training\n",
        "# labels_reduced = train_dataset.labels[:, 0].astype(np.int) # Only select first idx of each sample\n",
        "# class_weights = compute_class_weight(\n",
        "#     'balanced', \n",
        "#     classes=np.unique(labels_reduced), \n",
        "#     y=labels_reduced\n",
        "# )\n",
        "# class_weights = dict(zip(np.unique(labels_reduced), class_weights)) # Keras adaption\n",
        "# del labels_reduced # Save memory\n",
        "# gc.collect()\n",
        "# Fill in NULL class for tf 2.x\n",
        "# class_weights[0] = 0\n",
        "\n",
        "# class_weights"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Xd0mWdi5ri"
      },
      "source": [
        "# Create our model\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from architectures.resnet import make_resnet\n",
        "from tools.dataset import shl_dataset_label_order\n",
        "\n",
        "MODEL_DIR = Path('models/shl-resnet-all-attributes')\n",
        "\n",
        "model = make_resnet(\n",
        "    input_shape=[500, len(shl_dataset_X_attributes)], \n",
        "    output_classes=len(shl_dataset_label_order)\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky8plx1djfQu",
        "outputId": "619e15ac-f9b8-40b9-85a3-00e735d82ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 500, 19)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 500, 64)      9792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 500, 64)      256         conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 500, 64)      0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 64)      20544       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 500, 64)      256         conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 500, 64)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 500, 64)      1280        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 64)      12352       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 500, 64)      256         conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 500, 64)      256         conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 500, 64)      0           batch_normalization_3[0][0]      \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 500, 64)      0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 500, 128)     65664       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 500, 128)     512         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 500, 128)     82048       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 500, 128)     512         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 500, 128)     0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 500, 128)     8320        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 500, 128)     49280       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 500, 128)     512         conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 500, 128)     512         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 500, 128)     0           batch_normalization_7[0][0]      \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 500, 128)     0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 500, 128)     131200      activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 500, 128)     512         conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 500, 128)     0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 500, 128)     82048       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 500, 128)     512         conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 500, 128)     0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 500, 128)     49280       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 500, 128)     512         activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 500, 128)     512         conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 500, 128)     0           batch_normalization_11[0][0]     \n",
            "                                                                 batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 500, 128)     0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 9)            1161        global_average_pooling1d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 518,089\n",
            "Trainable params: 515,529\n",
            "Non-trainable params: 2,560\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lEV9qAyhjxQ",
        "outputId": "0118e12e-d717-4e93-e8bc-5c627beb77fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "shl_dataset_X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "]\n",
        "\n",
        "shl_dataset_y_file = 'Label.txt'\n",
        "\n",
        "class DatasetGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataset_dirs, batch_size=128, prefetch_size=65536, xdtype=np.float32, ydtype=np.int):\n",
        "        assert batch_size % len(dataset_dirs) == 0\n",
        "\n",
        "        self.dataset_dirs = dataset_dirs\n",
        "        self.batch_size = batch_size\n",
        "        self.xdtype = xdtype\n",
        "        self.ydtype = ydtype\n",
        "\n",
        "        # Count samples in datasets\n",
        "        self.n_samples = 0\n",
        "        for dataset_dir in dataset_dirs:\n",
        "            # Every file in the dataset has the same length\n",
        "            with open(dataset_dir / shl_dataset_y_file) as f:\n",
        "                for _ in tqdm(f, desc='Counting samples'):\n",
        "                    self.n_samples += 1\n",
        "        print(f'Total sample count: {self.n_samples}')\n",
        "\n",
        "        self.prefetch_size = prefetch_size\n",
        "        self.prefetched_window_start_idx = 0\n",
        "        self.prefetched_window_end_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.n_samples / self.batch_size))\n",
        "    \n",
        "    def __getitem__(self, batch_idx):\n",
        "        # Generate samples indexes of the batch\n",
        "        requested_start_idx = int(batch_idx * self.batch_size / len(self.dataset_dirs))\n",
        "        requested_end_idx = int((batch_idx + 1) * self.batch_size / len(self.dataset_dirs))\n",
        "\n",
        "        # If we are outside the prefetched window, prefetch the next one\n",
        "        if requested_start_idx < self.prefetched_window_start_idx or \\\n",
        "           requested_end_idx > self.prefetched_window_end_idx:\n",
        "            self.prefetched_window_start_idx = requested_start_idx\n",
        "            self.prefetched_window_end_idx = requested_end_idx + self.prefetch_size\n",
        "            csv_skiprows = self.prefetched_window_start_idx\n",
        "            csv_nrows = self.prefetched_window_end_idx - self.prefetched_window_start_idx\n",
        "\n",
        "            # Read this part of the dataset\n",
        "            for dataset_dir in self.dataset_dirs:\n",
        "                # Load attributes\n",
        "                self.window_X = None\n",
        "                for attribute, filename in zip(shl_dataset_X_attributes, shl_dataset_X_files):\n",
        "                    fpath = dataset_dir / filename\n",
        "                    df = pd.read_csv(\n",
        "                        fpath, header=None, sep=' ', dtype=self.xdtype, \n",
        "                        skiprows=csv_skiprows, nrows=csv_nrows\n",
        "                    )\n",
        "                    track = np.nan_to_num(df.to_numpy())\n",
        "                    scaled_track = scalers[attribute].transform(track)\n",
        "\n",
        "                    if self.window_X is None:\n",
        "                        self.window_X = scaled_track\n",
        "                    else:\n",
        "                        self.window_X = np.dstack((self.window_X, scaled_track))\n",
        "                # Load labels\n",
        "                fpath = dataset_dir / shl_dataset_y_file\n",
        "                df = pd.read_csv(\n",
        "                    fpath, header=None, sep=' ', dtype=self.ydtype, \n",
        "                    skiprows=csv_skiprows, nrows=csv_nrows\n",
        "                )\n",
        "                track = np.nan_to_num(df.to_numpy())\n",
        "                self.window_y = track[:, 0] # Only use first index\n",
        "        scoped_start_idx = requested_start_idx - self.prefetched_window_start_idx\n",
        "        scoped_end_idx = requested_end_idx - self.prefetched_window_end_idx\n",
        "        X = self.window_X[scoped_start_idx:scoped_end_idx]\n",
        "        y = self.window_y[scoped_start_idx:scoped_end_idx]\n",
        "        return X, y\n",
        "\n",
        "# Use batch generators to not preprocess the whole dataset at once   \n",
        "train_generator = DatasetGenerator(TRAIN_DATASET_DIRS)\n",
        "validation_generator = DatasetGenerator(VALIDATION_DATASET_DIRS)\n",
        "\n",
        "# Train model\n",
        "callbacks = [\n",
        "    keras.callbacks.CSVLogger(f'train.log', append=False),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(MODEL_DIR), save_best_only=True, monitor='val_loss', verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', factor=0.25, patience=25, min_lr=0.0001, verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=1),\n",
        "]\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=200,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    shuffle=False\n",
        "    # class_weight=class_weights\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting samples: 196072it [00:00, 944959.05it/s]\n",
            "Counting samples: 196072it [00:00, 903914.11it/s]\n",
            "Counting samples: 196072it [00:00, 914853.47it/s]\n",
            "Counting samples: 196072it [00:00, 923459.10it/s]\n",
            "Counting samples: 28789it [00:00, 861889.22it/s]\n",
            "Counting samples: 28789it [00:00, 896268.83it/s]\n",
            "Counting samples: 28789it [00:00, 897474.56it/s]\n",
            "Counting samples: 28789it [00:00, 918123.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Total sample count: 784288\n",
            "Total sample count: 115156\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}