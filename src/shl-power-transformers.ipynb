{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.8 64-bit"
    },
    "interpreter": {
      "hash": "097461492b3eec731f5f36facfab7d83b93854d821dc66544771e2db489a1966"
    },
    "colab": {
      "name": "shl-power-transformers.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-power-transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit and configure scalers"
      ],
      "metadata": {
        "id": "eWl81Y9fI1Ay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Get needed auxiliary files for colab\r\n",
        "!git clone https://github.com/philippmatthes/diplom\r\n",
        "%cd /content/diplom/src\r\n",
        "!mkdir shl-dataset\r\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_torso.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\r\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_bag.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\r\n",
        "!wget -nc -O shl-dataset/challenge-2019-train_hips.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\r\n",
        "!wget -nc -O shl-dataset/challenge-2020-train_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 1525, done.\u001b[K\n",
            "remote: Counting objects: 100% (862/862), done.\u001b[K\n",
            "remote: Compressing objects: 100% (557/557), done.\u001b[K\n",
            "remote: Total 1525 (delta 428), reused 682 (delta 265), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (1525/1525), 33.74 MiB | 22.33 MiB/s, done.\n",
            "Resolving deltas: 100% (804/804), done.\n",
            "/content/diplom/src\n",
            "--2021-08-10 12:57:39--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_torso.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5852446972 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_torso.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.45G  9.62MB/s    in 9m 56s  \n",
            "\n",
            "2021-08-10 13:07:36 (9.36 MB/s) - ‘shl-dataset/challenge-2019-train_torso.zip’ saved [5852446972/5852446972]\n",
            "\n",
            "--2021-08-10 13:07:36--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_bag.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5628524721 (5.2G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_bag.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.24G  9.65MB/s    in 9m 35s  \n",
            "\n",
            "2021-08-10 13:17:13 (9.34 MB/s) - ‘shl-dataset/challenge-2019-train_bag.zip’ saved [5628524721/5628524721]\n",
            "\n",
            "--2021-08-10 13:17:13--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2019/challenge-2019-train_hips.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5871677913 (5.5G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2019-train_hips.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.47G  9.72MB/s    in 9m 54s  \n",
            "\n",
            "2021-08-10 13:27:08 (9.43 MB/s) - ‘shl-dataset/challenge-2019-train_hips.zip’ saved [5871677913/5871677913]\n",
            "\n",
            "--2021-08-10 13:27:08--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-train_hand.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6040097130 (5.6G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-train_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   5.62G  9.62MB/s    in 10m 17s \n",
            "\n",
            "2021-08-10 13:37:26 (9.34 MB/s) - ‘shl-dataset/challenge-2020-train_hand.zip’ saved [6040097130/6040097130]\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "nuncWkC-I1A0",
        "outputId": "54eaed1c-cc48-4959-892b-6fe69d0036a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Switch to src dir and select tensorflow\r\n",
        "%cd /content/diplom/src\r\n",
        "%tensorflow_version 2.x"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diplom/src\n"
          ]
        }
      ],
      "metadata": {
        "id": "KzALa8KFI1A1",
        "outputId": "ea1ad960-02e2-493d-83dd-07d6dd79077e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Create our scalers\r\n",
        "from sklearn.preprocessing import PowerTransformer\r\n",
        "\r\n",
        "shl_dataset_X_attributes = [\r\n",
        "    'acc_x', 'acc_y', 'acc_z',\r\n",
        "    'mag_x', 'mag_y', 'mag_z',\r\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\r\n",
        "    'gra_x', 'gra_y', 'gra_z',\r\n",
        "    'lacc_x', 'lacc_y', 'lacc_z',\r\n",
        "    'ori_x', 'ori_y', 'ori_z', 'ori_w',\r\n",
        "]\r\n",
        "\r\n",
        "shl_dataset_y_attributes = ['labels']\r\n",
        "\r\n",
        "shl_dataset_attributes = shl_dataset_X_attributes + shl_dataset_y_attributes\r\n",
        "\r\n",
        "scalers = dict([(a, PowerTransformer()) for a in shl_dataset_X_attributes])\r\n",
        "scalers"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'acc_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'acc_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gra_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gra_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gra_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gyr_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gyr_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'gyr_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'lacc_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'lacc_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'lacc_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'mag_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'mag_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'mag_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_w': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_x': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_y': PowerTransformer(copy=True, method='yeo-johnson', standardize=True),\n",
              " 'ori_z': PowerTransformer(copy=True, method='yeo-johnson', standardize=True)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ],
      "metadata": {
        "id": "gZvAw4bNI1A2",
        "outputId": "df32d7d8-5763-4fd3-c7ae-67fad9825e0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from pathlib import Path\r\n",
        "\r\n",
        "DATASET_DIRS = [\r\n",
        "    Path('shl-dataset/challenge-2019-train_torso.zip'),\r\n",
        "    Path('shl-dataset/challenge-2019-train_bag.zip'),\r\n",
        "    Path('shl-dataset/challenge-2019-train_hips.zip'),\r\n",
        "    Path('shl-dataset/challenge-2020-train_hand.zip'),\r\n",
        "]"
      ],
      "outputs": [],
      "metadata": {
        "id": "twUxbLPHI1A3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Load the datasets\r\n",
        "\r\n",
        "import zipfile\r\n",
        "import tempfile\r\n",
        "import pathlib\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "shl_dataset_files = [\r\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\r\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\r\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\r\n",
        "    'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\r\n",
        "    'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\r\n",
        "    'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\r\n",
        "    'Label.txt'\r\n",
        "]\r\n",
        "\r\n",
        "class SHLDataset:\r\n",
        "    def __init__(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def concat_inplace(self, other):\r\n",
        "        for attribute in shl_dataset_attributes:\r\n",
        "            setattr(self, attribute, np.concatenate((\r\n",
        "                getattr(self, attribute),\r\n",
        "                getattr(other, attribute)\r\n",
        "            ), axis=0))\r\n",
        "\r\n",
        "\r\n",
        "def load_shl_dataset(dataset_dir: pathlib.Path, tqdm=None, nrows=None):\r\n",
        "    dataset = SHLDataset()\r\n",
        "    if tqdm is None:\r\n",
        "        tqdm = lambda x, desc: x # passthrough\r\n",
        "    for attribute, filename in tqdm(\r\n",
        "        list(zip(shl_dataset_attributes, shl_dataset_files)),\r\n",
        "        desc=f'Loading dataset subfiles'\r\n",
        "    ):\r\n",
        "        df = pd.read_csv(dataset_dir / filename, header=None, sep=' ', nrows=nrows, dtype=np.float16)\r\n",
        "        np_arr = np.nan_to_num(df.to_numpy())\r\n",
        "        setattr(dataset, attribute, np_arr)\r\n",
        "    return dataset\r\n",
        "\r\n",
        "\r\n",
        "def load_zipped_shl_dataset(zip_dir: pathlib.Path, tqdm=None, nrows=None, subdir_in_zip='train'):\r\n",
        "    with tempfile.TemporaryDirectory() as unzip_dir:\r\n",
        "        with zipfile.ZipFile(zip_dir, 'r') as zip_ref:\r\n",
        "            if tqdm:\r\n",
        "                for member in tqdm(zip_ref.infolist(), desc=f'Extracting {zip_dir}'):\r\n",
        "                    zip_ref.extract(member, unzip_dir)\r\n",
        "            else:\r\n",
        "                zip_ref.extractall(unzip_dir)\r\n",
        "\r\n",
        "        train_dir = pathlib.Path(unzip_dir) / subdir_in_zip\r\n",
        "        sub_dirs = [x for x in train_dir.iterdir() if train_dir.is_dir()]\r\n",
        "\r\n",
        "        result_dataset = None\r\n",
        "        for sub_dir in sub_dirs:\r\n",
        "            sub_dataset = load_shl_dataset(train_dir / sub_dir, tqdm=tqdm, nrows=nrows)\r\n",
        "            if result_dataset is None:\r\n",
        "                result_dataset = sub_dataset\r\n",
        "            else:\r\n",
        "                result_dataset.concat_inplace(sub_dataset)\r\n",
        "                del sub_dataset\r\n",
        "        return result_dataset\r\n",
        "\r\n",
        "dataset = None\r\n",
        "\r\n",
        "for dataset_dir in DATASET_DIRS:\r\n",
        "    # Load dataset from zip file into temporary directory\r\n",
        "    partial_dataset = load_zipped_shl_dataset(dataset_dir, tqdm=tqdm)\r\n",
        "    if dataset is None:\r\n",
        "        dataset = partial_dataset\r\n",
        "    else:\r\n",
        "        dataset.concat_inplace(partial_dataset)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting shl-dataset/challenge-2019-train_torso.zip: 100%|██████████| 22/22 [03:06<00:00,  8.50s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:03<00:00, 18.20s/it]\n",
            "Extracting shl-dataset/challenge-2019-train_bag.zip: 100%|██████████| 22/22 [03:02<00:00,  8.29s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:04<00:00, 18.21s/it]\n",
            "Extracting shl-dataset/challenge-2019-train_hips.zip: 100%|██████████| 22/22 [03:11<00:00,  8.68s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:22<00:00, 19.10s/it]\n",
            "Extracting shl-dataset/challenge-2020-train_hand.zip: 100%|██████████| 23/23 [03:11<00:00,  8.33s/it]\n",
            "Loading dataset subfiles: 100%|██████████| 20/20 [06:33<00:00, 19.67s/it]\n"
          ]
        }
      ],
      "metadata": {
        "id": "Hq6MXgsLI1A3",
        "outputId": "2dedc542-df06-48d5-bc8a-c2097ee0aa31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "import json\r\n",
        "import joblib\r\n",
        "\r\n",
        "export_dir = 'models/'\r\n",
        "num_random_samples = 10000\r\n",
        "\r\n",
        "for attribute, scaler in tqdm(scalers.items(), desc='Fitting scalers'):\r\n",
        "    samples = getattr(dataset, attribute)\r\n",
        "    random_samples_idx = np.random.choice(samples.shape[0], num_random_samples, replace=False)\r\n",
        "    random_samples = samples[random_samples_idx]\r\n",
        "    scaler.fit(random_samples.astype(np.float64))\r\n",
        "\r\n",
        "    # Platform independent export\r\n",
        "    transformer_params = {\r\n",
        "        'lambdas': list(scaler.lambdas_),\r\n",
        "    }\r\n",
        "    with open(export_dir + f'{attribute}.scaler.json', 'w') as f:\r\n",
        "        f.write(json.dumps(transformer_params))\r\n",
        "    # Python export\r\n",
        "    joblib.dump(scaler, export_dir + f'{attribute}.scaler.joblib')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "Fitting scalers:   0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:   5%|▌         | 1/19 [00:11<03:34, 11.92s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  11%|█         | 2/19 [00:23<03:21, 11.85s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  16%|█▌        | 3/19 [00:35<03:10, 11.90s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  21%|██        | 4/19 [00:48<03:01, 12.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  26%|██▋       | 5/19 [01:01<02:52, 12.34s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  32%|███▏      | 6/19 [01:14<02:45, 12.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  37%|███▋      | 7/19 [01:26<02:28, 12.37s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  42%|████▏     | 8/19 [01:37<02:12, 12.05s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  47%|████▋     | 9/19 [01:48<01:57, 11.74s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  53%|█████▎    | 10/19 [02:00<01:45, 11.72s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  58%|█████▊    | 11/19 [02:12<01:35, 11.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  63%|██████▎   | 12/19 [02:25<01:24, 12.12s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  68%|██████▊   | 13/19 [02:37<01:12, 12.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  74%|███████▎  | 14/19 [02:49<01:00, 12.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  79%|███████▉  | 15/19 [03:01<00:48, 12.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  84%|████████▍ | 16/19 [03:13<00:36, 12.03s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  89%|████████▉ | 17/19 [03:25<00:24, 12.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers:  95%|█████████▍| 18/19 [03:37<00:11, 11.95s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Fitting scalers: 100%|██████████| 19/19 [03:50<00:00, 12.11s/it]\n"
          ]
        }
      ],
      "metadata": {
        "id": "wluu6qhSI1A4",
        "outputId": "2e7a4f5b-6424-4be1-f03c-ee373d923bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "# Download updated model folder\r\n",
        "import shutil\r\n",
        "shutil.make_archive('models', 'zip', 'models')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/diplom/src/models.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ],
      "metadata": {
        "id": "urZHIrHql4SL",
        "outputId": "45c02023-4947-4f54-a614-aa51c9f1971f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      }
    }
  ]
}