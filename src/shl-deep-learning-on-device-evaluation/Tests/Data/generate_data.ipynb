{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlheKx/5VFzNGiXbKvv97/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilippMatthes/diplom/blob/master/src/shl-deep-learning-on-device-evaluation/Tests/Data/generate_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9LYCEdn5cdA",
        "outputId": "6de061f4-aa3a-45fd-e9ec-f97e3acb3541"
      },
      "source": [
        "!git clone https://github.com/philippmatthes/diplom"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diplom'...\n",
            "remote: Enumerating objects: 2176, done.\u001b[K\n",
            "remote: Counting objects: 100% (1513/1513), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1054/1054), done.\u001b[K\n",
            "remote: Total 2176 (delta 764), reused 1057 (delta 383), pack-reused 663\u001b[K\n",
            "Receiving objects: 100% (2176/2176), 55.57 MiB | 21.16 MiB/s, done.\n",
            "Resolving deltas: 100% (1140/1140), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WS25neMu5qxA",
        "outputId": "1a8080e4-ce44-4d25-9d15-24c4d2fdb2b6"
      },
      "source": [
        "%cd /content/diplom/src"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diplom/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0sHsU6v5x9h"
      },
      "source": [
        "!mkdir shl-dataset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP2nnepT5uYp",
        "outputId": "a122267c-d8f9-4788-d3ab-3466ede13edb"
      },
      "source": [
        "!wget -nc -O shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 12:05:48--  http://www.shl-dataset.org/wp-content/uploads/SHLChallenge2020/challenge-2020-validation.zip\n",
            "Resolving www.shl-dataset.org (www.shl-dataset.org)... 37.187.125.22\n",
            "Connecting to www.shl-dataset.org (www.shl-dataset.org)|37.187.125.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3263492796 (3.0G) [application/zip]\n",
            "Saving to: ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’\n",
            "\n",
            "shl-dataset/challen 100%[===================>]   3.04G  11.2MB/s    in 4m 42s  \n",
            "\n",
            "2021-09-03 12:10:32 (11.0 MB/s) - ‘shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip’ saved [3263492796/3263492796]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdq-_FUW5w6j",
        "outputId": "750c696f-ab7f-4510-86ec-7bf0305ed930"
      },
      "source": [
        "!unzip -n -d shl-dataset/challenge-2020-users23_torso_bag_hips_hand shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
        "!rm shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  shl-dataset/challenge-2020-users23_torso_bag_hips_hand.zip\n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/\n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips/Pressure.txt  \n",
            "   creating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/\n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Acc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gra_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Gyr_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/LAcc_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Label.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Mag_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_w.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_x.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_y.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Ori_z.txt  \n",
            "  inflating: shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso/Pressure.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qjbOo_u53Cw"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATASET_DIRS = [\n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso'),         \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips'),   \n",
        "    Path('shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand'),   \n",
        "]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7-T-a0C6CN0"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Attributes to load from our dataset\n",
        "X_attributes = [\n",
        "    'acc_x', 'acc_y', 'acc_z',\n",
        "    'mag_x', 'mag_y', 'mag_z',\n",
        "    'gyr_x', 'gyr_y', 'gyr_z',\n",
        "    # Parts that are not needed:\n",
        "    # 'gra_x', 'gra_y', 'gra_z',\n",
        "    # 'lacc_x', 'lacc_y', 'lacc_z',\n",
        "    # 'ori_x', 'ori_y', 'ori_z', 'ori_w',\n",
        "]\n",
        "\n",
        "# Files within the dataset that contain our attributes\n",
        "X_files = [\n",
        "    'Acc_x.txt', 'Acc_y.txt', 'Acc_z.txt',\n",
        "    'Mag_x.txt', 'Mag_y.txt', 'Mag_z.txt',\n",
        "    'Gyr_x.txt', 'Gyr_y.txt', 'Gyr_z.txt',\n",
        "    # Parts that are not needed:\n",
        "    # 'Gra_x.txt', 'Gra_y.txt', 'Gra_z.txt',\n",
        "    # 'LAcc_x.txt', 'LAcc_y.txt', 'LAcc_z.txt',\n",
        "    # 'Ori_x.txt', 'Ori_y.txt', 'Ori_z.txt', 'Ori_w.txt',\n",
        "]\n",
        "\n",
        "# Features to generate from our loaded attributes\n",
        "# Note that `a` is going to be a dict of attribute tracks\n",
        "X_features = OrderedDict({\n",
        "    'acc_mag': lambda a: np.sqrt(a['acc_x']**2 + a['acc_y']**2 + a['acc_z']**2),\n",
        "    'mag_mag': lambda a: np.sqrt(a['mag_x']**2 + a['mag_y']**2 + a['mag_z']**2),\n",
        "    'gyr_mag': lambda a: np.sqrt(a['gyr_x']**2 + a['gyr_y']**2 + a['gyr_z']**2),\n",
        "})\n",
        "\n",
        "# Define where to find our labels for supervised learning\n",
        "y_file = 'Label.txt'\n",
        "y_attribute = 'labels'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8r7K4x96IHZ",
        "outputId": "6e0bdb44-fefa-468c-a222-d84b83b66d6d"
      },
      "source": [
        "# Load pretrained power transformers for feature scaling\n",
        "\n",
        "import joblib\n",
        "\n",
        "X_feature_scalers = OrderedDict({})\n",
        "for feature_name, _ in X_features.items():\n",
        "    scaler_dir = f'models/shl-scalers/{feature_name}.scaler.joblib'\n",
        "    scaler = joblib.load(scaler_dir)\n",
        "    scaler.copy = True\n",
        "    X_feature_scalers[feature_name] = scaler\n",
        "    print(f'Loaded scaler from {scaler_dir}.')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded scaler from models/shl-scalers/acc_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/mag_mag.scaler.joblib.\n",
            "Loaded scaler from models/shl-scalers/gyr_mag.scaler.joblib.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator PowerTransformer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "cuuRGSyC6Kdh",
        "outputId": "25b43925-b655-41b7-a65c-7f3f5201cff6"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "from typing import Generator, List, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab.files import download\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def read_chunks(\n",
        "    n_chunks: int, \n",
        "    X_attr_readers: List[pd.io.parsers.TextFileReader], \n",
        "    y_attr_reader: pd.io.parsers.TextFileReader\n",
        ") -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
        "    for _ in range(n_chunks):\n",
        "        # Load raw attribute tracks\n",
        "        X_raw_attrs = OrderedDict({})\n",
        "        for X_attribute, X_attr_reader in zip(X_attributes, X_attr_readers):\n",
        "            X_attr_track = next(X_attr_reader)\n",
        "            X_attr_track = np.nan_to_num(X_attr_track.to_numpy())\n",
        "            X_raw_attrs[X_attribute] = X_attr_track\n",
        "\n",
        "        # Calculate features and concatenate raw tracks\n",
        "        X_feature_tracks = None\n",
        "        X_raw_tracks = None\n",
        "        for X_feature_name, X_feature_func in X_features.items():\n",
        "            X_raw_track = X_feature_func(X_raw_attrs)\n",
        "\n",
        "            if X_raw_tracks is None:\n",
        "                X_raw_tracks = X_raw_track\n",
        "            else:\n",
        "                X_raw_tracks = np.dstack((X_raw_tracks, X_raw_track))\n",
        "\n",
        "            X_feature_track = X_feature_scalers[X_feature_name].transform(X_raw_track)\n",
        "            if X_feature_tracks is None:\n",
        "                X_feature_tracks = X_feature_track\n",
        "            else:\n",
        "                X_feature_tracks = np.dstack((X_feature_tracks, X_feature_track))\n",
        "\n",
        "        # Load labels\n",
        "        y_attr_track = next(y_attr_reader) # dim (None, sample_length)\n",
        "        y_attr_track = np.nan_to_num(y_attr_track.to_numpy()) # dim (None, sample_length)\n",
        "        y_attr_track = y_attr_track[:, 0] # dim (None, 1)\n",
        "\n",
        "        yield X_raw_tracks, X_feature_tracks, y_attr_track\n",
        "\n",
        "def count_samples(dataset_dir: Path) -> int:\n",
        "    \"\"\"Count the total amount of samples in a shl dataset.\"\"\"\n",
        "    n_samples = 0\n",
        "    # Every file in the dataset has the same length, use the labels file\n",
        "    with open(dataset_dir / y_file) as f:\n",
        "        for _ in tqdm(f, desc=f'Counting samples in {dataset_dir}'):\n",
        "            n_samples += 1\n",
        "    return n_samples\n",
        "\n",
        "def create_chunked_readers(\n",
        "    dataset_dir: Path,\n",
        "    chunksize: int, \n",
        "    xdtype=np.float32, # Use np.float16 with caution, can lead to overflows\n",
        "    ydtype=np.int\n",
        ") -> Tuple[List[pd.io.parsers.TextFileReader], pd.io.parsers.TextFileReader]:\n",
        "    \"\"\"Initialize chunked csv readers and return them to the caller as a tuple.\"\"\"\n",
        "    read_csv_kwargs = { 'sep': ' ', 'header': None, 'chunksize': chunksize }\n",
        "\n",
        "    X_attr_readers = [] # (dim datasets x readers)\n",
        "    for filename in X_files:\n",
        "        X_reader = pd.read_csv(dataset_dir / filename, dtype=xdtype, **read_csv_kwargs)\n",
        "        X_attr_readers.append(X_reader)\n",
        "    y_attr_reader = pd.read_csv(dataset_dir / y_file, dtype=ydtype, **read_csv_kwargs)\n",
        "\n",
        "    return X_attr_readers, y_attr_reader\n",
        "\n",
        "def export_test_data(\n",
        "    dataset_dir: Path,\n",
        "    n_examples=100,\n",
        "):\n",
        "    \"\"\"Transform the given shl dataset into a memory efficient TFRecord.\"\"\"\n",
        "    target_dir = f'{dataset_dir}.tfrecord'\n",
        "    if os.path.isfile(target_dir):\n",
        "        print(f'{target_dir} already exists.')\n",
        "        return\n",
        "\n",
        "    print(f'Exporting to {target_dir}.')\n",
        "\n",
        "    X_attr_readers, y_attr_reader = create_chunked_readers(dataset_dir, n_examples)    \n",
        "\n",
        "    examples = []\n",
        "    for X_raw_tracks, X_feature_tracks, y_attr_track in read_chunks(1, X_attr_readers, y_attr_reader):\n",
        "        for X_raw, X_feature, y in zip(X_raw_tracks, X_feature_tracks, y_attr_track):\n",
        "            examples.append({\n",
        "                'X_raw': X_raw.tolist(),\n",
        "                'X_feature': X_feature.tolist(),\n",
        "                'y': int(y)\n",
        "            })\n",
        "\n",
        "    return examples\n",
        "\n",
        "test_data = {}\n",
        "for dataset_dir in DATASET_DIRS:\n",
        "    test_data[str(dataset_dir)] = export_test_data(dataset_dir)\n",
        "\n",
        "with open('testdata.json', 'w') as f:\n",
        "    f.write(json.dumps(test_data))\n",
        "\n",
        "download('testdata.json')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Torso.tfrecord.\n",
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Bag.tfrecord.\n",
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hips.tfrecord.\n",
            "Exporting to shl-dataset/challenge-2020-users23_torso_bag_hips_hand/validation/Hand.tfrecord.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7b318101-7694-4953-ae1b-4d96bf9f96c7\", \"testdata.json\", 24775941)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}