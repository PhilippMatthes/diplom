
@book{sosnovshchenko_machine_2018,
	title = {Machine {Learning} with {Swift}: {Artificial} {Intelligence} for {iOS}},
	isbn = {978-1-78712-352-6},
	shorttitle = {Machine {Learning} with {Swift}},
	abstract = {Leverage the power of machine learning and Swift programming to build intelligent iOS applications with easeKey FeaturesImplement effective machine learning solutions for your iOS applicationsUse Swift and Core ML to build and deploy popular machine learning modelsDevelop neural networks for natural language processing and computer visionBook DescriptionMachine learning as a field promises to bring increased intelligence to the software by helping us learn and analyse information efficiently and discover certain patterns that humans cannot. This book will be your guide as you embark on an exciting journey in machine learning using the popular Swift language. We’ll start with machine learning basics in the first part of the book to develop a lasting intuition about fundamental machine learning concepts. We explore various supervised and unsupervised statistical learning techniques and how to implement them in Swift, while the third section walks you through deep learning techniques with the help of typical real-world cases. In the last section, we will dive into some hard core topics such as model compression, GPU acceleration and provide some recommendations to avoid common mistakes during machine learning application development. By the end of the book, you'll be able to develop intelligent applications written in Swift that can learn for themselves.What you will learnLearn rapid model prototyping with Python and SwiftDeploy pre-trained models to iOS using Core MLFind hidden patterns in the data using unsupervised learningGet a deeper understanding of the clustering techniquesLearn modern compact architectures of neural networks for iOS devicesTrain neural networks for image processing and natural language processingWho this book is foriOS developers who wish to create smarter iOS applications using the power of machine learning will find this book to be useful. This book will also benefit data science professionals who are interested in performing machine learning on mobile devices. Familiarity with Swift programming is all you need to get started with this book.},
	language = {en},
	publisher = {Packt Publishing Ltd},
	author = {Sosnovshchenko, Oleksandr and Baiev, Oleksandr},
	month = feb,
	year = {2018},
	note = {Google-Books-ID: lOpODwAAQBAJ},
	keywords = {Computers / Intelligence (AI) \& Semantics, Computers / Neural Networks, Computers / Operating Systems / Apple (Mac OS \& iOS)}
}

@book{mckinney_datenanalyse_2018,
	title = {Datenanalyse mit {Python}: {Auswertung} von {Daten} mit {Pandas}, {NumPy} und {IPython}},
	isbn = {978-3-96010-214-4},
	shorttitle = {Datenanalyse mit {Python}},
	abstract = {Erfahren Sie alles über das Manipulieren, Bereinigen, Verarbeiten und Aufbereiten von Datensätzen mit Python: Aktualisiert auf Python 3.6, zeigt Ihnen dieses konsequent praxisbezogene Buch anhand konkreter Fallbeispiele, wie Sie eine Vielzahl von typischen Datenanalyse-Problemen effektiv lösen. Gleichzeitig lernen Sie die neuesten Versionen von pandas, NumPy, IPython und Jupyter kennen. Geschrieben von Wes McKinney, dem Begründer des pandas-Projekts, bietet Datenanalyse mit Python einen praktischen Einstieg in die Data-Science-Tools von Python. Das Buch eignet sich sowohl für Datenanalysten, für die Python Neuland ist, als auch für Python-Programmierer, die sich in Data Science und Scientific Computing einarbeiten wollen. Daten und zugehöriges Material des Buchs sind auf GitHub verfügbar. Aus dem Inhalt:  Nutzen Sie die IPython-Shell und Jupyter Notebook für das explorative Computing Lernen Sie Grundfunktionen und fortgeschrittene Features von NumPy kennen Setzen Sie die Datenanalyse-Tools der pandasBibliothek ein Verwenden Sie flexible Werkzeuge zum Laden, Bereinigen, Transformieren, Zusammenführen und Umformen von Daten Erstellen Sie interformative Visualisierungen mit matplotlib Wenden Sie die GroupBy-Mechanismen von pandas an, um Datensätzen zurechtzuschneiden, umzugestalten und zusammenzufassen Analysieren und manipulieren Sie verschiedenste Zeitreihen-Daten  Für diese aktualisierte 2. Auflage wurde der gesamte Code an Python 3.6 und die neuesten Versionen der pandas-Bibliothek angepasst. Neu in dieser Auflage: Informationen zu fortgeschrittenen pandas-Tools sowie eine kurze Einführung in statsmodels und scikit-learn.},
	language = {de},
	publisher = {O'Reilly},
	author = {McKinney, Wes},
	month = oct,
	year = {2018},
	note = {Google-Books-ID: d9N3DwAAQBAJ},
	keywords = {Computers / Databases / Data Mining, Computers / Programming Languages / Python}
}

@book{geron_praxiseinstieg_2018,
	title = {Praxiseinstieg {Machine} {Learning} mit {Scikit}-{Learn} und {TensorFlow}: {Konzepte}, {Tools} und {Techniken} für intelligente {Systeme}},
	isbn = {978-3-96010-114-7},
	shorttitle = {Praxiseinstieg {Machine} {Learning} mit {Scikit}-{Learn} und {TensorFlow}},
	abstract = {Durchbrüche beim Deep Learning haben das maschinelle Lernen in den letzten Jahren eindrucksvoll vorangebracht. Inzwischen können sogar Programmierer, die kaum etwas über diese Technologie wissen, mit einfachen, effizienten Werkzeugen Machine-Learning-Programme implementieren. Dieses praxisorientierte Buch zeigt Ihnen wie. Mit konkreten Beispielen, einem Minimum an Theorie und zwei unmittelbar anwendbaren Python-Frameworks – Scikit-Learn und TensorFlow – verhilft Ihnen der Autor Aurélien Géron zu einem intuitiven Verständnis der Konzepte und Tools für das Entwickeln intelligenter Systeme. Sie lernen eine Vielzahl von Techniken kennen, beginnend mit einfacher linearer Regression bis hin zu neuronalen Netzen. Übungen zu jedem Kapitel helfen Ihnen, das Gelernte in die Praxis umzusetzen. Sie benötigen lediglich etwas Programmiererfahrung, um direkt zu starten. - Entdecken Sie Machine Learning, insbesondere neuronale Netze und das Deep Learning - Verwenden Sie Scikit-Learn, um ein Machine-Learning-Beispielprojekt vom Anfang bis zum Ende nachzuvollziehen - Erkunden Sie verschiedene trainierbare Modelle, darunter Support Vector Machines, Entscheidungsbäume, Random Forests und Ensemble-Methoden - Nutzen Sie die Bibliothek TensorFlow, um neuronale Netze zu erstellen und zu trainieren - Lernen Sie Architekturen neuronaler Netze kennen, darunter Convolutional Nets, Recurrent Nets und Deep Reinforcement Learning - Eignen Sie sich Techniken zum Trainieren und Skalieren von neuronalen Netzen an - Wenden Sie Codebeispiele an, ohne exzessiv die Theorie von Machine Learning oder die Algorithmik durcharbeiten zu müssen},
	language = {de},
	publisher = {O'Reilly},
	author = {Géron, Aurélien},
	month = jan,
	year = {2018},
	note = {Google-Books-ID: Qq72DwAAQBAJ},
	keywords = {Computers / Databases / Data Mining}
}

@article{jahangiri_applying_2015,
	title = {Applying {Machine} {Learning} {Techniques} to {Transportation} {Mode} {Recognition} {Using} {Mobile} {Phone} {Sensor} {Data}},
	volume = {16},
	issn = {1558-0016},
	doi = {10.1109/TITS.2015.2405759},
	abstract = {This paper adopts different supervised learning methods from the field of machine learning to develop multiclass classifiers that identify the transportation mode, including driving a car, riding a bicycle, riding a bus, walking, and running. Methods that were considered include K-nearest neighbor, support vector machines (SVMs), and tree-based models that comprise a single decision tree, bagging, and random forest (RF) methods. For training and validating purposes, data were obtained from smartphone sensors, including accelerometer, gyroscope, and rotation vector sensors. K-fold cross-validation as well as out-of-bag error was used for model selection and validation purposes. Several features were created from which a subset was identified through the minimum redundancy maximum relevance method. Data obtained from the smartphone sensors were found to provide important information to distinguish between transportation modes. The performance of different methods was evaluated and compared. The RF and SVM methods were found to produce the best performance. Furthermore, an effort was made to develop a new additional feature that entails creating a combination of other features by adopting a simulated annealing algorithm and a random forest method.},
	number = {5},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Jahangiri, A. and Rakha, H. A.},
	month = oct,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {learning (artificial intelligence), Accelerometers, Cellular phone sensor data, Data models, decision tree, decision trees, feature selection, Global Positioning System, Gyroscopes, intelligent transportation systems, k-nearest neighbor, Kernel, machine learning algorithms, machine learning technique, model selection, multiclass classifier, pattern classification, random forest method, RF method, simulated annealing, simulated annealing algorithm, smart phone sensor data, smart phones, supervised learning method, support vector machine, support vector machines, Support vector machines, SVM, Transportation, transportation mode recognition, tree-based model},
	pages = {2406--2417},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/LWXGYU74/Jahangiri und Rakha - 2015 - Applying Machine Learning Techniques to Transporta.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/EL8GWPK8/7063936.html:text/html}
}

@article{stojanov_continuous_2020,
	title = {Continuous segmentation of recognized {Transport} {Modes} by using {Convolutional} {Neural} {Networks}},
	language = {en},
	author = {Stojanov, Viktor},
	month = aug,
	year = {2020},
	pages = {114},
	file = {Stojanov - Continuous segmentation of recognized Transport Mo.pdf:/Users/philippmatthes/Zotero/storage/4JI96HAU/Stojanov - Continuous segmentation of recognized Transport Mo.pdf:application/pdf}
}

@article{matusek_anwendung_2019,
	title = {Anwendung von {Deep}-{Learning} zur {Optimierung} der {Datenvorverarbeitung} von {Sensorrohdaten} im {Movebis}-{Projekt}},
	language = {de},
	author = {Matusek, Daniel},
	month = sep,
	year = {2019},
	pages = {130},
	file = {Matusek - Anwendung von Deep-Learning zur Optimierung der Da.pdf:/Users/philippmatthes/Zotero/storage/3SJGNPS6/Matusek - Anwendung von Deep-Learning zur Optimierung der Da.pdf:application/pdf}
}

@article{werner_kontinuierliche_2020,
	title = {Kontinuierliche {Segmentbildung} erkannter {Verkehrsmittel} durch {Anwendung} rekurrenter neuronaler {Netze}},
	language = {de},
	author = {Werner, Elias},
	month = aug,
	year = {2020},
	pages = {88},
	file = {Werner - Kontinuierliche Segmentbildung erkannter Verkehrsm.pdf:/Users/philippmatthes/Zotero/storage/R3HDN5AU/Werner - Kontinuierliche Segmentbildung erkannter Verkehrsm.pdf:application/pdf}
}

@article{ravi_deep_2017,
	title = {A {Deep} {Learning} {Approach} to on-{Node} {Sensor} {Data} {Analytics} for {Mobile} or {Wearable} {Devices}},
	volume = {21},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2016.2633287},
	abstract = {The increasing popularity of wearable devices in recent years means that a diverse range of physiological and functional data can now be captured continuously for applications in sports, wellbeing, and healthcare. This wealth of information requires efficient methods of classification and analysis where deep learning is a promising technique for large-scale data analytics. While deep learning has been successful in implementations that utilize high-performance computing platforms, its use on low-power wearable devices is limited by resource constraints. In this paper, we propose a deep learning methodology, which combines features learned from inertial sensor data together with complementary information from a set of shallow features to enable accurate and real-time activity classification. The design of this combined method aims to overcome some of the limitations present in a typical deep learning framework where on-node computation is required. To optimize the proposed method for real-time on-node computation, spectral domain preprocessing is used before the data are passed onto the deep learning framework. The classification accuracy of our proposed deep learning approach is evaluated against state-of-the-art methods using both laboratory and real world activity datasets. Our results show the validity of the approach on different human activity datasets, outperforming other methods, including the two methods used within our combined pipeline. We also demonstrate that the computation times for the proposed method are consistent with the constraints of real-time on-node processing on smartphones and a wearable sensor platform.},
	number = {1},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Ravì, D. and Wong, C. and Lo, B. and Yang, G.},
	month = jan,
	year = {2017},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {deep learning, learning (artificial intelligence), Machine learning, ActiveMiles, data analysis, deep learning approach, Feature extraction, HAR, health care, Human Activities, Human Activity Recognition, Human Activity Recognition (HAR), Humans, inertial sensor, Internet of Things, Internet-of-Things, Internet-of-Things (IoT), low-power devices, Machine Learning, Monitoring, Ambulatory, Neural Networks (Computer), on-node sensor data analytics, Performance evaluation, Pipelines, Real-time systems, Signal Processing, Computer-Assisted, smartphones, spectral domain preprocessing, Spectrogram, Time-frequency analysis, wearable, wearable computers, wearable devices},
	pages = {56--64},
	file = {IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/AUQM9MBR/7797232.html:text/html;Eingereichte Version:/Users/philippmatthes/Zotero/storage/29WFGL9Q/Ravì et al. - 2017 - A Deep Learning Approach to on-Node Sensor Data An.pdf:application/pdf}
}

@inproceedings{ravi_deep_2016,
	title = {Deep learning for human activity recognition: {A} resource efficient implementation on low-power devices},
	shorttitle = {Deep learning for human activity recognition},
	doi = {10.1109/BSN.2016.7516235},
	abstract = {Human Activity Recognition provides valuable contextual information for wellbeing, healthcare, and sport applications. Over the past decades, many machine learning approaches have been proposed to identify activities from inertial sensor data for specific applications. Most methods, however, are designed for offline processing rather than processing on the sensor node. In this paper, a human activity recognition technique based on a deep learning methodology is designed to enable accurate and real-time classification for low-power wearable devices. To obtain invariance against changes in sensor orientation, sensor placement, and in sensor acquisition rates, we design a feature generation process that is applied to the spectral domain of the inertial data. Specifically, the proposed method uses sums of temporal convolutions of the transformed input. Accuracy of the proposed approach is evaluated against the current state-of-the-art methods using both laboratory and real world activity datasets. A systematic analysis of the feature generation parameters and a comparison of activity recognition computation times on mobile devices and sensor nodes are also presented.},
	booktitle = {2016 {IEEE} 13th {International} {Conference} on {Wearable} and {Implantable} {Body} {Sensor} {Networks} ({BSN})},
	author = {Ravi, D. and Wong, C. and Lo, B. and Yang, G.},
	month = jun,
	year = {2016},
	note = {ISSN: 2376-8894},
	keywords = {deep learning, learning (artificial intelligence), Machine learning, ActiveMiles, Feature extraction, HAR, Spectrogram, Time-frequency analysis, contextual information, convolution, Convolution, Data mining, Deep Learning, feature extraction, feature generation, human activity recognition, image recognition, inertial sensor data, Low-Power Devices, low-power wearable devices, machine learning, mobile devices, offline processing, resource efficient implementation, sensor acquisition rates, sensor nodes, sensor placement, spectral domain, temporal convolutions},
	pages = {71--76},
	file = {IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/W9V29SU4/7516235.html:text/html;Eingereichte Version:/Users/philippmatthes/Zotero/storage/C6RWKQCW/Ravi et al. - 2016 - Deep learning for human activity recognition A re.pdf:application/pdf}
}

@inproceedings{zeng_convolutional_2014,
	title = {Convolutional {Neural} {Networks} for human activity recognition using mobile sensors},
	doi = {10.4108/icst.mobicase.2014.257786},
	abstract = {A variety of real-life mobile sensing applications are becoming available, especially in the life-logging, fitness tracking and health monitoring domains. These applications use mobile sensors embedded in smart phones to recognize human activities in order to get a better understanding of human behavior. While progress has been made, human activity recognition remains a challenging task. This is partly due to the broad range of human activities as well as the rich variation in how a given activity can be performed. Using features that clearly separate between activities is crucial. In this paper, we propose an approach to automatically extract discriminative features for activity recognition. Specifically, we develop a method based on Convolutional Neural Networks (CNN), which can capture local dependency and scale invariance of a signal as it has been shown in speech recognition and image recognition domains. In addition, a modified weight sharing technique, called partial weight sharing, is proposed and applied to accelerometer signals to get further improvements. The experimental results on three public datasets, Skoda (assembly line activities), Opportunity (activities in kitchen), Actitracker (jogging, walking, etc.), indicate that our novel CNN-based approach is practical and achieves higher accuracy than existing state-of-the-art methods.},
	booktitle = {6th {International} {Conference} on {Mobile} {Computing}, {Applications} and {Services}},
	author = {Zeng, M. and Nguyen, L. T. and Yu, B. and Mengshoel, O. J. and Zhu, J. and Wu, P. and Zhang, J.},
	month = nov,
	year = {2014},
	keywords = {Data models, smart phones, Feature extraction, health care, Convolution, Deep Learning, human activity recognition, image recognition, Activity Recognition, behavioural sciences computing, CNN, Convolutional Neural Network, convolutional neural networks, fitness tracking, health monitoring, human behavior, Image recognition, life-logging, mobile computing, mobile sensors, neural nets, Neural networks, partial weight sharing, Principal component analysis, real-life mobile sensing applications, speech recognition, Training},
	pages = {197--205},
	file = {Volltext:/Users/philippmatthes/Zotero/storage/WK8EFPNE/Zeng et al. - 2014 - Convolutional Neural Networks for human activity r.pdf:application/pdf}
}

@article{abu_alsheikh_deep_2015,
	title = {Deep {Activity} {Recognition} {Models} with {Triaxial} {Accelerometers}},
	volume = {1511},
	url = {http://adsabs.harvard.edu/abs/2015arXiv151104664A},
	abstract = {Despite the widespread installation of accelerometers in almost all mobile phones and wearable devices, activity recognition using
accelerometers is still immature due to the poor recognition accuracy of existing recognition methods and the scarcity of labeled training data. We consider the problem of human activity recognition using triaxial accelerometers and deep learning paradigms. This paper shows that deep activity recognition models (a) provide better recognition accuracy of human activities, (b) avoid the expensive design of handcrafted features in existing systems, and (c) utilize the massive unlabeled acceleration samples for unsupervised feature extraction. Moreover, a hybrid approach of deep learning and hidden Markov models (DL-HMM) is presented for sequential activity recognition. This hybrid approach integrates the hierarchical representations of deep activity recognition models with the stochastic modeling of temporal sequences in the hidden Markov models. We show substantial recognition improvement on real world datasets over state-of-the-art methods of human activity recognition using triaxial accelerometers.},
	urldate = {2021-02-27},
	journal = {arXiv e-prints},
	author = {Abu Alsheikh, Mohammad and Selim, Ahmed and Niyato, Dusit and Doyle, Linda and Lin, Shaowei and Tan, Hwee-Pink},
	month = nov,
	year = {2015},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {arXiv:1511.04664},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/T9RRHCQK/Abu Alsheikh et al. - 2015 - Deep Activity Recognition Models with Triaxial Acc.pdf:application/pdf}
}

@inproceedings{nurhanim_classification_2017,
	title = {Classification of human activity based on smartphone inertial sensor using support vector machine},
	doi = {10.1109/ROMA.2017.8231736},
	abstract = {The aim of this paper is to compare the performance of different kernel of classification support vector machine classification for classifying the physical daily living activities. Thirty subjects from a database performed activities such as walking, sitting, standing, laying, walking upstairs and downstairs. Inertial sensors signals ((accelerometer, gyroscope and magnetometer) from the smartphone are used to measure the human movements for each activity. The inertial sensor data were processed using signal processing method with several features of time domain and frequency domain. Multiclass support vector machine polynomial kernel (MC-SVM-Polynomial) and multiclass support vector machine Linear Kernel (MC-SVM-Linear) using One Versus All (OVA) methods were employed. These classification methods are assessed using the performance criteria such as precision percentage, recall percentage and correct accuracy classification rate percentage using 10-fold cross validation procedure. The results show that MC-SVM-Polynomial produces the best result with 98.57\% compared to MC-SVM-Linear with 97.04\% based on correct accuracy classification rate.},
	booktitle = {2017 {IEEE} 3rd {International} {Symposium} in {Robotics} and {Manufacturing} {Automation} ({ROMA})},
	author = {Nurhanim, K. and Elamvazuthi, I. and Izhar, L. I. and Ganesan, T.},
	month = sep,
	year = {2017},
	keywords = {Kernel, pattern classification, smart phones, support vector machines, Support vector machines, Human Activity Recognition, Performance evaluation, feature extraction, inertial sensor data, Legged locomotion, Activity recognition, Accelerometer, Classification, Feature Extraction, frequency domain, Frequency-domain analysis, gyroscope, human activity classification, human movements, Inertial sensor, inertial sensors signals, magnetometer, MC-SVM-Linear, MC-SVM-Polynomial, multiclass support vector machine Linear Kernel, multiclass support vector machine polynomial kernel, One Versus All method, physical daily living activities, polynomials, Robot sensing systems, signal classification, signal processing, smartphone inertial sensor, support vector machine classification, time domain},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/8KKC5F7Q/8231736.html:text/html}
}

@misc{ken_taylor_activity_2011,
	title = {Activity classification with smart phones for sports activities {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S187770581101023X?token=52A037BAA5646FEF19ADB784CBC7202AF2EC555B64F6297DBD6108BAC6A1721DA85AE0A336A430885DEBB30B49C19AFA},
	language = {en},
	urldate = {2021-02-27},
	author = {Ken Taylor and {Umran A. Abdulla} and {Richard J. N. Helmer} and {Jungoo Lee} and {Ian Blanchonette}},
	year = {2011},
	doi = {10.1016/j.proeng.2011.05.109},
	file = {Snapshot:/Users/philippmatthes/Zotero/storage/C86WMTK2/S187770581101023X.html:text/html;Volltext:/Users/philippmatthes/Zotero/storage/REPEF99K/Activity classification with smart phones for spor.pdf:application/pdf}
}

@article{martin_activity_2013,
	title = {Activity logging using lightweight classification techniques in mobile devices},
	volume = {17},
	issn = {1617-4917},
	url = {https://doi.org/10.1007/s00779-012-0515-4},
	doi = {10.1007/s00779-012-0515-4},
	abstract = {Automated activity recognition enables a wide variety of applications related to child and elderly care, disease diagnosis and treatment, personal health or sports training, for which it is key to seamlessly determine and log the user’s motion. This work focuses on exploring the use of smartphones to perform activity recognition without interfering in the user’s lifestyle. Thus, we study how to build an activity recognition system to be continuously executed in a mobile device in background mode. The system relies on device’s sensing, processing and storing capabilities to estimate significant movements/postures (walking at different paces—slow, normal, rush, running, sitting, standing). In order to evaluate the combinations of sensors, features and algorithms, an activity dataset of 16 individuals has been gathered. The performance of a set of lightweight classifiers (Naïve Bayes, Decision Table and Decision Tree) working on different sensor data has been fully evaluated and optimized in terms of accuracy, computational cost and memory fingerprint. Results have pointed out that a priori information on the relative position of the mobile device with respect to the user’s body enhances the estimation accuracy. Results show that computational low-cost Decision Tables using the best set of features among mean and variance and considering all the sensors (acceleration, gravity, linear acceleration, magnetometer, gyroscope) may be enough to get an activity estimation accuracy of around 88 \% (78 \% is the accuracy of the Naïve Bayes algorithm with the same characteristics used as a baseline). To demonstrate its applicability, the activity recognition system has been used to enable a mobile application to promote active lifestyles.},
	language = {en},
	number = {4},
	urldate = {2021-02-27},
	journal = {Personal and Ubiquitous Computing},
	author = {Martín, Henar and Bernardos, Ana M. and Iglesias, Josué and Casar, José R.},
	month = apr,
	year = {2013},
	pages = {675--695},
	file = {Springer Full Text PDF:/Users/philippmatthes/Zotero/storage/HTXVGDB4/Martín et al. - 2013 - Activity logging using lightweight classification .pdf:application/pdf}
}

@article{mairittha_-device_2019,
	title = {On-{Device} {Deep} {Learning} {Inference} for {Efficient} {Activity} {Data} {Collection}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/19/15/3434},
	doi = {10.3390/s19153434},
	abstract = {Labeling activity data is a central part of the design and evaluation of human activity recognition systems. The performance of the systems greatly depends on the quantity and \&ldquo;quality\&rdquo; of annotations; therefore, it is inevitable to rely on users and to keep them motivated to provide activity labels. While mobile and embedded devices are increasingly using deep learning models to infer user context, we propose to exploit on-device deep learning inference using a long short-term memory (LSTM)-based method to alleviate the labeling effort and ground truth data collection in activity recognition systems using smartphone sensors. The novel idea behind this is that estimated activities are used as feedback for motivating users to collect accurate activity labels. To enable us to perform evaluations, we conduct the experiments with two conditional methods. We compare the proposed method showing estimated activities using on-device deep learning inference with the traditional method showing sentences without estimated activities through smartphone notifications. By evaluating with the dataset gathered, the results show our proposed method has improvements in both data quality (i.e., the performance of a classification model) and data quantity (i.e., the number of data collected) that reflect our method could improve activity data collection, which can enhance human activity recognition systems. We discuss the results, limitations, challenges, and implications for on-device deep learning inference that support activity data collection. Also, we publish the preliminary dataset collected to the research community for activity recognition.},
	language = {en},
	number = {15},
	urldate = {2021-02-27},
	journal = {Sensors},
	author = {Mairittha, Nattaya and Mairittha, Tittaya and Inoue, Sozo},
	month = jan,
	year = {2019},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {activity recognition, data collection, on-device deep learning inference, smartphone sensors, user feedback},
	pages = {3434},
	file = {Snapshot:/Users/philippmatthes/Zotero/storage/SPYW8GMZ/3434.html:text/html;Full Text PDF:/Users/philippmatthes/Zotero/storage/WDDNZLER/Mairittha et al. - 2019 - On-Device Deep Learning Inference for Efficient Ac.pdf:application/pdf}
}

@article{mairittha_-device_2021,
	title = {On-{Device} {Deep} {Personalization} for {Robust} {Activity} {Data} {Collection}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/21/1/41},
	doi = {10.3390/s21010041},
	abstract = {One of the biggest challenges of activity data collection is the need to rely on users and keep them engaged to continually provide labels. Recent breakthroughs in mobile platforms have proven effective in bringing deep neural networks powered intelligence into mobile devices. This study proposes a novel on-device personalization for data labeling for an activity recognition system using mobile sensing. The key idea behind this system is that estimated activities personalized for a specific individual user can be used as feedback to motivate user contribution and improve data labeling quality. First, we exploited fine-tuning using a Deep Recurrent Neural Network to address the lack of sufficient training data and minimize the need for training deep learning on mobile devices from scratch. Second, we utilized a model pruning technique to reduce the computation cost of on-device personalization without affecting the accuracy. Finally, we built a robust activity data labeling system by integrating the two techniques outlined above, allowing the mobile application to create a personalized experience for the user. To demonstrate the proposed model\&rsquo;s capability and feasibility, we developed and deployed the proposed system to realistic settings. For our experimental setup, we gathered more than 16,800 activity windows from 12 activity classes using smartphone sensors. We empirically evaluated the proposed quality by comparing it with a baseline using machine learning. Our results indicate that the proposed system effectively improved activity accuracy recognition for individual users and reduced cost and latency for inference for mobile devices. Based on our findings, we highlight critical and promising future research directions regarding the design of efficient activity data collection with on-device personalization.},
	language = {en},
	number = {1},
	urldate = {2021-02-27},
	journal = {Sensors},
	author = {Mairittha, Nattaya and Mairittha, Tittaya and Inoue, Sozo},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, activity recognition, data collection, smartphone sensors, user feedback, fine-tuning, on-device personalization},
	pages = {41},
	file = {Snapshot:/Users/philippmatthes/Zotero/storage/EDS25LFF/41.html:text/html;Full Text PDF:/Users/philippmatthes/Zotero/storage/6EDBKBQR/Mairittha et al. - 2021 - On-Device Deep Personalization for Robust Activity.pdf:application/pdf}
}

@article{li_deep_2020,
	title = {Deep {Transfer} {Learning} for {Time} {Series} {Data} {Based} on {Sensor} {Modality} {Classification}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/15/4271},
	doi = {10.3390/s20154271},
	abstract = {The scarcity of labelled time-series data can hinder a proper training of deep learning models. This is especially relevant for the growing field of ubiquitous computing, where data coming from wearable devices have to be analysed using pattern recognition techniques to provide meaningful applications. To address this problem, we propose a transfer learning method based on attributing sensor modality labels to a large amount of time-series data collected from various application fields. Using these data, our method firstly trains a Deep Neural Network (DNN) that can learn general characteristics of time-series data, then transfers it to another DNN designed to solve a specific target problem. In addition, we propose a general architecture that can adapt the transferred DNN regardless of the sensors used in the target field making our approach in particular suitable for multichannel data. We test our method for two ubiquitous computing problems\&mdash;Human Activity Recognition (HAR) and Emotion Recognition (ER)\&mdash;and compare it a baseline training the DNN without using transfer learning. For HAR, we also introduce a new dataset, Cognitive Village-MSBand (CogAge), which contains data for 61 atomic activities acquired from three wearable devices (smartphone, smartwatch, and smartglasses). Our results show that our transfer learning approach outperforms the baseline for both HAR and ER.},
	language = {en},
	number = {15},
	urldate = {2021-02-27},
	journal = {Sensors},
	author = {Li, Frédéric and Shirahama, Kimiaki and Nisar, Muhammad Adeel and Huang, Xinyu and Grzegorzek, Marcin},
	month = jan,
	year = {2020},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, human activity recognition, emotion recognition, time-series classification, transfer learning, wearable computing},
	pages = {4271},
	file = {Snapshot:/Users/philippmatthes/Zotero/storage/6LNS9DJX/4271.html:text/html;Full Text PDF:/Users/philippmatthes/Zotero/storage/W6QYZPFA/Li et al. - 2020 - Deep Transfer Learning for Time Series Data Based .pdf:application/pdf}
}

@inproceedings{curukoglu_deep_2018,
	title = {Deep {Learning} on {Mobile} {Systems}},
	doi = {10.1109/ASYU.2018.8554039},
	abstract = {Training of deep learning methods which are widely utilized in intelligent systems requires powerful computers, however, they are also used on mobile systems. Hence, application programming interfaces providing execution of pre-trained models on mobile systems have been introduced. In this study, two interfaces running on Android systems are tested on 40 objects for both object detection and labelling. Morever, ease of installations, resource usages were also compared. According to the test results, labelling performance of the model provided by Tensorflow interface is higher than the model of MLKit.},
	booktitle = {2018 {Innovations} in {Intelligent} {Systems} and {Applications} {Conference} ({ASYU})},
	author = {Çürükoğlu, N. and Özyildirim, B. M.},
	month = oct,
	year = {2018},
	keywords = {android systems, application program interfaces, application programming interfaces, Computational modeling, Computer architecture, Deep Learning, deep learning methods, Google, intelligent systems, Labeling, learning (artificial intelligence), MLKit, mobile computing, mobile systems, Mobile Systems, object detection, Object detection, Tensorflow, tensorflow interface},
	pages = {1--4},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/EXQZPMZ6/Çürükoğlu und Özyildirim - 2018 - Deep Learning on Mobile Systems.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/ZMCU2EJ9/8554039.html:text/html}
}

@article{ota_deep_2017,
	title = {Deep {Learning} for {Mobile} {Multimedia}: {A} {Survey}},
	volume = {13},
	issn = {1551-6857},
	shorttitle = {Deep {Learning} for {Mobile} {Multimedia}},
	url = {https://doi.org/10.1145/3092831},
	doi = {10.1145/3092831},
	abstract = {Deep Learning (DL) has become a crucial technology for multimedia computing. It offers a powerful instrument to automatically produce high-level abstractions of complex multimedia data, which can be exploited in a number of applications, including object detection and recognition, speech-to- text, media retrieval, multimodal data analysis, and so on. The availability of affordable large-scale parallel processing architectures, and the sharing of effective open-source codes implementing the basic learning algorithms, caused a rapid diffusion of DL methodologies, bringing a number of new technologies and applications that outperform, in most cases, traditional machine learning technologies. In recent years, the possibility of implementing DL technologies on mobile devices has attracted significant attention. Thanks to this technology, portable devices may become smart objects capable of learning and acting. The path toward these exciting future scenarios, however, entangles a number of important research challenges. DL architectures and algorithms are hardly adapted to the storage and computation resources of a mobile device. Therefore, there is a need for new generations of mobile processors and chipsets, small footprint learning and inference algorithms, new models of collaborative and distributed processing, and a number of other fundamental building blocks. This survey reports the state of the art in this exciting research area, looking back to the evolution of neural networks, and arriving to the most recent results in terms of methodologies, technologies, and applications for mobile environments.},
	number = {3s},
	urldate = {2021-02-28},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Ota, Kaoru and Dao, Minh Son and Mezaris, Vasileios and Natale, Francesco G. B. De},
	month = jun,
	year = {2017},
	keywords = {Deep learning, deep neural networks, mobile multimedia computing},
	pages = {34:1--34:22},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/Y3W66QTC/Ota et al. - 2017 - Deep Learning for Mobile Multimedia A Survey.pdf:application/pdf}
}

@article{ma_survey_2019,
	title = {A {Survey} on {Deep} {Learning} {Empowered} {IoT} {Applications}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2958962},
	abstract = {The Internet of Things (IoT) is widely regarded as a key component of the Internet of the future and thereby has drawn significant interests in recent years. IoT consists of billions of intelligent and communicating “things”, which further extend borders of the world with physical and virtual entities. Such ubiquitous smart things produce massive data every day, posing urgent demands on quick data analysis on various smart mobile devices. Fortunately, the recent breakthroughs in deep learning have enabled us to address the problem in an elegant way. Deep models can be exported to process massive sensor data and learn underlying features quickly and efficiently for various IoT applications on smart devices. In this article, we survey the literature on leveraging deep learning to various IoT applications. We aim to give insights on how deep learning tools can be applied from diverse perspectives to empower IoT applications in four representative domains, including smart healthcare, smart home, smart transportation, and smart industry. A main thrust is to seamlessly merge the two disciplines of deep learning and IoT, resulting in a wide-range of new designs in IoT applications, such as health monitoring, disease analysis, indoor localization, intelligent control, home robotics, traffic prediction, traffic monitoring, autonomous driving, and manufacture inspection. We also discuss a set of issues, challenges, and future research directions that leverage deep learning to empower IoT applications, which may motivate and inspire further developments in this promising field.},
	journal = {IEEE Access},
	author = {Ma, X. and Yao, T. and Hu, M. and Dong, Y. and Liu, W. and Wang, F. and Liu, J.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {data analysis, Data models, deep learning, Deep learning, deep learning tools, Internet of Things, IoT applications, learning (artificial intelligence), Medical services, Monitoring, Recurrent neural networks, smart devices, smart healthcare, smart home, smart mobile devices, smart transportation, ubiquitous smart things},
	pages = {181721--181732},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/XCFPPSK6/Ma et al. - 2019 - A Survey on Deep Learning Empowered IoT Applicatio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/CEFXT2SD/8932460.html:text/html}
}

@article{zhang_deep_2019,
	title = {Deep {Learning} in {Mobile} and {Wireless} {Networking}: {A} {Survey}},
	volume = {21},
	issn = {1553-877X},
	shorttitle = {Deep {Learning} in {Mobile} and {Wireless} {Networking}},
	doi = {10.1109/COMST.2019.2904897},
	abstract = {The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, real-time extraction of fine-grained analytics, and agile management of network resources, so as to maximize user experience. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques, in order to help manage the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper, we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.},
	number = {3},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Zhang, C. and Patras, P. and Haddadi, H.},
	year = {2019},
	note = {Conference Name: IEEE Communications Surveys Tutorials},
	keywords = {5G mobile communication, 5G systems, algorithm-driven applications, Big Data, deep learning, Deep learning, encyclopedic review, fine-grained analytics real-time extraction, learning (artificial intelligence), machine learning, mobile big data, mobile networking, mobile traffic volumes, mobile wireless networking infrastructure, network management, network resources management, neural nets, Neural networks, telecommunication computing, telecommunication network management, telecommunication traffic, Tutorials, user experience, Wireless communication, wireless networking},
	pages = {2224--2287},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/JHRW2E3I/Zhang et al. - 2019 - Deep Learning in Mobile and Wireless Networking A.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/T3AGX7D5/8666641.html:text/html}
}

@article{nan_deep_2019,
	title = {Deep model compression for mobile platforms: {A} survey},
	volume = {24},
	issn = {1007-0214},
	shorttitle = {Deep model compression for mobile platforms},
	doi = {10.26599/TST.2018.9010103},
	abstract = {Despite the rapid development of mobile and embedded hardware, directly executing computation-expensive and storage-intensive deep learning algorithms on these devices' local side remains constrained for sensory data analysis. In this paper, we first summarize the layer compression techniques for the state-of-the-art deep learning model from three categories: weight factorization and pruning, convolution decomposition, and special layer architecture designing. For each category of layer compression techniques, we quantify their storage and computation tunable by layer compression techniques and discuss their practical challenges and possible improvements. Then, we implement Android projects using TensorFlow Mobile to test these 10 compression methods and compare their practical performances in terms of accuracy, parameter size, intermediate feature size, computation, processing latency, and energy consumption. To further discuss their advantages and bottlenecks, we test their performance over four standard recognition tasks on six resource-constrained Android smartphones. Finally, we survey two types of run-time Neural Network (NN) compression techniques which are orthogonal with the layer compression techniques, run-time resource management and cost optimization with special NN architecture, which are orthogonal with the layer compression techniques.},
	number = {6},
	journal = {Tsinghua Science and Technology},
	author = {Nan, K. and Liu, S. and Du, J. and Liu, H.},
	month = dec,
	year = {2019},
	note = {Conference Name: Tsinghua Science and Technology},
	keywords = {Android (operating system), Artificial neural networks, Computational modeling, Computer architecture, convolution, Convolution, convolution decomposition, cost optimization, data analysis, data compression, deep learning, Deep learning, deep learning model, deep model compression, layer compression techniques, learning (artificial intelligence), Matrix decomposition, mobile computing, mobile platforms, model compression, neural nets, Performance evaluation, pruning, resource-constrained Android smartphones, run-time neural network compression techniques, run-time resource management, sensory data analysis, smart phones, special layer architecture design, TensorFlow Mobile, weight factorization},
	pages = {677--693},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/PDYIUF5Q/Nan et al. - 2019 - Deep model compression for mobile platforms A sur.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/Y64TUK3H/8727762.html:text/html}
}

@article{chen_deep_2020,
	title = {Deep {Learning} on {Mobile} and {Embedded} {Devices}: {State}-of-the-art, {Challenges}, and {Future} {Directions}},
	volume = {53},
	issn = {0360-0300},
	shorttitle = {Deep {Learning} on {Mobile} and {Embedded} {Devices}},
	url = {https://doi.org/10.1145/3398209},
	doi = {10.1145/3398209},
	abstract = {Recent years have witnessed an exponential increase in the use of mobile and embedded devices. With the great success of deep learning in many fields, there is an emerging trend to deploy deep learning on mobile and embedded devices to better meet the requirement of real-time applications and user privacy protection. However, the limited resources of mobile and embedded devices make it challenging to fulfill the intensive computation and storage demand of deep learning models. In this survey, we conduct a comprehensive review on the related issues for deep learning on mobile and embedded devices. We start with a brief introduction of deep learning and discuss major challenges of implementing deep learning models on mobile and embedded devices. We then conduct an in-depth survey on important compression and acceleration techniques that help adapt deep learning models to mobile and embedded devices, which we specifically classify as pruning, quantization, model distillation, network design strategies, and low-rank factorization. We elaborate on the hardware-based solutions, including mobile GPU, FPGA, and ASIC, and describe software frameworks for mobile deep learning models, especially the development of frameworks based on OpenCL and RenderScript. After that, we present the application of mobile deep learning in a variety of areas, such as navigation, health, speech recognition, and information security. Finally, we discuss some future directions for deep learning on mobile and embedded devices to inspire further research in this area.},
	number = {4},
	urldate = {2021-02-28},
	journal = {ACM Computing Surveys},
	author = {Chen, Yanjiao and Zheng, Baolin and Zhang, Zihan and Wang, Qian and Shen, Chao and Zhang, Qian},
	month = aug,
	year = {2020},
	keywords = {Deep learning, hardware solutions, mobile devices, network compression and acceleration, software frameworks},
	pages = {84:1--84:37},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/CCYTIF6R/Chen et al. - 2020 - Deep Learning on Mobile and Embedded Devices Stat.pdf:application/pdf}
}

@inproceedings{radu_towards_2016,
	address = {New York, NY, USA},
	series = {{UbiComp} '16},
	title = {Towards multimodal deep learning for activity recognition on mobile devices},
	isbn = {978-1-4503-4462-3},
	url = {https://doi.org/10.1145/2968219.2971461},
	doi = {10.1145/2968219.2971461},
	abstract = {Current smartphones and smartwatches come equipped with a variety of sensors, from light sensor and inertial sensors to radio interfaces, enabling applications running on these devices to make sense of their surrounding environment. Rather than using sensors independently, combining their sensing capabilities facilitates more interesting and complex applications to emerge (e.g., user activity recognition). But differences between sensors ranging from sampling rate to data generation model (event triggered or continuous sampling) make integration of sensor streams challenging. Here we investigate the opportunity to use deep learning to perform this integration of sensor data from multiple sensors. The intuition is that neural networks can identify nonintuitive features largely from cross-sensor correlations which can result in a more accurate estimation. Initial results with a variant of a Restricted Boltzmann Machine (RBM), show better performance with this new approach compared to classic solutions.},
	urldate = {2021-02-28},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing}: {Adjunct}},
	publisher = {Association for Computing Machinery},
	author = {Radu, Valentin and Lane, Nicholas D. and Bhattacharya, Sourav and Mascolo, Cecilia and Marina, Mahesh K. and Kawsar, Fahim},
	month = sep,
	year = {2016},
	keywords = {activity recognition, context detection, deep learning, mobile sensing, multimodal sensing},
	pages = {185--188},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/CSP95LXB/Radu et al. - 2016 - Towards multimodal deep learning for activity reco.pdf:application/pdf}
}

@inproceedings{mairittha_improving_2020,
	address = {New York, NY, USA},
	series = {{UbiComp}-{ISWC} '20},
	title = {Improving activity data collection with on-device personalization using fine-tuning},
	isbn = {978-1-4503-8076-8},
	url = {https://doi.org/10.1145/3410530.3414370},
	doi = {10.1145/3410530.3414370},
	abstract = {One of the biggest challenges of activity data collection is the unavoidability of relying on users and keep them engaged to provide labels consistently. Recent breakthroughs in mobile platforms have proven effective in bringing deep neural networks powered intelligence into mobile devices. In this study, we propose on-device personalization using fine-tuning convolutional neural networks as a mechanism in optimizing human effort in data labeling. First, we transfer the knowledge gained by on-cloud pre-training based on crowdsourced data to mobile devices. Second, we incrementally fine-tune a personalized model on every individual device using its locally accumulated input. Then, we utilize estimated activities customized according to the on-device model inference as feedback to motivate participants to improve data labeling. We conducted a verification study and gathered activity labels with smartphone sensors. Our preliminary evaluation results indicate that the proposed method outperformed the baseline method by approximately 8\% regarding accuracy recognition.},
	urldate = {2021-02-28},
	booktitle = {Adjunct {Proceedings} of the 2020 {ACM} {International} {Joint} {Conference} on {Pervasive} and {Ubiquitous} {Computing} and {Proceedings} of the 2020 {ACM} {International} {Symposium} on {Wearable} {Computers}},
	publisher = {Association for Computing Machinery},
	author = {Mairittha, Nattaya and Mairittha, Tittaya and Inoue, Sozo},
	month = sep,
	year = {2020},
	keywords = {activity recognition, data collection, fine-tuning, on-device deep learning},
	pages = {255--260},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/KEQEYWD7/Mairittha et al. - 2020 - Improving activity data collection with on-device .pdf:application/pdf}
}

@article{inoue_deep_2018,
	title = {Deep recurrent neural network for mobile human activity recognition with high throughput},
	volume = {23},
	issn = {1614-7456},
	url = {https://doi.org/10.1007/s10015-017-0422-x},
	doi = {10.1007/s10015-017-0422-x},
	abstract = {In this paper, we propose a method of human activity recognition with high throughput from raw accelerometer data applying a deep recurrent neural network (DRNN), and investigate various architectures and its combination to find the best parameter values. The “high throughput” refers to short time at a time of recognition. We investigated various parameters and architectures of the DRNN by using the training dataset of 432 trials with 6 activity classes from 7 people. The maximum recognition rate was 95.42\% and 83.43\% against the test data of 108 segmented trials each of which has single activity class and 18 multiple sequential trials, respectively. Here, the maximum recognition rates by traditional methods were 71.65\% and 54.97\% for each. In addition, the efficiency of the found parameters was evaluated using additional dataset. Further, as for throughput of the recognition per unit time, the constructed DRNN was requiring only 1.347 ms, while the best traditional method required 11.031 ms which includes 11.027 ms for feature calculation. These advantages are caused by the compact and small architecture of the constructed real time oriented DRNN.},
	language = {en},
	number = {2},
	urldate = {2021-02-28},
	journal = {Artificial Life and Robotics},
	author = {Inoue, Masaya and Inoue, Sozo and Nishida, Takeshi},
	month = jun,
	year = {2018},
	pages = {173--185},
	file = {Springer Full Text PDF:/Users/philippmatthes/Zotero/storage/NW425SIL/Inoue et al. - 2018 - Deep recurrent neural network for mobile human act.pdf:application/pdf}
}

@inproceedings{chen_deep_2015,
	title = {A {Deep} {Learning} {Approach} to {Human} {Activity} {Recognition} {Based} on {Single} {Accelerometer}},
	doi = {10.1109/SMC.2015.263},
	abstract = {In this paper, we propose an acceleration-based human activity recognition method using popular deep architecture, Convolution Neural Network (CNN). In particular, we construct a CNN model and modify the convolution kernel to adapt the characteristics of tri-axial acceleration signals. Also, for comparison, we use some widely used methods to accomplish the recognition task on the same dataset. The large dataset we constructed consists of 31688 samples from eight typical activities. The experiment results show that the CNN works well, which can reach an average accuracy of 93.8\% without any feature extraction methods.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics}},
	author = {Chen, Y. and Xue, Y.},
	month = oct,
	year = {2015},
	keywords = {Acceleration, acceleration-based human activity recognition method, accelerometers, CNN model, Convolution, convolution kernel, convolution neural network, deep architecture, deep learning approach, Discrete cosine transforms, feature extraction, Feature extraction, feature extraction method, gesture recognition, human activity recognition, Kernel, learning (artificial intelligence), Legged locomotion, neural nets, popular deep architecture, recognition task, single accelerometer, Training, tri-axial acceleration signal},
	pages = {1488--1492},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/LLUDWDQW/Chen und Xue - 2015 - A Deep Learning Approach to Human Activity Recogni.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/UQZD7GFL/7379395.html:text/html}
}

@article{bulling_tutorial_2014,
	title = {A tutorial on human activity recognition using body-worn inertial sensors},
	volume = {46},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2499621},
	doi = {10.1145/2499621},
	abstract = {The last 20 years have seen ever-increasing research activity in the field of human activity recognition. With activity recognition having considerably matured, so has the number of challenges in designing, implementing, and evaluating activity recognition systems. This tutorial aims to provide a comprehensive hands-on introduction for newcomers to the field of human activity recognition. It specifically focuses on activity recognition using on-body inertial sensors. We first discuss the key research challenges that human activity recognition shares with general pattern recognition and identify those challenges that are specific to human activity recognition. We then describe the concept of an Activity Recognition Chain (ARC) as a general-purpose framework for designing and evaluating activity recognition systems. We detail each component of the framework, provide references to related research, and introduce the best practice methods developed by the activity recognition research community. We conclude with the educational example problem of recognizing different hand gestures from inertial sensors attached to the upper and lower arm. We illustrate how each component of this framework can be implemented for this specific activity recognition problem and demonstrate how different implementations compare and how they impact overall recognition performance.},
	number = {3},
	urldate = {2021-02-28},
	journal = {ACM Computing Surveys},
	author = {Bulling, Andreas and Blanke, Ulf and Schiele, Bernt},
	month = jan,
	year = {2014},
	keywords = {Activity recognition, Activity Recognition Chain (ARC), gesture recognition, on-body inertial sensors},
	pages = {33:1--33:33},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/5DWHDIPM/Bulling et al. - 2014 - A tutorial on human activity recognition using bod.pdf:application/pdf}
}

@article{kwapisz_activity_2011,
	title = {Activity recognition using cell phone accelerometers},
	volume = {12},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/1964897.1964918},
	doi = {10.1145/1964897.1964918},
	abstract = {Mobile devices are becoming increasingly sophisticated and the latest generation of smart cell phones now incorporates many diverse and powerful sensors. These sensors include GPS sensors, vision sensors (i.e., cameras), audio sensors (i.e., microphones), light sensors, temperature sensors, direction sensors (i.e., magnetic compasses), and acceleration sensors (i.e., accelerometers). The availability of these sensors in mass-marketed communication devices creates exciting new opportunities for data mining and data mining applications. In this paper we describe and evaluate a system that uses phone-based accelerometers to perform activity recognition, a task which involves identifying the physical activity a user is performing. To implement our system we collected labeled accelerometer data from twenty-nine users as they performed daily activities such as walking, jogging, climbing stairs, sitting, and standing, and then aggregated this time series data into examples that summarize the user activity over 10- second intervals. We then used the resulting training data to induce a predictive model for activity recognition. This work is significant because the activity recognition model permits us to gain useful knowledge about the habits of millions of users passively---just by having them carry cell phones in their pockets. Our work has a wide range of applications, including automatic customization of the mobile device's behavior based upon a user's activity (e.g., sending calls directly to voicemail if a user is jogging) and generating a daily/weekly activity profile to determine if a user (perhaps an obese child) is performing a healthy amount of exercise.},
	number = {2},
	urldate = {2021-02-28},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Kwapisz, Jennifer R. and Weiss, Gary M. and Moore, Samuel A.},
	month = mar,
	year = {2011},
	keywords = {accelerometer, activity recognition, cell phone, induction, sensor mining, sensors},
	pages = {74--82},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/GGSYEB73/Kwapisz et al. - 2011 - Activity recognition using cell phone acceleromete.pdf:application/pdf}
}

@inproceedings{bhattacharya_smart_2016,
	title = {From smart to deep: {Robust} activity recognition on smartwatches using deep learning},
	shorttitle = {From smart to deep},
	doi = {10.1109/PERCOMW.2016.7457169},
	abstract = {The use of deep learning for the activity recognition performed by wearables, such as smartwatches, is an understudied problem. To advance current understanding in this area, we perform a smartwatch-centric investigation of activity recognition under one of the most popular deep learning methods - Restricted Boltzmann Machines (RBM). This study includes a variety of typical behavior and context recognition tasks related to smartwatches (such as transportation mode, physical activities and indoor/outdoor detection) to which RBMs have previously never been applied. Our findings indicate that even a relatively simple RBM-based activity recognition pipeline is able to outperform a wide-range of common modeling alternatives for all tested activity classes. However, usage of deep models is also often accompanied by resource consumption that is unacceptably high for constrained devices like watches. Therefore, we complement this result with a study of the overhead of specifically RBM-based activity models on representative smartwatch hardware (the Snapdragon 400 SoC, present in many commercial smartwatches). These results show, contrary to expectation, RBM models for activity recognition have acceptable levels of resource use for smartwatch-class hardware already on the market. Collectively, these two experimental results make a strong case for more widespread adoption of deep learning techniques within smartwatch designs moving forward.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communication} {Workshops} ({PerCom} {Workshops})},
	author = {Bhattacharya, S. and Lane, N. D.},
	month = mar,
	year = {2016},
	keywords = {Boltzmann machines, Computational modeling, deep learning techniques, indoor-outdoor detection, learning (artificial intelligence), Machine learning, Mathematical model, Mobile communication, mobile computing, physical activities, Pipelines, RBM models, RBM-based activity models, RBM-based activity recognition pipeline, resource consumption, restricted Boltzmann machines, robust activity recognition, Sensors, smart phones, smartwatch hardware, smartwatch-centric investigation, Snapdragon 400 SoC, Transportation, transportation mode, watches, wearable computers},
	pages = {1--6},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/9QZTZMIQ/Bhattacharya und Lane - 2016 - From smart to deep Robust activity recognition on.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/AUIZDYM6/7457169.html:text/html}
}

@inproceedings{gudur_activeharnet_2019,
	address = {New York, NY, USA},
	series = {{EMDL} '19},
	title = {{ActiveHARNet}: {Towards} {On}-{Device} {Deep} {Bayesian} {Active} {Learning} for {Human} {Activity} {Recognition}},
	isbn = {978-1-4503-6771-4},
	shorttitle = {{ActiveHARNet}},
	url = {https://doi.org/10.1145/3325413.3329790},
	doi = {10.1145/3325413.3329790},
	abstract = {Various health-care applications such as assisted living, fall detection etc., require modeling of user behavior through Human Activity Recognition (HAR). HAR using mobile- and wearable-based deep learning algorithms have been on the rise owing to the advancements in pervasive computing. However, there are two other challenges that need to be addressed: first, the deep learning model should support on-device incremental training (model updation) from real-time incoming data points to learn user behavior over time, while also being resource-friendly; second, a suitable ground truthing technique (like Active Learning) should help establish labels on-the-fly while also selecting only the most informative data points to query from an oracle. Hence, in this paper, we propose ActiveHARNet, a resource-efficient deep ensembled model which supports on-device Incremental Learning and inference, with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using dropout. This is combined with suitable acquisition functions for active learning. Empirical results on two publicly available wrist-worn HAR and fall detection datasets indicate that ActiveHARNet achieves considerable efficiency boost during inference across different users, with a substantially low number of acquired pool points (at least 60\% reduction) during incremental learning on both datasets experimented with various acquisition functions, thus demonstrating deployment and Incremental Learning feasibility.},
	urldate = {2021-02-28},
	booktitle = {The 3rd {International} {Workshop} on {Deep} {Learning} for {Mobile} {Systems} and {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Gudur, Gautham Krishna and Sundaramoorthy, Prahalathan and Umaashankar, Venkatesh},
	month = jun,
	year = {2019},
	keywords = {bayesian active learning, fall detection, human activity recognition, incremental learning, on-device deep learning},
	pages = {7--12},
	file = {Full Text PDF:/Users/philippmatthes/Zotero/storage/8ZUDY6BA/Gudur et al. - 2019 - ActiveHARNet Towards On-Device Deep Bayesian Acti.pdf:application/pdf}
}

@inproceedings{huo_uncertainty_2020,
	title = {Uncertainty {Quantification} for {Deep} {Context}-{Aware} {Mobile} {Activity} {Recognition} and {Unknown} {Context} {Discovery}},
	url = {http://proceedings.mlr.press/v108/huo20a.html},
	abstract = {Activity recognition in wearable computing faces two key challenges: i) activity characteristics may be context-dependent and change under different contexts or situations; ii) unknown contexts and...},
	language = {en},
	urldate = {2021-02-28},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Huo, Zepeng and PakBin, Arash and Chen, Xiaohan and Hurley, Nathan and Yuan, Ye and Qian, Xiaoning and Wang, Zhangyang and Huang, Shuai and Mortazavi, Bobak},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3894--3904},
	file = {Snapshot:/Users/philippmatthes/Zotero/storage/M9RSEINL/huo20a.html:text/html;Full Text PDF:/Users/philippmatthes/Zotero/storage/QKWFHJRA/Huo et al. - 2020 - Uncertainty Quantification for Deep Context-Aware .pdf:application/pdf}
}

@inproceedings{nutter_design_2018,
	title = {Design of {Novel} {Deep} {Learning} {Models} for {Real}-time {Human} {Activity} {Recognition} with {Mobile} {Phones}},
	doi = {10.1109/IJCNN.2018.8489319},
	abstract = {In this paper we present deep learning based techniques for human activity classification that are designed to run in real time on mobile devices. Our methods minimize the size of the model and computational overhead in order to run on the embedded processor and preserve battery life. Prior work shows that the inertial measurement unit (IMU) data from waist-mounted mobile phones can be used to develop accurate classification models for various human activities such as walking, running, stair-climbing, etc. However, these models have largely been based on hand crafted features derived from temporal and spectral statistics. More recently, deep learning has been applied to IMU sensor data, but have not been optimized for resourceconstrained devices. We present a detailed study of the traditional hand-crafted features used for shallow/statistical models that consist of a over 561 manually chosen set of dimensions. We show, through principal component analysis (PCA) and application of a published support vector machine (SVM) pipeline, that the number of features can be significantly reduced - less than 100 features that give the same performance. In addition, we show that features derived from frequency-domain transformations do not contribute to the accuracy of these models. Finally, we provide details of our learning technique which creates 2D signal images from windowed samples of IMU data. Our pipeline includes a convolutional neural network (CNN) with several layers (1 convolutional layer and 1 averaging layer and a fully connected layer). We show that by removing the steps in the pipeline and layers in the CNN, we can still achieve 0.98 F1 score but with a much smaller memory footprint and corresponding computational cost. To increase the classification accuracy of our pipeline we added a hybrid bi-class support vector machine (SVM) that was trained using the labeled and flattened convolutional layer after each training image was processed. The learned feature set is almost half the size of the original hand crafted feature set and combining the CNN with the SVM results in 0.99 F1 score. We also investigate a novel application of transfer learning by using the time series 2D signal images to re-train two different publicly available networks, Inception/ImageNet and MobileNet. We find that re-trained ImageNet networks could be created \${\textless}; 5.5\$ MB (suitable for mobile phones) and classification accuracy ranging from 0.83 to 0.93 (F1 score), thus indicating that retraining can be a useful future direction to build new classifiers for continuously evolving activities quickly while also being applicable to mobile device classification. Finally, we show that these deep learning models may be generalizable enough such that classifiers built from a given set of users for a specified set of activities can be used for a new user/subject as well.},
	booktitle = {2018 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Nutter, M. and Crawford, C. H. and Ortiz, J.},
	month = jul,
	year = {2018},
	note = {ISSN: 2161-4407},
	keywords = {battery life, CNN, computational cost, Computational modeling, computational overhead, convolution, convolutional neural network, deep learning models, embedded processor, feature extraction, feedforward neural nets, frequency-domain transformations, health care, human activity classification, human activity recognition, hybrid bi-class support vector machine, image classification, IMU data, IMU sensor data, inertial measurement unit data, learned feature set, learning (artificial intelligence), Machine learning, memory footprint, mobile computing, mobile device classification, mobile devices, Mobile handsets, Neural networks, pattern classification, Pipelines, principal component analysis, resource constrained devices, sensors, shallow-statistical models, spectral statistics, stair-climbing, support vector machine pipeline, support vector machines, Support vector machines, SVM results, temporal statistics, time series 2D signal images, training image, transfer learning, Two dimensional displays, waist-mounted mobile phones},
	pages = {1--8},
	file = {IEEE Xplore Full Text PDF:/Users/philippmatthes/Zotero/storage/DPQLPCVM/Nutter et al. - 2018 - Design of Novel Deep Learning Models for Real-time.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/philippmatthes/Zotero/storage/NH67RXRH/8489319.html:text/html}
}
